{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT RECOMMENDED !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# THE COMMAND LINE VERSION IS MORE EFFICIENT\n",
    "from learn import *\n",
    "from utils.jupyter_agents_display import AgentDisplayManager\n",
    "\n",
    "# Please ensure to run cell \"# Agents display\" BEFORE \"# Experiment\" for proper visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d514b81c13d2445e9a7f42dff37cfd57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='experiment_240110_1434.html', continuous_update=False, description='File name:'), T…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook mode = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG Enter a number for subdirectory (leave empty for default): 1. IR_CPS_TechSynthesis; 2. minecraft ? \n",
      "CONFIG When adding the functions description in successful tasks, do you want to also include the code (it may overflow the maximum prompt length but can also guide generation) ? (yes/no):  \n",
      "CONFIG Please select the functions to load (separated by comma, or 'all' to load all, or just hit enter for none):  \n",
      "CONFIG Please select the functions to load (separated by comma, or 'all' to load all, or just hit enter for none):  \n",
      "\n",
      "\u001b[32mBEFORE\u001b[0m inference @ TaskIdentificationAgent-> Choose an action (or hit Enter for inference) : \n",
      "\n",
      "\u001b[32mAFTER\u001b[0m inference @ TaskIdentificationAgent-> Choose an action (or hit Enter for inference) : \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex QA & language models hybrid architectures, Survey /State-of-the-Art of Language Models for Complex Question-Answering /Introduction /Complex Question Answering Systems /Major Conferences /Definitions /Question Complexity and Information Retrieval /Modular approaches before transformers /Transformer-based Language Models /Analyzing: complexity, skills, tasks, and limits /Complex Questions /Interpreting & manipulating input /World Modeling /Experience learning in complex QA systems /QA complex tasks benchmark /Test Document /CQA tasks decomposition and primitives /LLM Limits /Hybridization /Evaluating: metrics, cost functions, datasets /Evaluation Measures for QA Models /Evaluation Metrics /New metrics for free-form/natural language QA /On the Blind Spots of Model-Based Evaluation Metrics for Text Generation /HELM multi-metrics and related findings /Pareto Frontier /Metrics for Complex QA /Taxonomy of existing explainable evaluation metrics /Eval4NLP 2021 /QA/CQA text datasets /Question Answering Datasets /Multimodal QA datasets /Structured Knowledge Datasets /Decomposition and multi-hop datasets /QA Datasets /Explainability tasks datasets /Generate or improve datasets /Long QA /Pre-training techniques /Self-supervised learning /Adversarial SSL learns by distinguishing corrupted tokens (replaced or shuffled), can be used alone or in continual pretraining like contrastive. Different techniques exist: replaced token detection (RTD - used by ELECTRA model), multi-lingual replaced token detection (MRTD) is used by XLM-E model, translation replaced token detection (TRTD), shuffled token detection (STD) is used by RoBERTa model. Hybrid SSL uses more than one type of SSL - e.g. U-Palm uses up to 7 denoising objectives (mixtures-of-denoisers [158]), BERT uses MLM (generative) and NSP (contrastive), ALBERT used MLM and SOP (contrastive), infoXLM uses MLM + TLM (generative) and XLCo (contrastive). Here XLCo represents the cross lingual contrastive pretraining task. /Vanilla Fine-Tuning /Transfer Learning, Knowledge Distillation, Active Learning /Meta Learning /Intermediate Fine-Tuning /Techniques for improving training /Hybrid Architectural Patterns for Solving Complex QA /LLM + Symbolic/structured Information Retriever /LLM + Program /LLM + Human/AI RL feedback /LLM + Router or Task discriminator /LLM + Multimodal /Designing my question (prompting) /Compositional Task Representations /Maieutic Prompting /Reinforcement Learning Methods /Human-in-the-loop (RLHF) /Limitations and Research Topics for Solving More Complex QA and Problems /Hallucination & credibility in language models /Data Availability and Quality /Data multi-sensitivity usage & protection /Decomposition of very complex QA and explainability /Conclusion /References 1 /References 2 /References 3 /References 4 /References 5 /References 6 /References 7 /References 8 /References 9 /References 10 /References 11 /References 12 /References 13 /References 14 /References 15 /References 16 /References 17 /References 18 /References 19 /References 20 /References 21 /References 22 /References 23 /References 24 /References 25 /References 26 /References 27 /References 28 /References 29 /References 30 /References 31 /References 32 /References 33 /References 34 /References 35 /References 36 /References 37 /References 38 /References 39 /References 40 /References 41 /References 42 /References 43 /References 44 /References 45 /References 46 /References 47 /References 48 /References 49 /References 50 /References 51 /References 52 /References 53 /References 54 /References 55 /\n",
      "Macroeconomic Effects of Inflation Targeting: A Survey of the Empirical Literature /Inflation Targeting and its Impact on Macroeconomic Benefits /Implementation of Inflation Targeting /Advantages of Inflation Targeting /Policy leading to lower average inflation accompanied by output stabilization /Effects of Inflation Targeting in Emerging Market Economies /Literature Review on the Macroeconomic Effects of IT /Empirical Literature on the Macroeconomic Performance of Inflation Targeting /Effects of IT /Prerequisites for Inflation Targeting /Determinants of IT Adoption /Role of Factors in the Adoption of IT /Monetary Policy and Economic Development /Effects of Economic Performance on Central Bank Strategy /Monetary Policy and Inflation Targeting /Monetary Regimes and Inflation Targeting Adoption /Fiscal Discipline and Adoption of IT /Role of Fiscal Discipline and Openness in Choosing Monetary Policy /Empirical Research on Financial Openness and Inflation Targeting /Empirical Research on Exchange Rate Flexibility and Central Bank Independence in the Implementation of Inflation Targeting /Central Bank Independence and the Adoption of Inflation Targeting /Adoption of IT /Determinants of the Adoption of Inflation Targeting: A Review of the Literature /Determinants of IT Regimes /Effects of Economic Activity on IT Adoption /The Lack of Robust Findings in the Field of Inflation Targeting /Effects of Inflation Targeting on Inflation Expectations and Inflation Persistence /Effects of Inflation Targeting on Inflation Expectations /Effectiveness of Inflation Targeting in Anchoring Inflation Expectations /Effect of IT on Inflation Expectations /Empirical Literature on the Effects of Inflation Targeting /Evidence on the Impact of IT on Inflation Persistence /Inflation Persistence and the Effectiveness of Inflation Targeting /Effects of Inflation Targeting /Effectiveness of Inflation Targeting Policy in OECD Countries /Effects of Inflation Targeting on Inflation Volatility /Empirical Evidence on the Adoption of IT in Monetary Regimes /Comparing the Effects of Inflation Targeting on Inflation and Inflation Volatility /None /Effect of Inflation Targeting in Emerging Market Economies /Effects of IT on Inflation in Developing Countries /Effects of Inflation Targeting on Emerging Market Economies /Effects of IT on Inflation and Output Growth /Effects of IT on Macroeconomic Performance /Effects of Inflation Targeting on Output Growth /Effects of Information Technology on Output Growth in OECD and Non-OECD Countries /IT and the Global crisis /Performance of Inflation Targeting during the Global Financial Crisis /Comparison between Hard Peggers and Inflation Targeters /Effects of Inflation Targeting on Interest Rates and Exchange Rates /Empirical Studies on the Effects of Inflation Targeting on Interest Rates /Impact of IT on Exchange Rates /Effects of IT on Exchange Rate Variability /Effects of Inflation Targeting on Exchange Rate Volatility /Impact of IT, Hard Pegs, and Intermediate Exchange Rate Regimes on Real Exchange Rate Volatility /Effect of IT on Exchange Rate Volatility /IT and Internal Real Exchange Rate Volatility /Effects of Inflation Targeting on Fiscal Discipline /Fiscal Rules and the Effects of Inflation Targeting on Budget Balance /Limitations of comparing IT countries to control group /Methodological Issues in Estimating the Effects of Inflation Targeting on Macroeconomic Performance /Propensity Score Matching Methodology /IT and Observable Characteristics /Approaches to Analyzing the Effects of Inflation Targeting /Methodology in Empirical Literature on IT Effects /Subjective Interpretation of Econometric Evidence /IT and Disinflation Costs /Variations of the Sacrifice Ratio in Disinflation Measures /Relationship between IT and Sacrifice Ratios /Effectiveness of Inflation Targeting in Reducing Disinflation Costs /Effects of IT on Sacrifice Ratios /Central Bank Credibility and Sacrifice Ratios in Asian Countries /Approach of Ball (1994) on estimating sacrifice ratio /Empirical Studies on the Determinants of Inflation Targeting Adoption /Empirical Evidence on the Effects of Information Technology Adoption /Effects of Inflation Targeting on Macroeconomic Performance /Conclusion from Survey of Empirical Literature on Macroeconomic Effects of IT /Inflation Targeting /Inflation Targeting: Assessing the Evidence /Central Bank Independence and Sacrifice Ratios /Monetary Policy /The effectiveness of monetary policy anchors: Firm-level evidence /Political Regimes and the Cost of Disinflation /Twenty years of inflation targeting /Inflation Targeting and Exchange Rate /Inflation /Inflation Targeting and its Effects on Deficits and Debt /Inflation Targeting and Financial Stability in Emerging Markets /The price stability under inflation targeting regime: An analysis with a new intermediate approach /Open-Economy Inflation-Forecast Targeting /Determinants of Inflation Targeting Adoption /Inflation Targeting in Emerging Market Countries /Regression Approach /Handbook of monetary economics /Inflation Targeting and Exchange Rate Regime /References /Czech Republic /Literature on Inflation Forecasting /1995 (Samarina and De Haan 2014, Thornton and Vasilakis 2017) 1999 (Canarella and Miller 2016, Canarella and Miller 2017a, Combes et al. 2014, Corbo et al. 2002, Gonçalves and Salles 2008, Levin et al 2004, Lin 2010, Mishkin and Schmidt-Hebbel 2002, Pétursson 2004, Rose 2007, Vega and Winkelried, 2005) 2001 (Ardakani et al. 2018, Combes et al. 2014, Freedman and Laxton 2009, Kose et al. 2018, Lucotte 2012, Samarina and De Haan 2014) 2002 (Batini and Laxton 2007) 1989 (Thornton and Vasilakis 2017) 1994 (Corbo et al. 2002, Gonçalves and Salles 2008, Mishkin and Schmidt-Hebbel 2002, Leyva 2008, Samarina and De Haan 2014) 2002 (Ardakani et al. 2018, Batini and Laxton 2007, Combes et al. 2014, Levin et al. 2004, Lin 2010, Pétursson 2004, Rose 2007, Samarina and De Haan 2014, Vega and Winkelried 2005) 1995 (Samarina and De Haan 2014) 2002 (most of the literature) /Research Papers /Selected empirical studies on the determinants of adopting IT /Advanced and Developing Countries Probit and Logit Models /28 advanced countries and EMEs during 1975-2005; logit and probit models /Lucotte (2012) /Development /Financial Factors and Macroeconomic Performance /Determinants of adopting inflation targeting /List of References /Fry-McKibbin and Wang (2014) (-) Hu (2006) (-) Lin (2010) (-) Lin and Ye (2007) (-) Lin and Ye (2009) (-) Minea et al. (2021) (-) /Sample /Exchange Rate Volatility /Trade and Financial Openness /Current account balance /Samarina and De Haan (2014) /crises /Alpanda and Honig (2014) /inflation /interest and inflation studies /Inflation and GDP Growth /inflation expectations /Kočenda and Varga (2018) /nominal volatility, inflation persistence, inflation, inflation expectations, short-term interest rate, long-term interest rate, GDP growth, exchange rate, real interest rate /inflation expectations, inflation persistence, exchange rate volatility, foreign reserves, current account, inflation, inflation volatility, long-term nominal interest rate, public revenue, government consumption, budget balance, GDP growth /Research Papers on Economic Analysis /Panel Data Models and IV in Advanced and Developing Countries /EMEs and Advanced Countries Analysis /Table 3B /Ginindza and Maasoumi (2013) /Dotsey (2006) /Inflation expectations and persistence /IMF (2006) /Mixed Sample /Ayres et al. (2014) /GDP growth /Gonçalves and Salles (2008) /Lin (2010) (-) Ouyang and Rajan (2016) (x) Ouyang et al. (2016) (+) Ouyang et al. (2016) (x) Pontines (2013) (-) Pontines (2013) (+) /EMEs and Advanced Countries /Empirical studies on the effects of IT on sacrifice ratios /Countries and Time Periods /\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32mBEFORE\u001b[0m inference @ CodingAgent-> Choose an action (or hit Enter for inference) : \n",
      "\n",
      "\u001b[32mAFTER\u001b[0m inference @ CodingAgent-> Choose an action (or hit Enter for inference) : \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated code:\n",
      "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
      "import spacy\n",
      "def extract_key_concepts_and_topics(bot, document_id):\n",
      "    # Get the abstract of the document\n",
      "    abstract = bot.get_document_abstract(document_id)\n",
      "\n",
      "    # Tokenize the abstract into sentences\n",
      "    nlp = spacy.load(\"en_core_web_sm\")\n",
      "    doc = nlp(abstract)\n",
      "    sentences = [sent.text for sent in doc.sents]\n",
      "\n",
      "    # Use a method to determine the importance of each sentence based on its content and relevance to the overall abstract\n",
      "    # Here, we can use a pre-trained model for text summarization to identify important sentences\n",
      "    summarization_pipeline = pipeline(\"summarization\")\n",
      "    summarized_abstract = summarization_pipeline(abstract, max_length=100, min_length=30, do_sample=False)[0]['summary_text']\n",
      "\n",
      "    # Utilize named entity recognition to identify important entities such as specific technologies, methods, or domains\n",
      "    ner_pipeline = pipeline(\"ner\")\n",
      "    named_entities = ner_pipeline(abstract)\n",
      "\n",
      "    # Apply topic modeling techniques to extract prevalent themes and topics from the key sentences\n",
      "    # Here, we can use a pre-trained model for token classification to extract topics from the abstract\n",
      "    tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
      "    model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
      "    topic_modeling_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
      "    topics = topic_modeling_pipeline(abstract)\n",
      "\n",
      "    # Store the extracted key concepts and topics in the document's resources\n",
      "    bot.add_or_update_result_in_resources(\n",
      "        {\n",
      "            \"document_id\": document_id,\n",
      "            \"key_sentences\": sentences,\n",
      "            \"summarized_abstract\": summarized_abstract,\n",
      "            \"named_entities\": named_entities,\n",
      "            \"topics\": topics\n",
      "        },\n",
      "        name=\"key_concepts_and_topics\"\n",
      "    )\n",
      "\n",
      "    # Log an event for the successful extraction of key concepts and topics\n",
      "    bot.add_event(\"Key concepts and topics extracted\", {\"document_id\": document_id})\n",
      "*******\n",
      "Output of code execution:\n",
      "['Failed to execute provided code. Error: No module named \\'transformers\\' Traceback: Traceback (most recent call last):\n",
      "  File \"/home/xavier/CollabFunctionsGPTCreator/learn.py\", line 59, in step\n",
      "    exec(action_code+helper, context)\n",
      "  File \"<string>\", line 78, in <module>\n",
      "ModuleNotFoundError: No module named \\'transformers\\'\n",
      "', 'Failed to execute provided code. Error: No module named \\'transformers\\' Traceback: Traceback (most recent call last):\n",
      "  File \"/home/xavier/CollabFunctionsGPTCreator/learn.py\", line 59, in step\n",
      "    exec(action_code+helper, context)\n",
      "  File \"<string>\", line 78, in <module>\n",
      "ModuleNotFoundError: No module named \\'transformers\\'\n",
      "']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Experiment\n",
    "AgentDisplayManager.display_export_html_interface()\n",
    "orchestrate_agents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1dff02b4010476bb6d90f9b665755d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Select Task:', options=(), value=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ad212856e74e508868aa601f834483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Agents display\n",
    "\n",
    "AgentDisplayManager.display()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (navalhackathon2023)",
   "language": "python",
   "name": "navalhackathon2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "02c737251231468b8a14a3fa9161856c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "03f375053a2e4199ab3d6ef292e6bf61": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_e2a75c1a334a462da94cdfa7e8b70b75",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "\u001b[35m***** TaskIdentificationAgent->identify_best_task AFTER *****\nLLM ANSWER:\n1. Reasoning: Considering the provided examples, it seems that a valuable task to learn would be to extract key concepts and topics from a research paper's abstract. This task would involve identifying the main ideas and topics discussed in the abstract, which could be useful for creating a table of contents or for providing a quick overview of the paper's content.\n\n2. Task: Extract key concepts and topics from a research paper's abstract using natural language processing tools.\n\n3. Plan:\n    1. Identify key sentences in the abstract that convey important concepts and topics.\n        1. Use natural language processing tools to tokenize the abstract into sentences.\n        2. Apply a method to determine the importance of each sentence based on its content and relevance to the overall abstract.\n    2. Extract key concepts and topics from the identified key sentences.\n        1. Utilize named entity recognition to identify important entities such as specific technologies, methods, or domains.\n        2. Apply topic modeling techniques to extract prevalent themes and topics from the key sentences.\n\n4. Tests:\n```python\n# document #cf0d353c-b43b-4a79-88f9-42c2c84cf75e usage test:\nextract_key_concepts_and_topics(bot, document_id=\"cf0d353c-b43b-4a79-88f9-42c2c84cf75e\")\n# document #42252c6c-12f3-4edf-9045-8acd69bc3356 usage test:\nextract_key_concepts_and_topics(bot, document_id=\"42252c6c-12f3-4edf-9045-8acd69bc3356\")\n```\n***** TaskIdentificationAgent->identify_best_task AFTER *****\u001b[0m\nA. Manually set/modify the answer/output (I don't want to try to improve agent's system prompt).\nB. Critic this answer/output to get an improved answer/output.\nC. Find a better Prompt by providing critic and ideal answer.\nD. Evaluate & comment answer (Score between 0(worst)-1(top), and explain) to improve future results by using scored/commented examples.\nE. Go back BEFORE inference to improve system prompt or add information to user message.\nG. Skip human actions for N rounds.\nH. Exit program.\n\n"
        }
       ]
      }
     },
     "08ca0bbf973244e28a7fa9ba7e83942c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "AccordionModel",
      "state": {
       "children": [
        "IPY_MODEL_03f375053a2e4199ab3d6ef292e6bf61",
        "IPY_MODEL_3eb115591784491b830904a8a6f88885",
        "IPY_MODEL_a6d08648739647b3801951e0877c7905",
        "IPY_MODEL_762c620464a8475f94ebf3fb12b45428",
        "IPY_MODEL_550037e086d345e8b20a28409278af13"
       ],
       "layout": "IPY_MODEL_26f2dfae3d6146c694e398412b2afae6",
       "selected_index": 0,
       "titles": [
        "5. AFTER inference action MENU 14:35:18",
        "4. NEW inference result recieved 14:35:18",
        "3. Inference streaming output 14:35:10",
        "2. BEFORE inference action MENU 14:34:54",
        "1. HumanLLMMonitor 14:34:54"
       ]
      }
     },
     "0c0ca7ef06b048fbb0797cd6a9aa3226": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "126269a864cc4918991f64bdd4a1be9e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1ca598b747b54c71b6309ff3e1efb4f2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1f5b90aebda843e1a970ab317be86f54": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "26f2dfae3d6146c694e398412b2afae6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "27dea91467a0471499054d725fe41de5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "29ee81a4c50f407f956f8043c26812af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "3430d3d79cc14648b5db4ea019f08b4e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "397b407fa56b4d06a1a275740daeb7e5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3b0df72295e7424990a9e94b1066137a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "3eb115591784491b830904a8a6f88885": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_fa4fa550088c4cbbbd16cb0a62c9534c",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "\u001b[0m**** New inference result recieved and added to outputs as #1\u001b[0m:\n1. Reasoning: Considering the provided examples, it seems that a valuable task to learn would be to extract key concepts and topics from a research paper's abstract. This task would involve identifying the main ideas and topics discussed in the abstract, which could be useful for creating a table of contents or for providing a quick overview of the paper's content.\n\n2. Task: Extract key concepts and topics from a research paper's abstract using natural language processing tools.\n\n3. Plan:\n    1. Identify key sentences in the abstract that convey important concepts and topics.\n        1. Use natural language processing tools to tokenize the abstract into sentences.\n        2. Apply a method to determine the importance of each sentence based on its content and relevance to the overall abstract.\n    2. Extract key concepts and topics from the identified key sentences.\n        1. Utilize named entity recognition to identify important entities such as specific technologies, methods, or domains.\n        2. Apply topic modeling techniques to extract prevalent themes and topics from the key sentences.\n\n4. Tests:\n```python\n# document #cf0d353c-b43b-4a79-88f9-42c2c84cf75e usage test:\nextract_key_concepts_and_topics(bot, document_id=\"cf0d353c-b43b-4a79-88f9-42c2c84cf75e\")\n# document #42252c6c-12f3-4edf-9045-8acd69bc3356 usage test:\nextract_key_concepts_and_topics(bot, document_id=\"42252c6c-12f3-4edf-9045-8acd69bc3356\")\n```\n\u001b[9mEND OF #1****\u001b[0m\n\u001b[POST INFERENCE for #1****\u001b[0m"
        }
       ]
      }
     },
     "43cabc6dc19e4325af7970b65ee4004f": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_9883787c07b74d438042d9b108923eac",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "Multiple tasks selection not yet supported. Selecting the first one.\n"
        }
       ]
      }
     },
     "47ae4c2ca3dd435eba3ecdb68ac53f67": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_fb8215c803ea4ade9ca2dc923e6f75f8",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "\u001b[0m**** New inference result recieved and added to outputs as #1\u001b[0m:\nReasoning: To extract key concepts and topics from a research paper's abstract, we can follow the plan by first identifying key sentences in the abstract and then extracting key concepts and topics from these sentences using natural language processing tools.\n\nCode:\n```python\nfrom transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\nimport spacy\n\ndef extract_key_concepts_and_topics(bot, document_id):\n    # Get the abstract of the document\n    abstract = bot.get_document_abstract(document_id)\n\n    # Tokenize the abstract into sentences\n    nlp = spacy.load(\"en_core_web_sm\")\n    doc = nlp(abstract)\n    sentences = [sent.text for sent in doc.sents]\n\n    # Use a method to determine the importance of each sentence based on its content and relevance to the overall abstract\n    # Here, we can use a pre-trained model for text summarization to identify important sentences\n    summarization_pipeline = pipeline(\"summarization\")\n    summarized_abstract = summarization_pipeline(abstract, max_length=100, min_length=30, do_sample=False)[0]['summary_text']\n\n    # Utilize named entity recognition to identify important entities such as specific technologies, methods, or domains\n    ner_pipeline = pipeline(\"ner\")\n    named_entities = ner_pipeline(abstract)\n\n    # Apply topic modeling techniques to extract prevalent themes and topics from the key sentences\n    # Here, we can use a pre-trained model for token classification to extract topics from the abstract\n    tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n    model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n    topic_modeling_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n    topics = topic_modeling_pipeline(abstract)\n\n    # Store the extracted key concepts and topics in the document's resources\n    bot.add_or_update_result_in_resources(\n        {\n            \"document_id\": document_id,\n            \"key_sentences\": sentences,\n            \"summarized_abstract\": summarized_abstract,\n            \"named_entities\": named_entities,\n            \"topics\": topics\n        },\n        name=\"key_concepts_and_topics\"\n    )\n\n    # Log an event for the successful extraction of key concepts and topics\n    bot.add_event(\"Key concepts and topics extracted\", {\"document_id\": document_id})\n```\n\u001b[9mEND OF #1****\u001b[0m\n\u001b[POST INFERENCE for #1****\u001b[0m"
        }
       ]
      }
     },
     "481b4dea5fb24090962de2f023702fc2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4a91fc9e329f400297ef05e7aca103db": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_6f590c5a2d024aa8959df5b63d7b75d3",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "\u001b[31m****ValidationAgent>validate_code calling HumanLLMMonitor****\u001b[0m\n"
        }
       ]
      }
     },
     "4ff3854979484c5998be54da717f0589": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_fc06904198e6472697b5d18b9b2c68f4",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "************ Retrieving failed tasks from database - LIST:\n"
        }
       ]
      }
     },
     "51049a4f8212430f824338fefaf127e4": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_99e18ee30fe648b8a0ac70b4f2d197bf",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "\u001b[32m***** CodingAgent->code_task_and_run_test  BEFORE *****\nSYSTEM PROMPT:\nYou are a helpful assistant that writes Python code to be executed using a restricted list of packages (BeautifulSoap, RegEx, Sklearn, Huggingface, Langchain, Voyager) to complete the task specified by me.\n\nAt each round of conversation, I will give you:\n- Reasoning: explanation of the task chosen...\n- Task: ...\n- Plan: ...\n- Tests: tests that will be done on target document\nCURRENT STATE OF THE ENVIRONMENT USED TO TEST TASK\nDocument #.... : 1.title: ...; 2. abstract: ...; 3. current table of content; 4. current resources; 5. sections titles progress; 6. sections content progress; 7. events counted\n...\n- General code for re-use or demonstration purpose: ...\n- Code from the last round trying to implement task: ...\n- Execution error: ...\n\nYou should then respond to me with:\n- Reasoning: How to best implement the plan with no errors and maximum result ?\n- Code:\n    1) Write a function taking the bot as the first parameter which is the shared object of document content and resources (it is an instance of the class SynthesisManager). \n    2) Ensure that the generated code adheres to principles of reusability and modularity. Specifically, functions should not hard-code strings, variables, or parameters that make them context-specific. Instead, any data or parameters that can vary should be passed as arguments to the functions, ensuring that the functions can be reused in different contexts or with different data without requiring modifications to the code itself. This ensures that the code is adaptable and can be utilized in various scenarios, enhancing its utility and longevity.\n    3) Call existing functions as much as possible.\n    4) Your function will be reused for building more complex functions. Therefore, you should make it generic and reusable. Avoid to include specific query or information in the function instead of using it as an argument\n    5) Anything defined outside a function will be ignored, define all your variables and classes inside your functions.\n    6) Ensure that your code is fully executable, it is not a skeleton and does not contain placeholders, unimplemented sections, or comments indicating future work (e.g., TODO, pass, \"....\", etc.). All functions and logic must be complete and runnable to facilitate immediate use and testing.\n    7) Do not write infinite loops or recursive functions.\n    8) Name your function in a meaningful way (can infer the task from the name).\n    9) Any packages/libraries used by the function should be imported inside the function (it will be ignored if imported outside)\n    10) Success of the task output is evaluated by analyzing the new state of resources and sections, and events.\n    11) If some content is generated for the task and that its quality impact task's success, you must log an event using `bot.add_event(event: str, data: dict)` to enable the critic agent to evaluate this content it through the events' list. The `event` should describe the type of event to help critic to understand what to check, and `data` should include all information to be analyzed. If this event is in a loop/for, just fully log the event 1 time to avoid too much logging and allow sampling evaluation. \n    12) Your function should include appropriate modification to resources and sections to measure task success. Main functions are:\n        - class Section(section_id: int, title: str, content: str, parent_id: int)\n        - Manipulate document sections: bot.create_and_add_section_then_return_id(title: str, content: str, section_id: int = None, parent_id: int = None) -> int, bot.get_all_sections() -> List[Section], bot.get_sections(ids: List[int]) -> List[Section], bot.edit_section(section_id: int, new_content: str = None, new_title: str = None, new_parent_id: int = None) -> bool, bot.remove_section(section_id: int) -> bool, bot.swap_sections(section_id_1: int, section_id_2: int) -> bool\n        - Manipulate document resources: bot.add_or_update_results_in_resources(results, metadatas_to_add:dict=None, store_linked_document_content:bool=False), bot.add_or_update_result_in_resources(metadatas:dict, name:str=None, content:dict=None, link:str=None, store_linked_document_content:bool=False), bot.get_all_resources(self) -> List[Dict[str, Any]], bot.semantic_search_resources(query_texts, n_results=10), bot.add_or_update_results_in_resources(results, metadatas:dict=None, store_linked_document_content:bool=False), bot.get_and_store_link_content(link:str=None, parent_id=None, chaining:bool=True), bot.remove_resource(resource_id)\n    13) Before the return of main function, ensure to store your results or text generated in resources or sections which are the only permanent storage. Also ensure that results are returned for future reuse of the function.\n\nRESPONSE FORMAT (You should only respond in the format as described below):\n\nReasoning: ...\n\nCode:\n```python\n# helper functions (only if needed, try to avoid them)\n# detailed content of the function...\n\n# main function after the helper functions\ndef your_main_function_name(bot, args...):\n    # detailed content of the function...\n\n```\n\nUSER MESSAGE:\nTASK DEFINITION: 1. Reasoning: Considering the provided examples, it seems that a valuable task to learn would be to extract key concepts and topics from a research paper's abstract. This task would involve identifying the main ideas and topics discussed in the abstract, which could be useful for creating a table of contents or for providing a quick overview of the paper's content.\n\n2. Task: Extract key concepts and topics from a research paper's abstract using natural language processing tools.\n\n3. Plan:\n    1. Identify key sentences in the abstract that convey important concepts and topics.\n        1. Use natural language processing tools to tokenize the abstract into sentences.\n        2. Apply a method to determine the importance of each sentence based on its content and relevance to the overall abstract.\n    2. Extract key concepts and topics from the identified key sentences.\n        1. Utilize named entity recognition to identify important entities such as specific technologies, methods, or domains.\n        2. Apply topic modeling techniques to extract prevalent themes and topics from the key sentences.\n\n4. Tests:\n```python\n# document #cf0d353c-b43b-4a79-88f9-42c2c84cf75e usage test:\nextract_key_concepts_and_topics(bot, document_id=\"cf0d353c-b43b-4a79-88f9-42c2c84cf75e\")\n# document #42252c6c-12f3-4edf-9045-8acd69bc3356 usage test:\nextract_key_concepts_and_topics(bot, document_id=\"42252c6c-12f3-4edf-9045-8acd69bc3356\")\n```\n\nCURRENT STATE OF THE ENVIRONMENT USED TO TEST TASK:\n<<< Document #cf0d353c-b43b-4a79-88f9-42c2c84cf75e properties:\n1. title: Complex QA and language models hybrid architectures, Survey\n2. abstract: This paper reviews the state-of-the-art of language models architectures and strategies for 'complex' question-answering (QA, CQA, CPS) with a focus on hybridization. Large Language Models (LLM) are good at leveraging public data on standard problems but once you want to tackle more specific complex questions or problems (e.g. How does the concept of personal freedom vary between different cultures ? What is the best mix of power generation methods to reduce climate change ?) you may need specific architecture, knowledge, skills, methods, sensitive data protection, explainability, human approval and versatile feedback... Recent projects like ChatGPT and GALACTICA have allowed non-specialists to grasp the great potential as well as the equally strong limitations of LLM in complex QA. In this paper, we start by reviewing required skills and evaluation techniques. We integrate findings from the robust community edited research papers BIG, BLOOM and HELM which open source, benchmark and analyze limits and challenges of LLM in terms of tasks complexity and strict evaluation on accuracy (e.g. fairness, robustness, toxicity, ...) as a baseline. We discuss some challenges associated with complex QA, including domain adaptation, decomposition and efficient multi-step QA, long form and non-factoid QA, safety and multi-sensitivity data protection, multimodal search, hallucinations, explainability and truthfulness, temporal reasoning. We analyze current solutions and promising research trends, using elements such as: hybrid LLM architectural patterns, training and prompting strategies, active human reinforcement learning supervised with AI, neuro-symbolic and structured knowledge grounding, program synthesis, iterated decomposition and others.\n3. current table of content: Empty\n4. current resources: Empty\n5. sections titles progress: 0.0\n6. sections content progress: 0.0\n7. events counted: Empty\n>>>\n<<< Document #42252c6c-12f3-4edf-9045-8acd69bc3356 properties:\n1. title: Macroeconomic Effects of Inflation Targeting A Survey of the Empirical  Literature\n2. abstract: This paper surveys the empirical literature of inflation targeting. The main findings from our review are the following: there is robust empirical evidence that larger and more developed countries are more likely to adopt the IT regime; the introduction of this regime is conditional on previous disinflation, greater exchange rate flexibility, central bank independence, and higher level of financial development; the empirical evidence has failed to provide convincing evidence that IT itself may serve as an effective tool for stabilizing inflation expectations and for reducing inflation persistence; the empirical research focused on advanced economies has failed to provide convincing evidence on the beneficial effects of IT on inflation performance, while there is some evidence that the gains from the IT regime may have been more prevalent in the emerging market economies; there is not convincing evidence that IT is associated with either higher output growth or lower output variability; the empirical research suggests that IT may have differential effects on exchange-rate volatility in advanced economies versus EMEs; although the empirical evidence on the impact of IT on fiscal policy is quite limited, it supports the idea that IT indeed improves fiscal discipline; the empirical support to the proposition that IT is associated with lower disinflation costs seems to be rather weak. Therefore, the accumulated empirical literature implies that IT does not produce superior macroeconomic benefits in comparison with the alternative monetary strategies or, at most, they are quite modest.\n3. current table of content: Empty\n4. current resources: Empty\n5. sections titles progress: 0.0\n6. sections content progress: 0.0\n7. events counted: Empty\n>>>\n\nCODE PRIMITIVES RE-USABLE OR FOR DEMONTRATION PURPOSE:\nfrom env.IR_CPS_TechSynthesis.env import SynthesisManager, Section\n\n# This is the main function that will be called by the CPS\n# It will return a list of articles given a query\ndef searchTop25ResearchPaper(bot: SynthesisManager, query, MAX_ARTICLES=25, MAX_SOURCES=10, SIMILARITY_THRESHOLD = 0.85, search_sources = ['paper_arxiv','paper_pubmed', 'websearch_wikipedia','paper_semantic_scholar']):\n    import requests\n    from sklearn.metrics.pairwise import cosine_similarity\n    from sentence_transformers import SentenceTransformer\n\n    def compute_embeddings(texts):\n        model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n        return model.encode(texts)\n\n    def remove_similar_articles(articles, SIMILARITY_THRESHOLD):\n        titles = [article['title'] for article in articles]\n        embeddings = compute_embeddings(titles)\n        similarity_matrix = cosine_similarity(embeddings)\n\n        to_remove = set()\n        for i in range(len(titles)):\n            for j in range(i+1, len(titles)):\n                if similarity_matrix[i][j] > SIMILARITY_THRESHOLD:\n                    to_remove.add(j)\n\n        unique_articles = [article for idx, article in enumerate(articles) if idx not in to_remove]\n        return unique_articles\n\n    # Collect the results from each source\n    all_results = []\n    for source in search_sources:\n        # search_generic returns a list of result dict, each of format: {'title': '...', 'link': '...', 'description': '...'}\n        results = SynthesisManager.search_generic(query, search_type=source, max_results=MAX_ARTICLES)\n        all_results.extend(results)\n\n    unique_results = remove_similar_articles(all_results, SIMILARITY_THRESHOLD)[:MAX_ARTICLES]\n\n    bot.add_or_update_results_in_resources(unique_results)\n\n    return unique_results\n\nfrom utils.file_utils import load_from_pickle, save_to_pickle\n@load_from_pickle\n@save_to_pickle\n# This function returns a text generated for a given task on a text by GPT3.5 Given a prompt template and a text in order to summarize it, or extract some key information...\ndef generateTextFromInput(prompt_template = \"\", text=\"\", temperature=0.5, request_timout=120):\n    from langchain.chat_models import ChatOpenAI\n    from langchain.prompts import ChatPromptTemplate\n    from config import OPENAI_API_KEY\n\n    if prompt_template == \"\":\n        prompt_template = \"\"\"Extract the following key elements from the research paper provided below:\n1. Abstract: Summarize the abstract and identify any key elements that are missing which are later provided in the introduction.\n2. Conclusion: Summarize the conclusion of the paper.\n3. Findings: Detail the main findings of the paper.\n4. Challenges/Discussion: Highlight the main challenges or discussion points mentioned in the paper.\n5. Methodology: Describe the methodology used in the paper.\n\nThe output should be in JSON format with the following keys (if any of the below elements are not present in the paper, the value for the respective JSON key should be 'not found'):\n- 'abstract_and_missing_elements': Max length of 500 words.\n- 'conclusion': Max length of 300 words.\n- 'findings': Max length of 500 words.\n- 'challenges_discussion': Max length of 400 words.\n- 'methodology': Max length of 400 words.\n\nResearch Paper Text: {text}\"\"\"\n\n    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=temperature, openai_api_key=openai_api_key)\n    prompter = ChatPromptTemplate.from_template(prompt_template)\n    message = prompter.format_messages(text=text)\n    generated_text = llm(message)\n    return generated_text.content\n\n***** CodingAgent->code_task_and_run_test BEFORE *****\u001b[0m\nA. Modify agent's 'role' / 'system prompt' (role, global context, constraints, examples).\nB. Add instruction or information to agent.\nC. Skip and set LLM output from recent outputs or manually define it.\nD. Log comments (not used by the model, just for information).\nE. See all previous results for this agent.\nF. See previous MODIFIED/SCORED/COMMENTED results for this agent.\nG. Skip human actions for N rounds.\nH. Exit program.\nJ. Change num of parallel inferences - Current value=1\nP. Proceed to inference using a PREMIUM LLM.\n\n"
        }
       ]
      }
     },
     "550037e086d345e8b20a28409278af13": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_1f5b90aebda843e1a970ab317be86f54",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "\u001b[35m****TaskIdentificationAgent>identify_best_task calling HumanLLMMonitor****\u001b[0m\n"
        }
       ]
      }
     },
     "566e83dcedf242dc8d6a3412c8860865": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5a4922e4eec54e6190b1c9b0826b3e09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "AccordionModel",
      "state": {
       "children": [
        "IPY_MODEL_4ff3854979484c5998be54da717f0589",
        "IPY_MODEL_64d43da073c84fabb7f380b9b3d49eb6"
       ],
       "layout": "IPY_MODEL_e244c622f9e24deeaadfe812611e8a03",
       "selected_index": 0,
       "titles": [
        "2. retrieve_saved_tasks_in_db DATABASE ACCESS 14:34:53",
        "1. retrieve_saved_tasks_in_db DATABASE ACCESS 14:34:52"
       ]
      }
     },
     "5ccaae5f48f248eaaff7355e8d5b53d8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5e1147a0ab6c4159801ea17edbb71e5f": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_a0986bad0523418093de56c6bfce4ca7",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "\u001b[32m****CodingAgent>code_task_and_run_test calling HumanLLMMonitor****\u001b[0m\n"
        }
       ]
      }
     },
     "60b19433b9fd4604bb75bd421f0e82b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "AccordionModel",
      "state": {
       "children": [
        "IPY_MODEL_fdc5b51a94f142acbcd5e4b2b78036d8",
        "IPY_MODEL_43cabc6dc19e4325af7970b65ee4004f"
       ],
       "layout": "IPY_MODEL_126269a864cc4918991f64bdd4a1be9e",
       "selected_index": 0,
       "titles": [
        "2. orchestrate_agents RESULT 14:35:21",
        "1. orchestrate_agents WARNING 14:35:21"
       ]
      }
     },
     "64d43da073c84fabb7f380b9b3d49eb6": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_481b4dea5fb24090962de2f023702fc2",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "************ Retrieving successful tasks from database - LIST:\n"
        }
       ]
      }
     },
     "6c2d2427c47c4c16b1a30131bf2853d4": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_b9d1761ed4c54b2e8b0da8834ec5da6b",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "\u001b[32m***** CodingAgent->code_task_and_run_test AFTER *****\nLLM ANSWER:\nReasoning: To extract key concepts and topics from a research paper's abstract, we can follow the plan by first identifying key sentences in the abstract and then extracting key concepts and topics from these sentences using natural language processing tools.\n\nCode:\n```python\nfrom transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\nimport spacy\n\ndef extract_key_concepts_and_topics(bot, document_id):\n    # Get the abstract of the document\n    abstract = bot.get_document_abstract(document_id)\n\n    # Tokenize the abstract into sentences\n    nlp = spacy.load(\"en_core_web_sm\")\n    doc = nlp(abstract)\n    sentences = [sent.text for sent in doc.sents]\n\n    # Use a method to determine the importance of each sentence based on its content and relevance to the overall abstract\n    # Here, we can use a pre-trained model for text summarization to identify important sentences\n    summarization_pipeline = pipeline(\"summarization\")\n    summarized_abstract = summarization_pipeline(abstract, max_length=100, min_length=30, do_sample=False)[0]['summary_text']\n\n    # Utilize named entity recognition to identify important entities such as specific technologies, methods, or domains\n    ner_pipeline = pipeline(\"ner\")\n    named_entities = ner_pipeline(abstract)\n\n    # Apply topic modeling techniques to extract prevalent themes and topics from the key sentences\n    # Here, we can use a pre-trained model for token classification to extract topics from the abstract\n    tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n    model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n    topic_modeling_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n    topics = topic_modeling_pipeline(abstract)\n\n    # Store the extracted key concepts and topics in the document's resources\n    bot.add_or_update_result_in_resources(\n        {\n            \"document_id\": document_id,\n            \"key_sentences\": sentences,\n            \"summarized_abstract\": summarized_abstract,\n            \"named_entities\": named_entities,\n            \"topics\": topics\n        },\n        name=\"key_concepts_and_topics\"\n    )\n\n    # Log an event for the successful extraction of key concepts and topics\n    bot.add_event(\"Key concepts and topics extracted\", {\"document_id\": document_id})\n```\n***** CodingAgent->code_task_and_run_test AFTER *****\u001b[0m\nA. Manually set/modify the answer/output (I don't want to try to improve agent's system prompt).\nB. Critic this answer/output to get an improved answer/output.\nC. Find a better Prompt by providing critic and ideal answer.\nD. Evaluate & comment answer (Score between 0(worst)-1(top), and explain) to improve future results by using scored/commented examples.\nE. Go back BEFORE inference to improve system prompt or add information to user message.\nG. Skip human actions for N rounds.\nH. Exit program.\n\n"
        }
       ]
      }
     },
     "6f590c5a2d024aa8959df5b63d7b75d3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "762c620464a8475f94ebf3fb12b45428": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_fdcbe050e7b9413ea364ee4fafb5e2a3",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "\u001b[35m***** TaskIdentificationAgent->identify_best_task  BEFORE *****\nSYSTEM PROMPT:\nYou are a research assistant that progressively learns best tasks to be able to quickly produce high quality technical synthesis (e.g. state-of-the-art research survey paper, Wikipedia like article, patent) given a [Title] and an [Abstract].\nEach task you propose will be prompted to a language model to convert it into a Python code, if the code is successful and information gain sufficient, this learnt task is made available to next learning iteration. You should keep this in mind to propose adressable task.\n\nI will provide you:\n- Learnt tasks available (with information gain between 0 (minimum) and 1 (maximum) on plan's titles, and contents): ...\n- Failed tasks to learn that are too hard: ...\n- Current status of examples of technical synthesis the proposed next task will be tested on: ...\n\nYou should tell me the next best novel task you should learn given your internal knowledge and available learnt tasks and basic commands to maximize technical quality and speed to produce a complete technical synthesis.\n\nYou must follow the criteria below:\n1) Reason step by step to find out best task. You cannot propose a task which will not add or improve titles and contents because, task will be rejected. \n2) Task should be written in the form of \"[verb] [quantity] [object] [tools] [detailed instructions and parameters]\" - ie. ’verb’  is the verb of this action, ’object’ refers to the target object of the action, ’tools’ specifies the tools required for the action, ’detailed instructions and parameters’ are all important instructions and parameters to be detailed additional to the other information to describe the goal.\n3) Task shouldn’t be too difficult to convert into Python code given available commands and learnt tasks.\n4) Task should be novel compared to learnt and failed tasks.\n5) You should then propose a plan to achieve this task by breaking it down as a tree-structure. The plan tree should be exactly of depth 2. You should index the two levels like ’1.’, ’1.1.’, ’1.2.’, ’2.’, ’2.1.’, etc. The sub-tasks at the bottom level should use basic commands or learnt tasks and logic that could be easily converted to python code.\n6) Tasks provided should not be specific to examples so the reasoning can mention examples but proposed task and plan should not mention any information related to examples\n7) After proposing the task, you should provide a test of the function corresponding to this task:\n    a) Write a one liner python call to the main function for each \"Document to be tested\", this call should be designed to maximize the expected results for the \"Document to be tested\"\n    b) Precede each one liner call with a line of comment in this form \"# document #uuid usage test\" (e.g. \"#document #125dc4bc-54e0-4336-82bc-417e40ec9b8f usage test\"...) to indicate to which document the code of the next line applies to given its unique id\n    c) Call to the main function uses bot as first required parameter, then provide parameters sepecific to the document for this function\n    d) No more than one test for each document, the total number of function calls in this test list should be equal to the number of \"Document to be tested\"\n\nYou should only respond in the format as described below:\nRESPONSE FORMAT:\n\n1. Reasoning: Based on the information listed above, do reasoning about what the next task should be.\n\n2. Task: Next best task to learn.\n\n3. Plan: Tree-structured plan of depth 2 breaking-down next best task into logic and actions\n\n4. Tests:\n```python\n# document #125dc4bc-54e0-4336-82bc-417e40ec9b8f usage test:\ntask_function_name(bot, keyword arguments with value specific to document #125dc4bc-54e0-4336-82bc-417e40ec9b8f for the given task...)\n# document #2fa754cb-2e90-3376-3b2c-142f29c9ebf8 usage test:\ntask_function_name(bot, keyword arguments with value specific to document #2fa754cb-2e90-3376-3b2c-142f29c9ebf8 for the given task...)\n...\n```\n\nUSER MESSAGE:\n- Already developed tasks: None\n- Already failed tasks (too hard): None\n- Current status of examples on which the task will be tested on: <<< Document #cf0d353c-b43b-4a79-88f9-42c2c84cf75e properties:\n1. title: Complex QA and language models hybrid architectures, Survey\n2. abstract: This paper reviews the state-of-the-art of language models architectures and strategies for 'complex' question-answering (QA, CQA, CPS) with a focus on hybridization. Large Language Models (LLM) are good at leveraging public data on standard problems but once you want to tackle more specific complex questions or problems (e.g. How does the concept of personal freedom vary between different cultures ? What is the best mix of power generation methods to reduce climate change ?) you may need specific architecture, knowledge, skills, methods, sensitive data protection, explainability, human approval and versatile feedback... Recent projects like ChatGPT and GALACTICA have allowed non-specialists to grasp the great potential as well as the equally strong limitations of LLM in complex QA. In this paper, we start by reviewing required skills and evaluation techniques. We integrate findings from the robust community edited research papers BIG, BLOOM and HELM which open source, benchmark and analyze limits and challenges of LLM in terms of tasks complexity and strict evaluation on accuracy (e.g. fairness, robustness, toxicity, ...) as a baseline. We discuss some challenges associated with complex QA, including domain adaptation, decomposition and efficient multi-step QA, long form and non-factoid QA, safety and multi-sensitivity data protection, multimodal search, hallucinations, explainability and truthfulness, temporal reasoning. We analyze current solutions and promising research trends, using elements such as: hybrid LLM architectural patterns, training and prompting strategies, active human reinforcement learning supervised with AI, neuro-symbolic and structured knowledge grounding, program synthesis, iterated decomposition and others.\n3. current table of content: Empty\n4. current resources: Empty\n>>>\n<<< Document #42252c6c-12f3-4edf-9045-8acd69bc3356 properties:\n1. title: Macroeconomic Effects of Inflation Targeting A Survey of the Empirical  Literature\n2. abstract: This paper surveys the empirical literature of inflation targeting. The main findings from our review are the following: there is robust empirical evidence that larger and more developed countries are more likely to adopt the IT regime; the introduction of this regime is conditional on previous disinflation, greater exchange rate flexibility, central bank independence, and higher level of financial development; the empirical evidence has failed to provide convincing evidence that IT itself may serve as an effective tool for stabilizing inflation expectations and for reducing inflation persistence; the empirical research focused on advanced economies has failed to provide convincing evidence on the beneficial effects of IT on inflation performance, while there is some evidence that the gains from the IT regime may have been more prevalent in the emerging market economies; there is not convincing evidence that IT is associated with either higher output growth or lower output variability; the empirical research suggests that IT may have differential effects on exchange-rate volatility in advanced economies versus EMEs; although the empirical evidence on the impact of IT on fiscal policy is quite limited, it supports the idea that IT indeed improves fiscal discipline; the empirical support to the proposition that IT is associated with lower disinflation costs seems to be rather weak. Therefore, the accumulated empirical literature implies that IT does not produce superior macroeconomic benefits in comparison with the alternative monetary strategies or, at most, they are quite modest.\n3. current table of content: Empty\n4. current resources: Empty\n>>>\n\n***** TaskIdentificationAgent->identify_best_task BEFORE *****\u001b[0m\nA. Modify agent's 'role' / 'system prompt' (role, global context, constraints, examples).\nB. Add instruction or information to agent.\nC. Skip and set LLM output from recent outputs or manually define it.\nD. Log comments (not used by the model, just for information).\nE. See all previous results for this agent.\nF. See previous MODIFIED/SCORED/COMMENTED results for this agent.\nG. Skip human actions for N rounds.\nH. Exit program.\nJ. Change num of parallel inferences - Current value=1\nP. Proceed to inference using a PREMIUM LLM.\n\n"
        }
       ]
      }
     },
     "8357a30ddd4d4f5683dbd4af8992c08a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "continuous_update": false,
       "description": "File name:",
       "layout": "IPY_MODEL_b9492ab446e147d1841fa5f02d7d62b4",
       "style": "IPY_MODEL_863f9957e75c4925bb0fdf50c32a642c",
       "value": "experiment_240110_1434.html"
      }
     },
     "863f9957e75c4925bb0fdf50c32a642c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "89806447329f413494b3a1a64c5aa079": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8b24dfffb7174c11ad897af0f8db02de": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8d7ded5a998644f5827030bdf53c277c": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_1ca598b747b54c71b6309ff3e1efb4f2",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "ImportFrom node: IMPORT SHOULD BE DONE INSIDE FUNCTIONS !!!\n"
        }
       ]
      }
     },
     "9883787c07b74d438042d9b108923eac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "99e18ee30fe648b8a0ac70b4f2d197bf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a0986bad0523418093de56c6bfce4ca7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a1cfa7f68aac42419d7b9cd3633eddf8": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_8b24dfffb7174c11ad897af0f8db02de",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "ImportFrom node: IMPORT SHOULD BE DONE INSIDE FUNCTIONS !!!\n"
        }
       ]
      }
     },
     "a60b0c4397854e3d8244ee62def30ec0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "AccordionModel",
      "state": {
       "children": [
        "IPY_MODEL_ad470595208b4e8b9e1a8ecef6598698",
        "IPY_MODEL_4a91fc9e329f400297ef05e7aca103db"
       ],
       "layout": "IPY_MODEL_0c0ca7ef06b048fbb0797cd6a9aa3226",
       "selected_index": 0,
       "titles": [
        "2. BEFORE inference action MENU 14:37:01",
        "1. HumanLLMMonitor 14:37:01"
       ]
      }
     },
     "a6d08648739647b3801951e0877c7905": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_27dea91467a0471499054d725fe41de5",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "\n1. Reasoning: Considering the provided examples, it seems that a valuable task to learn would be to extract key concepts and topics from a research paper's abstract. This task would involve identifying the main ideas and topics discussed in the abstract, which could be useful for creating a table of contents or for providing a quick overview of the paper's content.\n\n2. Task: Extract key concepts and topics from a research paper's abstract using natural language processing tools.\n\n3. Plan:\n    1. Identify key sentences in the abstract that convey important concepts and topics.\n        1. Use natural language processing tools to tokenize the abstract into sentences.\n        2. Apply a method to determine the importance of each sentence based on its content and relevance to the overall abstract.\n    2. Extract key concepts and topics from the identified key sentences.\n        1. Utilize named entity recognition to identify important entities such as specific technologies, methods, or domains.\n        2. Apply topic modeling techniques to extract prevalent themes and topics from the key sentences.\n\n4. Tests:\n```python\n# document #cf0d353c-b43b-4a79-88f9-42c2c84cf75e usage test:\nextract_key_concepts_and_topics(bot, document_id=\"cf0d353c-b43b-4a79-88f9-42c2c84cf75e\")\n# document #42252c6c-12f3-4edf-9045-8acd69bc3356 usage test:\nextract_key_concepts_and_topics(bot, document_id=\"42252c6c-12f3-4edf-9045-8acd69bc3356\")\n```"
        }
       ]
      }
     },
     "acb22bb07e764d02ba6eb86c8b4090ca": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_5ccaae5f48f248eaaff7355e8d5b53d8",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "\u001b[91mTODO: add support to include best code from successful tasks from CapitalizationAgent self.db_successful_tasks and self.db_failed_tasks\u001b[0m\n"
        }
       ]
      }
     },
     "ad470595208b4e8b9e1a8ecef6598698": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_f147e169506249ffb12eb6426366d531",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "\u001b[31m***** ValidationAgent->validate_code  BEFORE *****\nSYSTEM PROMPT:\nYou are a Python expert and domain expert in the field of the task, you should validate the Python code provided and its result regarding the code implementing the task and feedback.\n\nI will provide you:\nTASK: {task}\nCODE: {code}\nSome additional information to evaluate code: {runtime_errors}\nExecution result returned by exec command of code provided: {exec_result}\nHuman evaluation of the function result: {human_evaluation}\n\nRESPONSE FORMAT: you should only respond in the format as described below:\nReasoning: Based on the information I listed above, do reasoning to evaluate if the code implementation and execution is aligned with the task goal to decide if it is a success.\nSuccess: write \"True\" if code is a success, \"False\" otherwise\nExplain: explain in detail your evaluation of your success evaluation.\n\nEXAMPLES:\nReasoning: The initial task was to list all GPS points of vessels in the zone. The code is aligned with this task, it ran without errors, your confirmed what I could not check.\nSuccess: \"True\"\nExplain: code generated result expected by task without errors.\n\nUSER MESSAGE:\nTask: 1. Reasoning: Considering the provided examples, it seems that a valuable task to learn would be to extract key concepts and topics from a research paper's abstract. This task would involve identifying the main ideas and topics discussed in the abstract, which could be useful for creating a table of contents or for providing a quick overview of the paper's content.\n\n2. Task: Extract key concepts and topics from a research paper's abstract using natural language processing tools.\n\n3. Plan:\n    1. Identify key sentences in the abstract that convey important concepts and topics.\n        1. Use natural language processing tools to tokenize the abstract into sentences.\n        2. Apply a method to determine the importance of each sentence based on its content and relevance to the overall abstract.\n    2. Extract key concepts and topics from the identified key sentences.\n        1. Utilize named entity recognition to identify important entities such as specific technologies, methods, or domains.\n        2. Apply topic modeling techniques to extract prevalent themes and topics from the key sentences.\n\n4. Tests:\n```python\n# document #cf0d353c-b43b-4a79-88f9-42c2c84cf75e usage test:\nextract_key_concepts_and_topics(bot, document_id=\"cf0d353c-b43b-4a79-88f9-42c2c84cf75e\")\n# document #42252c6c-12f3-4edf-9045-8acd69bc3356 usage test:\nextract_key_concepts_and_topics(bot, document_id=\"42252c6c-12f3-4edf-9045-8acd69bc3356\")\n```\n\nCode: from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\nimport spacy\ndef extract_key_concepts_and_topics(bot, document_id):\n    # Get the abstract of the document\n    abstract = bot.get_document_abstract(document_id)\n\n    # Tokenize the abstract into sentences\n    nlp = spacy.load(\"en_core_web_sm\")\n    doc = nlp(abstract)\n    sentences = [sent.text for sent in doc.sents]\n\n    # Use a method to determine the importance of each sentence based on its content and relevance to the overall abstract\n    # Here, we can use a pre-trained model for text summarization to identify important sentences\n    summarization_pipeline = pipeline(\"summarization\")\n    summarized_abstract = summarization_pipeline(abstract, max_length=100, min_length=30, do_sample=False)[0]['summary_text']\n\n    # Utilize named entity recognition to identify important entities such as specific technologies, methods, or domains\n    ner_pipeline = pipeline(\"ner\")\n    named_entities = ner_pipeline(abstract)\n\n    # Apply topic modeling techniques to extract prevalent themes and topics from the key sentences\n    # Here, we can use a pre-trained model for token classification to extract topics from the abstract\n    tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n    model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n    topic_modeling_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n    topics = topic_modeling_pipeline(abstract)\n\n    # Store the extracted key concepts and topics in the document's resources\n    bot.add_or_update_result_in_resources(\n        {\n            \"document_id\": document_id,\n            \"key_sentences\": sentences,\n            \"summarized_abstract\": summarized_abstract,\n            \"named_entities\": named_entities,\n            \"topics\": topics\n        },\n        name=\"key_concepts_and_topics\"\n    )\n\n    # Log an event for the successful extraction of key concepts and topics\n    bot.add_event(\"Key concepts and topics extracted\", {\"document_id\": document_id})\n\nCode execution returned: runtime errors at execution\n\nExecution result returned by exec command of code provided: ['Failed to execute provided code. Error: No module named \\'transformers\\' Traceback: Traceback (most recent call last):\\n  File \"/home/xavier/CollabFunctionsGPTCreator/learn.py\", line 59, in step\\n    exec(action_code+helper, context)\\n  File \"<string>\", line 78, in <module>\\nModuleNotFoundError: No module named \\'transformers\\'\\n', 'Failed to execute provided code. Error: No module named \\'transformers\\' Traceback: Traceback (most recent call last):\\n  File \"/home/xavier/CollabFunctionsGPTCreator/learn.py\", line 59, in step\\n    exec(action_code+helper, context)\\n  File \"<string>\", line 78, in <module>\\nModuleNotFoundError: No module named \\'transformers\\'\\n']\n\nNew environment status of examples on which the task has been tested on: <<< Document #cf0d353c-b43b-4a79-88f9-42c2c84cf75e properties:\n1. title: Complex QA and language models hybrid architectures, Survey\n2. abstract: This paper reviews the state-of-the-art of language models architectures and strategies for 'complex' question-answering (QA, CQA, CPS) with a focus on hybridization. Large Language Models (LLM) are good at leveraging public data on standard problems but once you want to tackle more specific complex questions or problems (e.g. How does the concept of personal freedom vary between different cultures ? What is the best mix of power generation methods to reduce climate change ?) you may need specific architecture, knowledge, skills, methods, sensitive data protection, explainability, human approval and versatile feedback... Recent projects like ChatGPT and GALACTICA have allowed non-specialists to grasp the great potential as well as the equally strong limitations of LLM in complex QA. In this paper, we start by reviewing required skills and evaluation techniques. We integrate findings from the robust community edited research papers BIG, BLOOM and HELM which open source, benchmark and analyze limits and challenges of LLM in terms of tasks complexity and strict evaluation on accuracy (e.g. fairness, robustness, toxicity, ...) as a baseline. We discuss some challenges associated with complex QA, including domain adaptation, decomposition and efficient multi-step QA, long form and non-factoid QA, safety and multi-sensitivity data protection, multimodal search, hallucinations, explainability and truthfulness, temporal reasoning. We analyze current solutions and promising research trends, using elements such as: hybrid LLM architectural patterns, training and prompting strategies, active human reinforcement learning supervised with AI, neuro-symbolic and structured knowledge grounding, program synthesis, iterated decomposition and others.\n3. current table of content: Empty\n4. current resources: Empty\n5. sections titles progress: 0.0\n6. sections content progress: 0.0\n7. events counted: Empty\n>>>\n<<< Document #42252c6c-12f3-4edf-9045-8acd69bc3356 properties:\n1. title: Macroeconomic Effects of Inflation Targeting A Survey of the Empirical  Literature\n2. abstract: This paper surveys the empirical literature of inflation targeting. The main findings from our review are the following: there is robust empirical evidence that larger and more developed countries are more likely to adopt the IT regime; the introduction of this regime is conditional on previous disinflation, greater exchange rate flexibility, central bank independence, and higher level of financial development; the empirical evidence has failed to provide convincing evidence that IT itself may serve as an effective tool for stabilizing inflation expectations and for reducing inflation persistence; the empirical research focused on advanced economies has failed to provide convincing evidence on the beneficial effects of IT on inflation performance, while there is some evidence that the gains from the IT regime may have been more prevalent in the emerging market economies; there is not convincing evidence that IT is associated with either higher output growth or lower output variability; the empirical research suggests that IT may have differential effects on exchange-rate volatility in advanced economies versus EMEs; although the empirical evidence on the impact of IT on fiscal policy is quite limited, it supports the idea that IT indeed improves fiscal discipline; the empirical support to the proposition that IT is associated with lower disinflation costs seems to be rather weak. Therefore, the accumulated empirical literature implies that IT does not produce superior macroeconomic benefits in comparison with the alternative monetary strategies or, at most, they are quite modest.\n3. current table of content: Empty\n4. current resources: Empty\n5. sections titles progress: 0.0\n6. sections content progress: 0.0\n7. events counted: Empty\n>>>\n\n***** ValidationAgent->validate_code BEFORE *****\u001b[0m\nA. Modify agent's 'role' / 'system prompt' (role, global context, constraints, examples).\nB. Add instruction or information to agent.\nC. Skip and set LLM output from recent outputs or manually define it.\nD. Log comments (not used by the model, just for information).\nE. See all previous results for this agent.\nF. See previous MODIFIED/SCORED/COMMENTED results for this agent.\nG. Skip human actions for N rounds.\nH. Exit program.\nJ. Change num of parallel inferences - Current value=1\nP. Proceed to inference using a PREMIUM LLM.\n\n"
        }
       ]
      }
     },
     "b1dff02b4010476bb6d90f9b665755d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DropdownModel",
      "state": {
       "description": "Select Task:",
       "index": null,
       "layout": "IPY_MODEL_89806447329f413494b3a1a64c5aa079",
       "style": "IPY_MODEL_29ee81a4c50f407f956f8043c26812af"
      }
     },
     "b9492ab446e147d1841fa5f02d7d62b4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b9d1761ed4c54b2e8b0da8834ec5da6b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c112d62c15a14c97ba286dae18c49d47": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "continuous_update": false,
       "description": "Path:",
       "layout": "IPY_MODEL_02c737251231468b8a14a3fa9161856c",
       "style": "IPY_MODEL_566e83dcedf242dc8d6a3412c8860865",
       "value": "./"
      }
     },
     "c3ad212856e74e508868aa601f834483": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TabModel",
      "state": {
       "children": [
        "IPY_MODEL_5a4922e4eec54e6190b1c9b0826b3e09",
        "IPY_MODEL_08ca0bbf973244e28a7fa9ba7e83942c",
        "IPY_MODEL_60b19433b9fd4604bb75bd421f0e82b0",
        "IPY_MODEL_fbdad4aae1424086a728682ddfb43c45",
        "IPY_MODEL_a60b0c4397854e3d8244ee62def30ec0"
       ],
       "layout": "IPY_MODEL_3430d3d79cc14648b5db4ea019f08b4e",
       "selected_index": 4,
       "titles": [
        "CapitalizationAgent",
        "****TaskIdentificationAgent****",
        "***orchestrate_agents***",
        "**CodingAgent**",
        "*ValidationAgent*"
       ]
      }
     },
     "c6f126e3b0c94b549ddaa195591a7d24": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cf18b742a8104f80a74866b43aa66205": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_c6f126e3b0c94b549ddaa195591a7d24",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "************ Code parsed result************\n{'program_code': 'from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\nimport spacy\ndef extract_key_concepts_and_topics(bot, document_id):\n    # Get the abstract of the document\n    abstract = bot.get_document_abstract(document_id)\n\n    # Tokenize the abstract into sentences\n    nlp = spacy.load(\"en_core_web_sm\")\n    doc = nlp(abstract)\n    sentences = [sent.text for sent in doc.sents]\n\n    # Use a method to determine the importance of each sentence based on its content and relevance to the overall abstract\n    # Here, we can use a pre-trained model for text summarization to identify important sentences\n    summarization_pipeline = pipeline(\"summarization\")\n    summarized_abstract = summarization_pipeline(abstract, max_length=100, min_length=30, do_sample=False)[0][\\'summary_text\\']\n\n    # Utilize named entity recognition to identify important entities such as specific technologies, methods, or domains\n    ner_pipeline = pipeline(\"ner\")\n    named_entities = ner_pipeline(abstract)\n\n    # Apply topic modeling techniques to extract prevalent themes and topics from the key sentences\n    # Here, we can use a pre-trained model for token classification to extract topics from the abstract\n    tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n    model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n    topic_modeling_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n    topics = topic_modeling_pipeline(abstract)\n\n    # Store the extracted key concepts and topics in the document\\'s resources\n    bot.add_or_update_result_in_resources(\n        {\n            \"document_id\": document_id,\n            \"key_sentences\": sentences,\n            \"summarized_abstract\": summarized_abstract,\n            \"named_entities\": named_entities,\n            \"topics\": topics\n        },\n        name=\"key_concepts_and_topics\"\n    )\n\n    # Log an event for the successful extraction of key concepts and topics\n    bot.add_event(\"Key concepts and topics extracted\", {\"document_id\": document_id})', 'main_function_name': 'extract_key_concepts_and_topics', 'runnable_code': '', 'tests': [('cf0d353c-b43b-4a79-88f9-42c2c84cf75e', 'extract_key_concepts_and_topics(bot, document_id=\"cf0d353c-b43b-4a79-88f9-42c2c84cf75e\")'), ('42252c6c-12f3-4edf-9045-8acd69bc3356', 'extract_key_concepts_and_topics(bot, document_id=\"42252c6c-12f3-4edf-9045-8acd69bc3356\")')]}\n************************\n"
        }
       ]
      }
     },
     "d514b81c13d2445e9a7f42dff37cfd57": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_8357a30ddd4d4f5683dbd4af8992c08a",
        "IPY_MODEL_c112d62c15a14c97ba286dae18c49d47",
        "IPY_MODEL_e3aefc86989f46cd835cbab6d0d28522"
       ],
       "layout": "IPY_MODEL_397b407fa56b4d06a1a275740daeb7e5"
      }
     },
     "d9470ec4766042688cdbda2dee7f3220": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e244c622f9e24deeaadfe812611e8a03": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e2a75c1a334a462da94cdfa7e8b70b75": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e3aefc86989f46cd835cbab6d0d28522": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Export to HTML",
       "layout": "IPY_MODEL_d9470ec4766042688cdbda2dee7f3220",
       "style": "IPY_MODEL_3b0df72295e7424990a9e94b1066137a",
       "tooltip": null
      }
     },
     "f147e169506249ffb12eb6426366d531": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f7953a4fa546461da6fde1c7ef794a67": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f8558a281921445a8aa434acdca06108": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fa4fa550088c4cbbbd16cb0a62c9534c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fb8215c803ea4ade9ca2dc923e6f75f8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fbdad4aae1424086a728682ddfb43c45": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "AccordionModel",
      "state": {
       "children": [
        "IPY_MODEL_cf18b742a8104f80a74866b43aa66205",
        "IPY_MODEL_8d7ded5a998644f5827030bdf53c277c",
        "IPY_MODEL_a1cfa7f68aac42419d7b9cd3633eddf8",
        "IPY_MODEL_6c2d2427c47c4c16b1a30131bf2853d4",
        "IPY_MODEL_47ae4c2ca3dd435eba3ecdb68ac53f67",
        "IPY_MODEL_51049a4f8212430f824338fefaf127e4",
        "IPY_MODEL_5e1147a0ab6c4159801ea17edbb71e5f",
        "IPY_MODEL_acb22bb07e764d02ba6eb86c8b4090ca"
       ],
       "layout": "IPY_MODEL_f7953a4fa546461da6fde1c7ef794a67",
       "selected_index": 0,
       "titles": [
        "8. code_task_and_run_test RESULT 14:37:01",
        "7. process_ai_generated_code SystemMessage 14:37:01",
        "6. process_ai_generated_code SystemMessage 14:37:01",
        "5. AFTER inference action MENU 14:35:37",
        "4. NEW inference result recieved 14:35:37",
        "3. BEFORE inference action MENU 14:35:21",
        "2. HumanLLMMonitor 14:35:21",
        "1. code_task_and_run_test SystemMessage 14:35:21"
       ]
      }
     },
     "fc06904198e6472697b5d18b9b2c68f4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fdc5b51a94f142acbcd5e4b2b78036d8": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_f8558a281921445a8aa434acdca06108",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "Identified Task: 1. Reasoning: Considering the provided examples, it seems that a valuable task to learn would be to extract key concepts and topics from a research paper's abstract. This task would involve identifying the main ideas and topics discussed in the abstract, which could be useful for creating a table of contents or for providing a quick overview of the paper's content.\n\n2. Task: Extract key concepts and topics from a research paper's abstract using natural language processing tools.\n\n3. Plan:\n    1. Identify key sentences in the abstract that convey important concepts and topics.\n        1. Use natural language processing tools to tokenize the abstract into sentences.\n        2. Apply a method to determine the importance of each sentence based on its content and relevance to the overall abstract.\n    2. Extract key concepts and topics from the identified key sentences.\n        1. Utilize named entity recognition to identify important entities such as specific technologies, methods, or domains.\n        2. Apply topic modeling techniques to extract prevalent themes and topics from the key sentences.\n\n4. Tests:\n```python\n# document #cf0d353c-b43b-4a79-88f9-42c2c84cf75e usage test:\nextract_key_concepts_and_topics(bot, document_id=\"cf0d353c-b43b-4a79-88f9-42c2c84cf75e\")\n# document #42252c6c-12f3-4edf-9045-8acd69bc3356 usage test:\nextract_key_concepts_and_topics(bot, document_id=\"42252c6c-12f3-4edf-9045-8acd69bc3356\")\n```\n"
        }
       ]
      }
     },
     "fdcbe050e7b9413ea364ee4fafb5e2a3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
