{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import json\n",
    "from copy import copy, deepcopy\n",
    "from pprint import pprint\n",
    "from typing import List\n",
    "from uuid import uuid4\n",
    "\n",
    "import evaluate\n",
    "import openai\n",
    "import requests\n",
    "import tiktoken\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from doctran import Doctran, ExtractProperty\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from evaluate import load\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.embeddings import (\n",
    "    OpenAIEmbeddings,\n",
    "    HuggingFaceEmbeddings,\n",
    ")\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.text_splitter import (\n",
    "    MarkdownTextSplitter,\n",
    "    MarkdownHeaderTextSplitter,\n",
    "    LineType,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "from loguru import logger\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "from main import (\n",
    "    extract_plan_and_content_wikipedia,\n",
    "    compare_documents_content,\n",
    "    compare_documents_sections,\n",
    ")\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "llm_default = ChatOpenAI(model_name=\"gpt-3.5-turbo\", streaming=True)\n",
    "llm_16k = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\", streaming=True)\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str = \"gpt-3.5-turbo\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.get_encoding(encoding_name)\n",
    "    except ValueError:\n",
    "        encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def convert_to_markdown(article_dict):\n",
    "    md_text = \"\"\n",
    "\n",
    "    for heading, content in article_dict.items():\n",
    "        # heading is of form: 'h3 Example'\n",
    "        # Define the markdown equivalent for the heading level\n",
    "        heading_level = \"#\" * int(heading[1])\n",
    "        heading = heading[3:]\n",
    "        # Append the heading and the content to the markdown text\n",
    "        md_text += f\"{heading_level} {heading}\\n\\n{content}\\n\\n\"\n",
    "\n",
    "    return md_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "2 2\n",
      "3 3\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "b = [1, 2, 3, 4, 5, 6, 7]\n",
    "for a_, b_ in zip(b, a):\n",
    "    print(a_, b_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-08-16 14:12:19.870\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mextract_plan_and_content\u001b[0m:\u001b[36m761\u001b[0m - \u001b[1m\n",
      "\tExtracting plan and content for: https://en.wikipedia.org/wiki/Simulated_annealing\u001b[0m\n",
      "\u001b[32m2023-08-16 14:12:20.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mload_wikipedia_url\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mSuccessfully downloaded content from Wikipedia page https://en.wikipedia.org/wiki/Simulated_annealing. Extracted 19 sections.\u001b[0m\n",
      "\u001b[32m2023-08-16 14:12:20.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mdivide_sections_if_too_large\u001b[0m:\u001b[36m294\u001b[0m - \u001b[1mDividing sections if too large in plan and section content.\u001b[0m\n",
      "\u001b[32m2023-08-16 14:12:24.346\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mdivide_sections_if_too_large\u001b[0m:\u001b[36m349\u001b[0m - \u001b[1mAdded 'h1 Simulated Annealing' split original heading 'h1 Simulated annealing'\u001b[0m\n",
      "\u001b[32m2023-08-16 14:12:25.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mdivide_sections_if_too_large\u001b[0m:\u001b[36m349\u001b[0m - \u001b[1mAdded 'h1 Simulated Annealing Algorithm' split original heading 'h1 Simulated annealing'\u001b[0m\n",
      "\u001b[32m2023-08-16 14:12:26.388\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mdivide_sections_if_too_large\u001b[0m:\u001b[36m349\u001b[0m - \u001b[1mAdded 'h3 Simulated Annealing' split original heading 'h3 Acceptance probabilities'\u001b[0m\n",
      "\u001b[32m2023-08-16 14:12:27.283\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mdivide_sections_if_too_large\u001b[0m:\u001b[36m349\u001b[0m - \u001b[1mAdded 'h3 Simulated Annealing' split original heading 'h3 Acceptance probabilities'\u001b[0m\n",
      "\u001b[32m2023-08-16 14:12:27.294\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mdivide_sections_if_too_large\u001b[0m:\u001b[36m334\u001b[0m - \u001b[1mAdded 'h2 References 1' split original heading 'h2 References'\u001b[0m\n",
      "\u001b[32m2023-08-16 14:12:27.294\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mdivide_sections_if_too_large\u001b[0m:\u001b[36m334\u001b[0m - \u001b[1mAdded 'h2 References 2' split original heading 'h2 References'\u001b[0m\n",
      "\u001b[32m2023-08-16 14:12:27.295\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mdivide_sections_if_too_large\u001b[0m:\u001b[36m334\u001b[0m - \u001b[1mAdded 'h2 References 3' split original heading 'h2 References'\u001b[0m\n",
      "\u001b[32m2023-08-16 14:12:27.295\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mdivide_sections_if_too_large\u001b[0m:\u001b[36m334\u001b[0m - \u001b[1mAdded 'h2 References 4' split original heading 'h2 References'\u001b[0m\n",
      "\u001b[32m2023-08-16 14:12:27.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mdivide_sections_if_too_large\u001b[0m:\u001b[36m355\u001b[0m - \u001b[1m\n",
      "\tFinished dividing sections if too large in plan and section content.\n",
      "\tStarted with 19 sections and got 23 final sections.\n",
      "\tThat's a 1.21x increase in sections\u001b[0m\n",
      "\u001b[32m2023-08-16 14:12:27.297\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mgenerate_embeddings_plan_and_section_content\u001b[0m:\u001b[36m444\u001b[0m - \u001b[1mCreating plan json\u001b[0m\n",
      "\u001b[32m2023-08-16 14:12:27.297\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mgenerate_embeddings_plan_and_section_content\u001b[0m:\u001b[36m478\u001b[0m - \u001b[1mTitle: Simulated Annealing\u001b[0m\n",
      "\u001b[32m2023-08-16 14:12:27.298\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mgenerate_embeddings_plan_and_section_content\u001b[0m:\u001b[36m479\u001b[0m - \u001b[1mAbstract: Simulated annealing (SA) is a probabilistic technique for approximating the global optimum of a given function. Specifically, it is a metaheuristic to approximate global optimization in a large search space for an optimization problem. For large numbers of local optima, SA can find the global optima.[1] It is often used when the search space is discrete (for example the traveling salesman problem, the boolean satisfiability problem, protein structure prediction, and job-shop scheduling). For problems where finding an approximate global optimum is more important than finding a precise local optimum in a fixed amount of time, simulated annealing may be preferable to exact algorithms such as gradient descent or branch and bound. \n",
      " The name of the algorithm comes from annealing in metallurgy, a technique involving heating and controlled cooling of a material to alter its physical properties. Both are attributes of the material that depend on their thermodynamic free energy. Heating and cooling the material affects both the temperature and the thermodynamic free energy or Gibbs energy.\n",
      "Simulated annealing can be used for very hard computational optimization problems where exact algorithms fail; even though it usually achieves an approximate solution to the global minimum, it could be enough for many practical problems.\n",
      " The problems solved by SA are currently formulated by an objective function of many variables, subject to several mathematical constraints. In practice, the constraint can be penalized as part of the objective function.\n",
      " Similar techniques have been independently introduced on several occasions, including Pincus (1970),[2] Khachaturyan et al (1979,[3] 1981[4]), Kirkpatrick, Gelatt and Vecchi (1983), and Cerny (1985).[5] In 1983, this approach was used by Kirkpatrick, Gelatt Jr., Vecchi,[6] for a solution of the traveling salesman problem. They also proposed its current name, simulated annealing.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1a9a5e39034bc3ba457f4a053fd392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)dc087/.gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7194a88eee460692a5ec9a105f0057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203860a025d44c429200c3acc91a7295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)c3826dc087/README.md:   0%|          | 0.00/67.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71bf2b8e79ec4a80a7e1a536a3252037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)826dc087/config.json:   0%|          | 0.00/650 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d70a1196fd4a9d948fe2a3287c1c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf3aea5b27a42019d5b15ccce91d02c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b3600625d240ff92fa78f2cef3ae01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4048b395404f43ab863126297ea616bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f937d8f08f4630a8c8076c600f3319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)dc087/tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658777327a6d41b29e653edd9a7d7123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dcb663eb9194e05b4ba74a8d026d6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)c3826dc087/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f68d22866204c378f7261bbfe63d630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)26dc087/modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-08-16 14:14:58.403\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m1/22 - created section + content embeddings for h1 Simulated Annealing Algorithm\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:02.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m2/22 - created section + content embeddings for h2 Overview\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:04.794\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m3/22 - created section + content embeddings for h3 The basic iteration\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:08.329\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m4/22 - created section + content embeddings for h3 The neighbours of a state\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:12.062\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m5/22 - created section + content embeddings for h3 Simulated Annealing\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:13.924\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m6/22 - created section + content embeddings for h3 The annealing schedule\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:16.362\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m7/22 - created section + content embeddings for h2 Pseudocode\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:18.603\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m8/22 - created section + content embeddings for h2 Selecting the parameters\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:20.872\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m9/22 - created section + content embeddings for h3 Sufficiently near neighbour\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:22.684\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m10/22 - created section + content embeddings for h3 Transition probabilities\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:24.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m11/22 - created section + content embeddings for h3 Efficient candidate generation\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:26.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m12/22 - created section + content embeddings for h3 Barrier avoidance\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:28.443\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m13/22 - created section + content embeddings for h3 Cooling schedule\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:30.205\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m14/22 - created section + content embeddings for h2 Restarts\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:31.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m15/22 - created section + content embeddings for h2 Related methods\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:33.620\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m16/22 - created section + content embeddings for h2 See also\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:36.949\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m17/22 - created section + content embeddings for h2 References 1\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:39.907\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m18/22 - created section + content embeddings for h2 References 2\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:42.577\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m19/22 - created section + content embeddings for h2 References 3\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:44.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m20/22 - created section + content embeddings for h2 References 4\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:46.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m21/22 - created section + content embeddings for h2 Further reading\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:48.342\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m22/22 - created section + content embeddings for h2 External links\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:48.351\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_plan\u001b[0m:\u001b[36m428\u001b[0m - \u001b[1mCreated plan embedding 1\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:48.353\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_plan\u001b[0m:\u001b[36m428\u001b[0m - \u001b[1mCreated plan embedding 2\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:50.682\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mgenerate_embeddings_plan_and_section_content\u001b[0m:\u001b[36m535\u001b[0m - \u001b[1mFinished creating plan json\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:50.825\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mextract_plan_and_content\u001b[0m:\u001b[36m792\u001b[0m - \u001b[1m\n",
      "\tSuccessfully extracted plan and content for https://en.wikipedia.org/wiki/Simulated_annealing\n",
      "\tWritten to file: /home/xav/document_extraction/output/wikipedia/Simulated Annealing.json\n",
      "\tTime taken: 3mins 31.0s\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:50.826\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mextract_plan_and_content\u001b[0m:\u001b[36m761\u001b[0m - \u001b[1m\n",
      "\tExtracting plan and content for: https://en.wikipedia.org/wiki/Dual-phase_evolution\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:51.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mload_wikipedia_url\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mSuccessfully downloaded content from Wikipedia page https://en.wikipedia.org/wiki/Dual-phase_evolution. Extracted 14 sections.\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:51.194\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mdivide_sections_if_too_large\u001b[0m:\u001b[36m294\u001b[0m - \u001b[1mDividing sections if too large in plan and section content.\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:51.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mdivide_sections_if_too_large\u001b[0m:\u001b[36m334\u001b[0m - \u001b[1mAdded 'h2 References 1' split original heading 'h2 References'\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:51.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mdivide_sections_if_too_large\u001b[0m:\u001b[36m334\u001b[0m - \u001b[1mAdded 'h2 References 2' split original heading 'h2 References'\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:51.203\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mdivide_sections_if_too_large\u001b[0m:\u001b[36m355\u001b[0m - \u001b[1m\n",
      "\tFinished dividing sections if too large in plan and section content.\n",
      "\tStarted with 14 sections and got 15 final sections.\n",
      "\tThat's a 1.07x increase in sections\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:51.203\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mgenerate_embeddings_plan_and_section_content\u001b[0m:\u001b[36m444\u001b[0m - \u001b[1mCreating plan json\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:51.204\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mgenerate_embeddings_plan_and_section_content\u001b[0m:\u001b[36m478\u001b[0m - \u001b[1mTitle: Dual-phase evolution\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:51.205\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mgenerate_embeddings_plan_and_section_content\u001b[0m:\u001b[36m479\u001b[0m - \u001b[1mAbstract: Dual phase evolution (DPE) is a process that drives self-organization within complex adaptive systems.[1] It arises in response to phase changes within the network of connections formed by a system's components. DPE occurs in a wide range of physical, biological and social systems. Its applications to technology include methods for manufacturing novel materials and algorithms to solve complex problems in computation.\n",
      "\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:53.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m1/14 - created section + content embeddings for h2 Introduction\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:56.159\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m2/14 - created section + content embeddings for h2 The DPE mechanism\u001b[0m\n",
      "\u001b[32m2023-08-16 14:15:58.139\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m3/14 - created section + content embeddings for h3 Underlying network\u001b[0m\n",
      "\u001b[32m2023-08-16 14:16:00.075\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m4/14 - created section + content embeddings for h3 Phase shifts\u001b[0m\n",
      "\u001b[32m2023-08-16 14:16:01.839\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m5/14 - created section + content embeddings for h3 Selection and variation\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-08-16 14:16:03.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m6/14 - created section + content embeddings for h3 System memory\u001b[0m\n",
      "\u001b[32m2023-08-16 14:16:06.197\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m7/14 - created section + content embeddings for h2 Examples\u001b[0m\n",
      "\u001b[32m2023-08-16 14:16:08.503\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m8/14 - created section + content embeddings for h3 Social networks\u001b[0m\n",
      "\u001b[32m2023-08-16 14:16:10.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m9/14 - created section + content embeddings for h3 Socio-economics\u001b[0m\n",
      "\u001b[32m2023-08-16 14:16:12.353\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m10/14 - created section + content embeddings for h3 Forest ecology\u001b[0m\n",
      "\u001b[32m2023-08-16 14:16:14.236\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m11/14 - created section + content embeddings for h3 Search algorithms\u001b[0m\n",
      "\u001b[32m2023-08-16 14:16:15.892\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m12/14 - created section + content embeddings for h2 Related processes\u001b[0m\n",
      "\u001b[32m2023-08-16 14:16:18.054\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m13/14 - created section + content embeddings for h2 References 1\u001b[0m\n",
      "\u001b[32m2023-08-16 14:16:19.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_section_content\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1m14/14 - created section + content embeddings for h2 References 2\u001b[0m\n",
      "\u001b[32m2023-08-16 14:16:19.946\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_plan\u001b[0m:\u001b[36m428\u001b[0m - \u001b[1mCreated plan embedding 1\u001b[0m\n",
      "\u001b[32m2023-08-16 14:16:19.948\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_gen_embed_plan\u001b[0m:\u001b[36m428\u001b[0m - \u001b[1mCreated plan embedding 2\u001b[0m\n",
      "\u001b[32m2023-08-16 14:16:21.579\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mgenerate_embeddings_plan_and_section_content\u001b[0m:\u001b[36m535\u001b[0m - \u001b[1mFinished creating plan json\u001b[0m\n",
      "\u001b[32m2023-08-16 14:16:21.674\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mextract_plan_and_content\u001b[0m:\u001b[36m792\u001b[0m - \u001b[1m\n",
      "\tSuccessfully extracted plan and content for https://en.wikipedia.org/wiki/Dual-phase_evolution\n",
      "\tWritten to file: /home/xav/document_extraction/output/wikipedia/Dual-phase evolution.json\n",
      "\tTime taken:  30.8s\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "url_1 = 'https://en.wikipedia.org/wiki/Simulated_annealing'\n",
    "url_2 = 'https://en.wikipedia.org/wiki/Dual-phase_evolution'\n",
    "\n",
    "doc_1 = await extract_plan_and_content_wikipedia(url_1)\n",
    "doc_2 = await extract_plan_and_content_wikipedia(url_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['section_id', 'section', 'content', 'section_embedding_1', 'section_embedding_2', 'content_embedding_1', 'content_embedding_2'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_1['plan'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "compare_documents_content() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m compare_plan \u001b[38;5;241m=\u001b[39m \u001b[43mcompare_documents_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m compare_sections \u001b[38;5;241m=\u001b[39m compare_documents_sections(doc_1, doc_2, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: compare_documents_content() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "compare_plan = compare_documents_plans(doc_1, doc_2, None)\n",
    "compare_sections = compare_documents_sections(doc_1, doc_2, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_ada = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",\n",
    ")\n",
    "\n",
    "embed_e5 = HuggingFaceEmbeddings(\n",
    "    model_name='intfloat/e5-base-v2',\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "text_1 = 'hello'\n",
    "text_2 = 'hi'\n",
    "text_3 = 'England is a small country'\n",
    "\n",
    "# e5-base-v2 requires all embeddings to be prepended with 'query: '\n",
    "e51 = embed_e5.embed_query(\"query: \" + text_1)\n",
    "e52 = embed_e5.embed_query(\"query: \" + text_2)\n",
    "e53 = embed_e5.embed_query(\"query: \" + text_3)\n",
    "print(len(e51))\n",
    "ada1 = embed_ada.embed_query(text_1)\n",
    "ada2 = embed_ada.embed_query(text_2)\n",
    "ada3 = embed_ada.embed_query(text_3)\n",
    "print(len(ada1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3]\n",
    "b = [10, 20, 30]\n",
    "\n",
    "print(np.mean([a, b], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 4, 5]\n",
    "b = [10, 20, 30, 40, 50]\n",
    "# a = np.array(a)\n",
    "# b = np.array(b)\n",
    "\n",
    "d = dict(a=ada1, b=ada2)\n",
    "\n",
    "# np.mean([ada1, ada2], axis=0)\n",
    "np.mean([*d.values()], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Dual-phase_evolution'\n",
    "url = 'https://en.wikipedia.org/wiki/Self-driving_car'\n",
    "r = requests.get(url)\n",
    "html_content = r.text\n",
    "\n",
    "# Create a BeautifulSoup object\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Remove unwanted tags: script, style, [document], head, title\n",
    "for element in soup([\"script\", \"style\", \"head\", \"title\", \"[document]\"]):\n",
    "    element.decompose()\n",
    "\n",
    "# Also remove HTML comments\n",
    "for element in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
    "    element.extract()\n",
    "\n",
    "# Define the tags to find\n",
    "tags = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']\n",
    "found_tags = soup.find_all(tags)\n",
    "\n",
    "# Extract tags and their associated content into a dictionary\n",
    "doc_dict = {}\n",
    "i = 0\n",
    "for tag in found_tags:\n",
    "    content = []\n",
    "    next_tag = tag.find_next()\n",
    "\n",
    "    # Look for next tags until the next header tag\n",
    "    while next_tag and next_tag.name not in tags:\n",
    "        # Reference section can contain both p and li tags\n",
    "        if 'reference' in str(next_tag).lower() and next_tag.name in ['p', 'li']:\n",
    "            # print(str(next_tag).lower())\n",
    "            content.append(next_tag.get_text(strip=False))\n",
    "        elif next_tag.name == 'p':\n",
    "            content.append(next_tag.get_text(strip=False))\n",
    "        next_tag = next_tag.find_next()\n",
    "\n",
    "    key = f\"{tag.name} {tag.get_text(strip=True)}\"\n",
    "    doc_dict[key] = \" \".join(content)\n",
    "\n",
    "for key in list(doc_dict.keys()):  # Using list() to avoid changing the dictionary size during iteration\n",
    "    if key.endswith('[edit]'):\n",
    "        new_key = key.rsplit('[edit]', 1)[0]\n",
    "        doc_dict[new_key] = doc_dict.pop(key)\n",
    "#     new_key = key.rstrip('\\[edit\\]')\n",
    "#     if new_key != key:\n",
    "#         doc_dict[new_key] = doc_dict.pop(key)\n",
    "#\n",
    "# processed = []\n",
    "# for s in strings:\n",
    "#     if s.endswith('[edit]'):\n",
    "#         s = s.rsplit('[edit]', 1)[0]\n",
    "#     processed.append(s)\n",
    "\n",
    "del doc_dict['h2 Contents']\n",
    "\n",
    "pprint(doc_dict, sort_dicts=False)\n",
    "pprint(doc_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctran = Doctran(openai_api_key=os.getenv('OPENAI_API_KEY'), openai_model='gpt-3.5-turbo')\n",
    "\n",
    "async def extract_topics(string: str):\n",
    "    \"\"\"Extract topics discussed in a string.\"\"\"\n",
    "    document = doctran.parse(content=string)\n",
    "    properties = ExtractProperty(\n",
    "        # name=\"keywords_max_three\",\n",
    "        # description=\"The 1, 2, or 3 most important keywords from the document.\",\n",
    "        name='title',\n",
    "        description='The title of the document (max 7 words).',\n",
    "        type=\"string\",\n",
    "        required=True\n",
    "    )\n",
    "    document = await document.extract(properties=[properties]).execute()\n",
    "    return document.transformed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(doc_dict, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict: dict = {}\n",
    "start_dict = copy(doc_dict)\n",
    "for heading, content in start_dict.items():\n",
    "    print(heading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict: dict = {}\n",
    "start_dict = copy(doc_dict)\n",
    "for heading, content in start_dict.items():\n",
    "    if 'reference' in heading.lower() or 'further reading' in heading.lower() or 'see also' in heading.lower():\n",
    "        print(f\"Found 'referece' in {heading}\")\n",
    "        final_dict[heading] = content\n",
    "        continue\n",
    "    num_tokens = num_tokens_from_string(content)\n",
    "    max_allowed_tokens = 512\n",
    "    if num_tokens <= max_allowed_tokens:\n",
    "        final_dict[heading] = content\n",
    "    else:\n",
    "        # Split the document into smaller chunks, then add topics\n",
    "        char_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=max_allowed_tokens,\n",
    "            chunk_overlap=0,\n",
    "            # ' ' separator means sometimes sentences will be cut in two to ensure\n",
    "            # the chunk size is not exceeded\n",
    "            separators=[\"\\n\\n\", \"\\n\", ' '],\n",
    "            length_function=num_tokens_from_string,\n",
    "        )\n",
    "        splits: List[str] = char_splitter.split_text(content)\n",
    "        for split in splits:\n",
    "            # Headings are of the form h1, h2, h3 etc. we split it into more of the same level\n",
    "            heading_level = int(heading[1])\n",
    "            title = await extract_topics(split)\n",
    "            new_heading = f\"h{heading_level}_{title}\"\n",
    "            final_dict[new_heading] = split\n",
    "            print(f'Added {new_heading} split original heading {heading}')\n",
    "\n",
    "n_keys_start = len(start_dict.keys())\n",
    "n_keys_final = len(final_dict.keys())\n",
    "print(f\"We started with {n_keys_start} sections and got {n_keys_final} final sections\")\n",
    "print(f\"That's a {n_keys_final / n_keys_start:.2f}x increase in sections\")\n",
    "# pprint(final_dict, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(list(final_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(final_dict, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = final_dict['h2 Regulation']\n",
    "text_2 = final_dict['h2 Commercialization']\n",
    "\n",
    "# Normalized by default\n",
    "embed_ada = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",\n",
    ")\n",
    "\n",
    "embed_e5 = HuggingFaceEmbeddings(\n",
    "    model_name='intfloat/e5-base-v2',\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# e5-base-v2 requires all embeddings to be prepended with 'query: '\n",
    "e51 = embed_e5.embed_query(\"query: \" + text_1)\n",
    "e52 = embed_e5.embed_query(\"query: \" + text_2)\n",
    "print(len(e51))\n",
    "ada1 = embed_ada.embed_query(text_1)\n",
    "ada2 = embed_ada.embed_query(text_2)\n",
    "print(len(ada1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity([e51], [e51])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_dict = copy(final_dict)\n",
    "modified_dict['h3_extra key'] = 'extra value'\n",
    "article_1 = convert_to_markdown(final_dict)\n",
    "article_2 = convert_to_markdown(modified_dict)\n",
    "\n",
    "short_1 = article_1[:1000]\n",
    "short_2 = article_2[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens_from_string(short_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.logging.set_verbosity_info()\n",
    "# Mauve has to compare the actual text strings themselves, not embeddings\n",
    "mauve = load('mauve')\n",
    "mauve_results = mauve.compute(\n",
    "    predictions=[short_1],\n",
    "    references=[short_2],\n",
    "    device_id=0,\n",
    ")\n",
    "mauve_results.mauve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mauve_results.mauve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = load('rouge')\n",
    "results = rouge.compute(predictions=[short_1], references=[short_2], rouge_types=['rougeL'])\n",
    "print(results['rougeL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = embed_ada.embed_query('hello')\n",
    "a_ = str(a)\n",
    "print(num_tokens_from_string(a_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for heading, content in final_dict.items():\n",
    "    print(heading, num_tokens_from_string(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict['h2 See also']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_e5.embed_query('query:  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create json plan\n",
    "\n",
    "final_headings = list(final_dict.keys())\n",
    "final_content = list(final_dict.values())\n",
    "\n",
    "def create_section_json(heading, content, id=1):\n",
    "    result = {\n",
    "        'section_id': id,\n",
    "        'section': heading,\n",
    "        'content': content,\n",
    "        'section_embedding_1': embed_ada.embed_query(heading),\n",
    "        'section_embedding_2': embed_e5.embed_query(\"query: \" + heading),\n",
    "        'content_embedding_1': embed_ada.embed_query(content),\n",
    "        'content_embedding_2': embed_e5.embed_query(\"query: \" + content),\n",
    "    }\n",
    "    print(f\"Created embeddings for {heading}\")\n",
    "    return result\n",
    "\n",
    "def calcuate_plan_embedding(plan: List[dict], i: int) -> List[float]:\n",
    "    \"\"\"Calculate plan embedding by averaging the section embeddings and content embeddings\n",
    "    sequentially.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    plan : List[dict]\n",
    "        List of dictionaries, each containing the section and content embeddings.\n",
    "    i : int\n",
    "        The index of the embedding to use.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s1_mean = np.mean([x[f'section_embedding_{i}'] for x in plan], axis=0)\n",
    "        c1_mean = np.mean([x[f'content_embedding_{i}'] for x in plan], axis=0)\n",
    "    except KeyError:\n",
    "        raise KeyError(\n",
    "            f'Could not find section_embedding_{i} or content_embedding_{i} in '\n",
    "            f'every element in plan. Please check that every element has the correct keys.'\n",
    "        )\n",
    "    total_mean = np.mean([c1_mean, s1_mean], axis=0)\n",
    "    total_mean = list(total_mean)\n",
    "    print(f\"Calculated plan embedding {i}\")\n",
    "    return total_mean\n",
    "\n",
    "# title is h1_example\n",
    "title = final_headings[0][3:]\n",
    "abstract = final_content[0]\n",
    "plan = [\n",
    "    create_section_json(heading, content, id=i)\n",
    "    for i, (heading, content) in enumerate(zip(\n",
    "        final_headings[1:], final_content[1:]), start=1\n",
    "    )\n",
    "    if 0 < num_tokens_from_string(content) <= 8000\n",
    "]\n",
    "plan_embed_1 = calcuate_plan_embedding(plan, 1)\n",
    "plan_embed_2 = calcuate_plan_embedding(plan, 2)\n",
    "plan_json = {\n",
    "    'id': str(uuid4()),\n",
    "    'title': title,\n",
    "    'abstract': abstract,\n",
    "    'title_embedding_1': embed_ada.embed_query(title),\n",
    "    'title_embedding_2': embed_e5.embed_query(\"query: \" + title),\n",
    "    'abstract_embedding_1': embed_ada.embed_query(abstract),\n",
    "    'abstract_embedding_2': embed_e5.embed_query(\"query: \" + abstract),\n",
    "    'plan': plan,\n",
    "    'plan_embedding_1': plan_embed_1,\n",
    "    'plan_embedding_2': plan_embed_2,\n",
    "    'embedding1_model': 'text-embedding-ada-002',\n",
    "    'embedding2_model': 'e5-base-v2',\n",
    "    'success': True,\n",
    "    'error': None,\n",
    "}\n",
    "plan_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'plan' in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_json['plan'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_embeddings(\n",
    "    input: str | List[str], model: str = \"text-embedding-ada-002\"\n",
    ") -> List[float] | List[List[float]]:\n",
    "    \"\"\"This function takes one string or a list of strings and a model name,\n",
    "    generates an embedding for each string in the list using the specified model,\n",
    "    and returns a list of embeddings, each represented as a list of floating point\n",
    "    numbers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input : str | List[str]\n",
    "        The input string or list of strings to be embedded.\n",
    "    model : str, optional ['text-embedding-ada-002', 'e5-base-v2']\n",
    "        The name of the model to be used for embedding.\n",
    "    \"\"\"\n",
    "    if model == \"text-embedding-ada-002\":\n",
    "        embedder = OpenAIEmbeddings(model=model)\n",
    "    elif model == \"e5-base-v2\":\n",
    "        embedder = HuggingFaceEmbeddings(\n",
    "            model_name=f\"intfloat/{model}\", encode_kwargs={\"normalize_embeddings\": True}\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Model name must be 'text-embedding-ada-002' or 'e5-base-v2'. Received {model}\"\n",
    "        )\n",
    "\n",
    "    if isinstance(input, str):\n",
    "        try:\n",
    "            return await embedder.aembed_query(input)\n",
    "        except NotImplementedError as e:\n",
    "            return embedder.embed_query(input)\n",
    "    elif isinstance(input, list):\n",
    "        try:\n",
    "            return await embedder.aembed_documents(input)\n",
    "        except NotImplementedError as e:\n",
    "            return embedder.embed_documents(input)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Input must be a string or a list of strings. Received {type(input)}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await get_embeddings(['hello', 'there'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = deepcopy(plan_json)\n",
    "document = deepcopy(plan_json)\n",
    "\n",
    "def compare_documents(document: dict, prediction: dict, compare_on: str ='section'):\n",
    "    \"\"\"Compare the 'compare_on' sections of document and prediction. Calculate MAUVE,\n",
    "    and ROUGE-L scores on the actual text, and cosine similarity on the embeddings.\n",
    "    \"\"\"\n",
    "    if compare_on not in ['section', 'content']:\n",
    "        raise ValueError(\n",
    "            f\"`compare_on` must be 'section' or 'content'. Received {compare_on}\"\n",
    "        )\n",
    "\n",
    "    if not isinstance(document, dict) or not isinstance(prediction, dict):\n",
    "        raise TypeError(\n",
    "            'Both `document` and `prediction` must be dictionaries. Received '\n",
    "            f'{type(document)} and {type(prediction)}'\n",
    "        )\n",
    "\n",
    "    if 'plan' not in document or 'plan' not in prediction:\n",
    "        raise ValueError(\n",
    "            f'Both `document` and `prediction` must contain the key \"plan\". At least '\n",
    "            f'one of them does not.'\n",
    "        )\n",
    "\n",
    "    mauve = load(\"mauve\")\n",
    "    rouge = load(\"rouge\")\n",
    "\n",
    "    section_results = []\n",
    "    predict_plan = prediction['plan']\n",
    "    doc_plan = document['plan']\n",
    "    for i, (p_dict, d_dict) in enumerate(zip(predict_plan, doc_plan), start=1):\n",
    "        idx = i\n",
    "        # Compute MAUVE\n",
    "        mauve_results = mauve.compute(\n",
    "            predictions=[p_dict['section']], references=[d_dict['section']]\n",
    "        )\n",
    "        mauve_score = mauve_results.mauve\n",
    "        # Compute ROUGE-L\n",
    "        results = rouge.compute(\n",
    "            predictions=[p_dict['section']],\n",
    "            references=[d_dict['section']],\n",
    "            rouge_types=[\"rougeL\"],\n",
    "        )\n",
    "        rouge_score = results[\"rougeL\"]\n",
    "        # Compute cosine distance between both section embeddings\n",
    "        cosine_1 = cosine_similarity(\n",
    "            [p_dict[\"section_embedding_1\"]], [d_dict[\"section_embedding_1\"]]\n",
    "        )[0][0]\n",
    "        cosine_2 = cosine_similarity(\n",
    "            [p_dict[\"section_embedding_2\"]], [d_dict[\"section_embedding_2\"]]\n",
    "        )[0][0]\n",
    "        # Combine results\n",
    "        result = {\n",
    "            'section_id': idx,\n",
    "            'mauve_similarity': mauve_score,\n",
    "            'rouge_L_similarity': rouge_score,\n",
    "            'embedding1_cosine_similarity': cosine_1,\n",
    "            'embedding2_cosine_similarity': cosine_2,\n",
    "        }\n",
    "        section_results.append(result)\n",
    "\n",
    "    # Calcualte total scores\n",
    "    mauve_total = np.mean([x['mauve_similarity'] for x in section_results])\n",
    "    rouge_total = np.mean([x['rouge_L_similarity'] for x in section_results])\n",
    "    cosine_1_total = np.mean([x['embedding1_cosine_similarity'] for x in section_results])\n",
    "    cosine_2_total = np.mean([x['embedding2_cosine_similarity'] for x in section_results])\n",
    "\n",
    "    total_results = {\n",
    "        'mauve_similarity': mauve_total,\n",
    "        'rouge_L_similarity': rouge_total,\n",
    "        'embedding1_cosine_similarity': cosine_1_total,\n",
    "        'embedding2_cosine_similarity': cosine_2_total,\n",
    "    }\n",
    "\n",
    "    if compare_on == 'section':\n",
    "        compare_on = 'plan'\n",
    "\n",
    "    output = {\n",
    "        'document_id': document['id'],\n",
    "        'prediction_id': prediction['id'],\n",
    "        f'{compare_on}_total_similarity': total_results,\n",
    "        f'{compare_on}_bysection_similarity': section_results,\n",
    "    }\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_results = compare_documents(document, prediction, compare_on='section')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_results = compare_documents(document, prediction, compare_on='content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtotal_results\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'total_results' is not defined"
     ]
    }
   ],
   "source": [
    "total_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens_from_string('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_section_json(heading, content, id=1):\n",
    "    return {\n",
    "        'section_id': id,\n",
    "        'section': heading,\n",
    "        'content': content,\n",
    "        'section_embedding_1': embed_ada.embed_query(heading),\n",
    "        'section_embedding_2': embed_e5.embed_query(\"query: \" + heading),\n",
    "        'content_embedding_1': embed_ada.embed_query(content),\n",
    "        'content_embedding_2': embed_e5.embed_query(\"query: \" + content),\n",
    "    }\n",
    "\n",
    "# plan_embedding_1 = np.mean([s['section_embedding_1']])\n",
    "plan = [\n",
    "    dict(s1=[1,2,3], c1=[4,5,6]),\n",
    "    dict(s1=[7,8,9], c1=[10,11,12]),\n",
    "]\n",
    "s1_mean = np.mean([x['s1'] for x in plan], axis=0)\n",
    "c1_mean = np.mean([x['c1'] for x in plan], axis=0)\n",
    "total_mean = np.mean([c1_mean, s1_mean], axis=0)\n",
    "total_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(26)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_content(plan_1: List[dict], plan_2: List[dict]):\n",
    "    document_id = uuid4()\n",
    "    prediction_id = uuid4()\n",
    "    total_similarity = 'not possible'\n",
    "    by_section_similarity = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_plans(plan_1, plan_2):\n",
    "    # Mauve has to compare the actual text strings themselves, not embeddings\n",
    "    mauve = load('mauve')\n",
    "    mauve_results = mauve.compute(predictions=[article_1], references=[article_2])\n",
    "    mauve_results.mauve\n",
    "\n",
    "    cosine_1 = cosine_similarity([plan_1['plan_embedding_1']], [plan_2['plan_embedding_1']])[0][0]\n",
    "    cosine_2 = cosine_similarity([plan_1['plan_embedding_2']], [plan_2['plan_embedding_2']])[0][0]\n",
    "    mauve_1 = ...\n",
    "    return {\n",
    "        'document_id': 'what is this?',\n",
    "        'prediction_id': 'what is this?',\n",
    "        'plan_similarity': '...'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = [{\"section_id\": 1, \"section\": \"Introduction\", \"content\": \"Introduction content...\", \"section_embedding1\": [0.1, 0.2, 0.3]}]\n",
    "embed_ada.embed_query(str(ex))\n",
    "embed_e5.embed_query(\"query: \" + str(ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unused Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_markdown(article_dict):\n",
    "    md_text = \"\"\n",
    "\n",
    "    for heading, content in article_dict.items():\n",
    "        # heading is of form: 'h3 Example'\n",
    "        # Define the markdown equivalent for the heading level\n",
    "        heading_level = \"#\" * int(heading[1])\n",
    "        heading = heading[3:]\n",
    "        # Append the heading and the content to the markdown text\n",
    "        md_text += f\"{heading_level} {heading}\\n\\n{content}\\n\\n\"\n",
    "\n",
    "    return md_text\n",
    "\n",
    "article_dict = {\n",
    "    'h1_Self-driving car': 'Content for Self-driving car',\n",
    "    'h2_History': 'Content for History',\n",
    "    'h2_Definitions': 'Content for Definitions',\n",
    "    'h3_Terminology and safety considerations': 'Content for Terminology and safety considerations',\n",
    "    'h3_Autonomous vs. automa': 'Content for Autonomous vs. automa',\n",
    "    'h3_Autonomous versus cooperativ': 'Content for Autonomous versus cooperativ',\n",
    "    'h2_Classifications': 'Content for Classifications',\n",
    "    'h3_Self-driving car': 'Content for Self-driving car',\n",
    "    'h3_SAE classification': 'Content for SAE classification',\n",
    "    'h3_Levels of driving automation': 'Content for Levels of driving automation'\n",
    "}\n",
    "\n",
    "self_driving_md = convert_to_markdown(doc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "    (\"####\", \"Header 4\"),\n",
    "    (\"#####\", \"Header 5\"),\n",
    "    (\"######\", \"Header 6\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "# markdown_splitter = MarkdownTextSplitter(chunk_size=512, chunk_overlap=0)\n",
    "docs = markdown_splitter.split_text(self_driving_md)\n",
    "# docs[0]Document(page_content=\"Dual phase evolution (DPE) is a process that drives self-organization within complex adaptive systems.[1] It arises in response to phase changes within the network of connections formed by a system's components. DPE occurs in a wide range of physical, biological and social systems. Its applications to technology include methods for manufacturing novel materials and algorithms to solve complex problems in computation.\", metadata={'Header 1': 'Dual-phase evolution'})\n",
    "docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_e5 = SentenceTransformer('intfloat/e5-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"query: \" + final_docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embed_e5.encode(\"query: \" + final_docs[0].page_content, normalize_embeddings=True)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_openai = openai.Embedding.create(\n",
    "    input = [final_docs[0].page_content],\n",
    "    model=\"text-embedding-ada-002\")\n",
    "['data'][0]['embedding']\n",
    "embedding_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(docs):\n",
    "    tokens = num_tokens_from_string(doc.page_content)\n",
    "    if tokens > 512:\n",
    "        print(f\"Document {i}\")\n",
    "        print(doc.metadata)\n",
    "        print(f\"{num_tokens_from_string(doc.page_content):,} tokens\")\n",
    "        print(doc.page_content[:100])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = docs[1]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_docs: List[Document] = []\n",
    "start_docs = copy(docs)\n",
    "for i, doc in enumerate(start_docs):\n",
    "    num_tokens = num_tokens_from_string(doc.page_content)\n",
    "    max_allowed_tokens = 512\n",
    "    if num_tokens <= max_allowed_tokens:\n",
    "        # Add topics\n",
    "        topics = await extract_topics(doc.page_content)\n",
    "        doc.metadata['topics'] = topics\n",
    "        final_docs.append(doc)\n",
    "    else:\n",
    "        # Split the document into smaller chunks, then add topics\n",
    "        char_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=max_allowed_tokens,\n",
    "            chunk_overlap=0,\n",
    "            # ' ' separator means sometimes sentences will be cut in two to ensure\n",
    "            # the chunk size is not exceeded\n",
    "            separators=[\"\\n\\n\", \"\\n\", ' '],\n",
    "            length_function=num_tokens_from_string,\n",
    "        )\n",
    "        splits: List[str] = char_splitter.split_text(doc.page_content)\n",
    "        for split in splits:\n",
    "            new_metadata = doc.metadata.copy()\n",
    "            topics = await extract_topics(split)\n",
    "            new_metadata['topics'] = topics\n",
    "            final_docs.append(Document(page_content=split, metadata=new_metadata))\n",
    "\n",
    "print(f\"We started with {len(start_docs)} docs and got {len(final_docs)} final docs\")\n",
    "print(f\"That's a {len(final_docs) / len(start_docs):.2f}x increase in docs\")\n",
    "for d in final_docs:\n",
    "    print(d.metadata)\n",
    "    print(f\"{num_tokens_from_string(d.page_content):,} tokens\")\n",
    "    print(d.page_content[:100])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\"],\n",
    "    length_function=num_tokens_from_string,\n",
    ")\n",
    "splits = char_splitter.split_text(history.page_content)\n",
    "print(f\"We got {len(splits)} splits\")\n",
    "\n",
    "for i, split in enumerate(splits):\n",
    "    topics = await extract_topics(split)\n",
    "    print(f\"Split {i} - Topics: {topics}\")\n",
    "    print(f\"{num_tokens_from_string(split):,} tokens, {len(split)} characters\")\n",
    "    print(split)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.metadata['example'] = 'hello'\n",
    "history.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_metadata = history.metadata.copy()\n",
    "base_metadata['topics'] = topics\n",
    "Document(page_content=split, metadata=base_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkdownHeaderTextSplitterMaxTokenLimit(MarkdownHeaderTextSplitter):\n",
    "    def __init__(self, max_tokens=512, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    def calculate_tokens(self, string: str, encoding_name: str = \"gpt-3.5-turbo\") -> int:\n",
    "        \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "        try:\n",
    "            encoding = tiktoken.get_encoding(encoding_name)\n",
    "        except ValueError:\n",
    "            encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "        num_tokens = len(encoding.encode(string))\n",
    "        return num_tokens\n",
    "\n",
    "    def generate_extra_metadata(self, string: str):\n",
    "        return 'EXTRA INFO ADDED'\n",
    "\n",
    "    def aggregate_lines_to_chunks(self, lines: List[LineType]) -> List[Document]:\n",
    "        \"\"\"Combine lines with common metadata into chunks\n",
    "        Args:\n",
    "            lines: Line of text / associated header metadata\n",
    "        \"\"\"\n",
    "        aggregated_chunks: List[LineType] = []\n",
    "\n",
    "        for line in lines:\n",
    "            if (\n",
    "                aggregated_chunks\n",
    "                and aggregated_chunks[-1][\"metadata\"] == line[\"metadata\"]\n",
    "            ):\n",
    "                # If the last line in the aggregated list\n",
    "                # has the same metadata as the current line,\n",
    "                # append the current content to the last lines's content\n",
    "                new_content = aggregated_chunks[-1][\"content\"] + \"  \\n\" + line[\"content\"]\n",
    "                new_token_count = self.calculate_tokens(new_content)\n",
    "\n",
    "                # If less than 512 tokens, update the last line's content\n",
    "                if new_token_count <= 512:\n",
    "                    aggregated_chunks[-1][\"content\"] = new_content\n",
    "                # If more than 512 tokens, start a new chunk and add extra metadata to it\n",
    "                else:\n",
    "                    # Generate the extra metadata for the current chunk.\n",
    "                    new_metadata = self.generate_extra_metadata(line[\"content\"])\n",
    "                    line[\"metadata\"] += new_metadata\n",
    "                    # If the new token count would exceed the limit, start a new chunk\n",
    "                    aggregated_chunks.append(line)\n",
    "            else:\n",
    "                # Otherwise, append the current line to the aggregated list\n",
    "                aggregated_chunks.append(line)\n",
    "\n",
    "        return [\n",
    "            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\n",
    "            for chunk in aggregated_chunks\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import urllib.request\n",
    "from urllib.parse import quote\n",
    "\n",
    "def download_html(url, output_file):\n",
    "    response = urllib.request.urlopen(url)\n",
    "    data = response.read()\n",
    "\n",
    "    with open(output_file, 'wb') as file:\n",
    "        file.write(data)\n",
    "\n",
    "def convert_html_to_markdown(input_file, output_file):\n",
    "    command = ['pandoc', '-f', 'html', '-t', 'markdown', '-o', output_file, input_file]\n",
    "    subprocess.run(command, check=True)\n",
    "\n",
    "# Prepare the URL\n",
    "wiki_page = 'https://en.wikipedia.org/wiki/OpenAI'\n",
    "wiki_page_encoded = quote(wiki_page, safe=':/')\n",
    "html_file = 'openai.html'\n",
    "markdown_file = 'openai.md'\n",
    "\n",
    "# Download the HTML page\n",
    "download_html(wiki_page_encoded, html_file)\n",
    "\n",
    "# Convert HTML to Markdown\n",
    "convert_html_to_markdown(html_file, markdown_file)\n",
    "\n",
    "print(f\"Markdown file {markdown_file} has been created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('openai.md', 'r', encoding='utf-8') as file:\n",
    "    markdown_text = file.read()\n",
    "\n",
    "markdown_splitter = MarkdownTextSplitter(chunk_size=512, chunk_overlap=0)\n",
    "docs = markdown_splitter.create_documents([markdown_text])\n",
    "docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import urllib.request\n",
    "from urllib.parse import quote\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def download_html(url):\n",
    "    response = urllib.request.urlopen(url)\n",
    "    data = response.read().decode()\n",
    "    return data\n",
    "\n",
    "def parse_html(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    content_text = soup.find('div', {'id': 'mw-content-text'})\n",
    "    return str(content_text)\n",
    "\n",
    "def convert_html_to_markdown(html_content, output_file):\n",
    "    with open('temp.html', 'w') as f:\n",
    "        f.write(html_content)\n",
    "    command = ['pandoc', '-f', 'html', '-t', 'markdown', '-o', output_file, 'temp.html']\n",
    "    subprocess.run(command, check=True)\n",
    "\n",
    "# Prepare the URL\n",
    "wiki_page = 'https://en.wikipedia.org/wiki/OpenAI'\n",
    "wiki_page_encoded = quote(wiki_page, safe=':/')\n",
    "markdown_file = 'openai.md'\n",
    "\n",
    "# Download the HTML page\n",
    "html_data = download_html(wiki_page_encoded)\n",
    "\n",
    "# Parse HTML\n",
    "parsed_html = parse_html(html_data)\n",
    "\n",
    "# Convert HTML to Markdown\n",
    "convert_html_to_markdown(parsed_html, markdown_file)\n",
    "\n",
    "print(f\"Markdown file {markdown_file} has been created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()\n",
    "\n",
    "wikipedia_articles = [\n",
    "    \"https://en.wikipedia.org/wiki/Large_language_model\",\n",
    "    \"https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)\",\n",
    "    \"https://en.wikipedia.org/wiki/Dual-phase_evolution\",\n",
    "    'https://en.wikipedia.org/wiki/Simulated_annealing',\n",
    "    \"https://en.wikipedia.org/wiki/Tessellation\",\n",
    "    \"https://en.wikipedia.org/wiki/Climate_change\",\n",
    "    \"https://en.wikipedia.org/wiki/DNA_nanotechnology\",\n",
    "    \"https://en.wikipedia.org/wiki/Self-driving_car\",\n",
    "    \"https://en.wikipedia.org/wiki/Unmanned_aerial_vehicle\",\n",
    "    \"https://en.wikipedia.org/wiki/2022%E2%80%932023_food_crises\",\n",
    "    \"https://en.wikipedia.org/wiki/Economic_impacts_of_climate_change\",\n",
    "]\n",
    "\n",
    "dual_phase = wikipedia_articles[2]\n",
    "\n",
    "article = wikipedia_articles[0]\n",
    "\n",
    "# Send an HTTP request to the URL\n",
    "response = requests.get(article)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Get the HTML content from the response\n",
    "    html_content = response.text\n",
    "\n",
    "    # Use BeautifulSoup to parse the HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "else:\n",
    "    print(f\"Failed to download the webpage. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = WebBaseLoader(wikipedia_articles).load()\n",
    "print('Total number of articles: ', len(all_articles))\n",
    "for i, doc in enumerate(all_articles):\n",
    "    print(f\"Article {i} contains {num_tokens_from_string(doc.page_content):,} tokens - {wikipedia_articles[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(article)\n",
    "data = loader.load()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual_phase = 'https://en.wikipedia.org/wiki/Dual-phase_evolution'\n",
    "# dual_phase = 'https://en.wikipedia.org/wiki/Climate_change'\n",
    "response_dp = requests.get(dual_phase)\n",
    "html_content_dp = response_dp.text\n",
    "soup_dp = BeautifulSoup(html_content_dp, 'html.parser')\n",
    "\n",
    "# Extract all headers and titles\n",
    "titles_and_headers = []\n",
    "\n",
    "# Find all title tags and extract their text\n",
    "title_tags = soup_dp.find_all('title')\n",
    "for title in title_tags:\n",
    "    titles_and_headers.append(title.text.strip())\n",
    "\n",
    "# Find all header tags (h1 to h6) and extract their text\n",
    "header_tags = soup_dp.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "for header in header_tags:\n",
    "    titles_and_headers.append(header.text.strip())\n",
    "\n",
    "# Print the extracted headers and titles\n",
    "for item in titles_and_headers:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_dp.h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('self_driving.html', 'w', encoding='utf-8') as file:\n",
    "    file.write(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in soup.find_all('h3') if 'Approaches' in x.get_text()][0].find_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']\n",
    "found_tags = soup.find_all(tags)\n",
    "\n",
    "# Extract tags and their associated content into a dictionary\n",
    "found_tags[1].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'my_dict' is your dictionary\n",
    "my_dict = {\n",
    "    'Contents': None,\n",
    "    'Dual-phase evolution': None,\n",
    "    'Introduction[edit]': None,\n",
    "    'The DPE mechanism[edit]': None,\n",
    "    'Underlying network[edit]': None,\n",
    "    'Phase shifts[edit]': None,\n",
    "    'Selection and variation[edit]': None\n",
    "}\n",
    "\n",
    "# Remove '[edit]' from the end of the keys\n",
    "for key in list(my_dict.keys()):  # Using list() to avoid changing the dictionary size during iteration\n",
    "    new_key = key.rstrip('[edit]')\n",
    "    if new_key != key:\n",
    "        my_dict[new_key] = my_dict.pop(key)\n",
    "\n",
    "print(my_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.h1.find_next_sibling().find_next_sibling().find_next_sibling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the webpage content\n",
    "url = 'https://en.wikipedia.org/wiki/Dual-phase_evolution'\n",
    "r = requests.get(url)\n",
    "html_content = r.text\n",
    "\n",
    "# Create a BeautifulSoup object\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Remove unwanted tags: script, style, [document], head, title\n",
    "for element in soup([\"script\", \"style\", \"head\", \"title\", \"[document]\"]):\n",
    "    element.decompose()\n",
    "\n",
    "# Also remove HTML comments\n",
    "for element in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
    "    element.extract()\n",
    "\n",
    "# Define the tags to find\n",
    "tags = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']\n",
    "found_tags = soup.find_all(tags)\n",
    "\n",
    "# Extract tags and their associated content into a dictionary\n",
    "doc_dict = {}\n",
    "for tag in found_tags:\n",
    "    content = []\n",
    "    next_sibling = tag.find_next_sibling('p')\n",
    "\n",
    "    while next_sibling and next_sibling.name not in tags:\n",
    "        if next_sibling.name == 'p':\n",
    "            content.append(next_sibling.get_text(strip=True))\n",
    "        next_sibling = next_sibling.find_next_sibling()\n",
    "\n",
    "    doc_dict[tag.get_text(strip=True)] = \" \".join(content)\n",
    "\n",
    "pprint(doc_dict, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('soup.html', 'w', encoding='utf-8') as file:\n",
    "    file.write(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_html = \"\"\"<h1 class=\"firstHeading mw-first-heading\" id=\"firstHeading\">\n",
    "        <span class=\"mw-page-title-main\">\n",
    "         Dual-phase evolution\n",
    "        </span>\n",
    "       </h1>\n",
    "       <div class=\"vector-dropdown mw-portlet mw-portlet-lang\" id=\"p-lang-btn\">\n",
    "        <input aria-haspopup=\"true\" aria-label=\"Go to an article in another language. Available in 1 language\" class=\"vector-dropdown-checkbox mw-interlanguage-selector\" data-event-name=\"ui.dropdown-p-lang-btn\" id=\"p-lang-btn-checkbox\" role=\"button\" type=\"checkbox\">\n",
    "         <label aria-hidden=\"true\" class=\"vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--action-progressive mw-portlet-lang-heading-1\" for=\"p-lang-btn-checkbox\" id=\"p-lang-btn-label\">\n",
    "         </label>\n",
    "         <span class=\"vector-icon mw-ui-icon-language-progressive mw-ui-icon-wikimedia-language-progressive\">\n",
    "         </span>\n",
    "         <span class=\"vector-dropdown-label-text\">\n",
    "          1 language\n",
    "         </span>\n",
    "         <div class=\"vector-dropdown-content\">\n",
    "          <div class=\"vector-menu-content\">\n",
    "           <ul class=\"vector-menu-content-list\">\n",
    "            <li class=\"interlanguage-link interwiki-fa mw-list-item\">\n",
    "             <a class=\"interlanguage-link-target\" href=\"https://fa.wikipedia.org/wiki/%D9%81%D8%B1%DA%AF%D8%B4%D8%AA_%D8%AF%D9%88%D9%81%D8%A7%D8%B2%DB%8C\" hreflang=\"fa\" lang=\"fa\" title=\"فرگشت دوفازی – Persian\">\n",
    "              <span>\n",
    "               فارسی\n",
    "              </span>\n",
    "             </a>\n",
    "            </li>\n",
    "           </ul>\n",
    "           <div class=\"after-portlet after-portlet-lang\">\n",
    "            <span class=\"wb-langlinks-edit wb-langlinks-link\">\n",
    "             <a class=\"wbc-editpage\" href=\"https://www.wikidata.org/wiki/Special:EntityPage/Q25109659#sitelinks-wikipedia\" title=\"Edit interlanguage links\">\n",
    "              Edit links\n",
    "             </a>\n",
    "            </span>\n",
    "           </div>\n",
    "          </div>\n",
    "         </div>\n",
    "        </input>\n",
    "       </div>\n",
    "      </header>\n",
    "      <div class=\"vector-page-toolbar\">\n",
    "       <div class=\"vector-page-toolbar-container\">\n",
    "        <div id=\"left-navigation\">\n",
    "         <nav aria-label=\"Namespaces\">\n",
    "          <div class=\"vector-menu vector-menu-tabs mw-portlet mw-portlet-associated-pages\" id=\"p-associated-pages\">\n",
    "           <div class=\"vector-menu-content\">\n",
    "            <ul class=\"vector-menu-content-list\">\n",
    "             <li class=\"selected vector-tab-noicon mw-list-item\" id=\"ca-nstab-main\">\n",
    "              <a accesskey=\"c\" class=\"\" data-mw=\"interface\" href=\"/wiki/Dual-phase_evolution\" title=\"View the content page [c]\">\n",
    "               <span>\n",
    "                Article\n",
    "               </span>\n",
    "              </a>\n",
    "             </li>\n",
    "             <li class=\"vector-tab-noicon mw-list-item\" id=\"ca-talk\">\n",
    "              <a accesskey=\"t\" class=\"\" data-mw=\"interface\" href=\"/wiki/Talk:Dual-phase_evolution\" rel=\"discussion\" title=\"Discuss improvements to the content page [t]\">\n",
    "               <span>\n",
    "                Talk\n",
    "               </span>\n",
    "              </a>\n",
    "             </li>\n",
    "            </ul>\n",
    "           </div>\n",
    "          </div>\n",
    "          <div class=\"vector-dropdown mw-portlet mw-portlet-variants emptyPortlet\" id=\"p-variants\">\n",
    "           <input aria-haspopup=\"true\" aria-label=\"Change language variant\" class=\"vector-dropdown-checkbox\" data-event-name=\"ui.dropdown-p-variants\" id=\"p-variants-checkbox\" role=\"button\" type=\"checkbox\"/>\n",
    "           <label aria-hidden=\"true\" class=\"vector-dropdown-label\" for=\"p-variants-checkbox\" id=\"p-variants-label\">\n",
    "           </label>\n",
    "           <span class=\"vector-dropdown-label-text\">\n",
    "            English\n",
    "           </span>\n",
    "           <div class=\"vector-dropdown-content\">\n",
    "            <div class=\"vector-menu-content\">\n",
    "             <ul class=\"vector-menu-content-list\">\n",
    "             </ul>\n",
    "            </div>\n",
    "           </div>\n",
    "          </div>\n",
    "         </nav>\n",
    "        </div>\n",
    "        <div class=\"vector-collapsible\" id=\"right-navigation\">\n",
    "         <nav aria-label=\"Views\">\n",
    "          <div class=\"vector-menu vector-menu-tabs mw-portlet mw-portlet-views\" id=\"p-views\">\n",
    "           <div class=\"vector-menu-content\">\n",
    "            <ul class=\"vector-menu-content-list\">\n",
    "             <li class=\"selected vector-tab-noicon mw-list-item\" id=\"ca-view\">\n",
    "              <a class=\"\" data-mw=\"interface\" href=\"/wiki/Dual-phase_evolution\">\n",
    "               <span>\n",
    "                Read\n",
    "               </span>\n",
    "              </a>\n",
    "             </li>\n",
    "             <li class=\"vector-tab-noicon mw-list-item\" id=\"ca-edit\">\n",
    "              <a accesskey=\"e\" class=\"\" data-mw=\"interface\" href=\"/w/index.php?title=Dual-phase_evolution&amp;action=edit\" title=\"Edit this page [e]\">\n",
    "               <span>\n",
    "                Edit\n",
    "               </span>\n",
    "              </a>\n",
    "             </li>\n",
    "             <li class=\"vector-tab-noicon mw-list-item\" id=\"ca-history\">\n",
    "              <a accesskey=\"h\" class=\"\" data-mw=\"interface\" href=\"/w/index.php?title=Dual-phase_evolution&amp;action=history\" title=\"Past revisions of this page [h]\">\n",
    "               <span>\n",
    "                View history\n",
    "               </span>\n",
    "              </a>\n",
    "             </li>\n",
    "            </ul>\n",
    "           </div>\n",
    "          </div>\n",
    "         </nav>\n",
    "         <nav aria-label=\"More options\" class=\"vector-page-tools-landmark\">\n",
    "          <div class=\"vector-dropdown vector-page-tools-dropdown\" id=\"vector-page-tools-dropdown\">\n",
    "           <input aria-haspopup=\"true\" aria-label=\"Tools\" class=\"vector-dropdown-checkbox\" data-event-name=\"ui.dropdown-vector-page-tools-dropdown\" id=\"vector-page-tools-dropdown-checkbox\" role=\"button\" type=\"checkbox\"/>\n",
    "           <label aria-hidden=\"true\" class=\"vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet\" for=\"vector-page-tools-dropdown-checkbox\" id=\"vector-page-tools-dropdown-label\">\n",
    "           </label>\n",
    "           <span class=\"vector-dropdown-label-text\">\n",
    "            Tools\n",
    "           </span>\n",
    "           <div class=\"vector-dropdown-content\">\n",
    "            <div class=\"vector-unpinned-container\" id=\"vector-page-tools-unpinned-container\">\n",
    "             <div class=\"vector-page-tools vector-pinnable-element\" id=\"vector-page-tools\">\n",
    "              <div class=\"vector-pinnable-header vector-page-tools-pinnable-header vector-pinnable-header-unpinned\" data-feature-name=\"page-tools-pinned\" data-pinnable-element-id=\"vector-page-tools\" data-pinned-container-id=\"vector-page-tools-pinned-container\" data-unpinned-container-id=\"vector-page-tools-unpinned-container\">\n",
    "               <div class=\"vector-pinnable-header-label\">\n",
    "                Tools\n",
    "               </div>\n",
    "               <button class=\"vector-pinnable-header-toggle-button vector-pinnable-header-pin-button\" data-event-name=\"pinnable-header.vector-page-tools.pin\">\n",
    "                move to sidebar\n",
    "               </button>\n",
    "               <button class=\"vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button\" data-event-name=\"pinnable-header.vector-page-tools.unpin\">\n",
    "                hide\n",
    "               </button>\n",
    "              </div>\n",
    "              <div class=\"vector-menu mw-portlet mw-portlet-cactions emptyPortlet vector-has-collapsible-items\" id=\"p-cactions\" title=\"More options\">\n",
    "               <div class=\"vector-menu-heading\">\n",
    "                Actions\n",
    "               </div>\n",
    "               <div class=\"vector-menu-content\">\n",
    "                <ul class=\"vector-menu-content-list\">\n",
    "                 <li class=\"selected vector-more-collapsible-item mw-list-item\" id=\"ca-more-view\">\n",
    "                  <a href=\"/wiki/Dual-phase_evolution\">\n",
    "                   <span>\n",
    "                    Read\n",
    "                   </span>\n",
    "                  </a>\n",
    "                 </li>\n",
    "                 <li class=\"vector-more-collapsible-item mw-list-item\" id=\"ca-more-edit\">\n",
    "                  <a href=\"/w/index.php?title=Dual-phase_evolution&amp;action=edit\">\n",
    "                   <span>\n",
    "                    Edit\n",
    "                   </span>\n",
    "                  </a>\n",
    "                 </li>\n",
    "                 <li class=\"vector-more-collapsible-item mw-list-item\" id=\"ca-more-history\">\n",
    "                  <a href=\"/w/index.php?title=Dual-phase_evolution&amp;action=history\">\n",
    "                   <span>\n",
    "                    View history\n",
    "                   </span>\n",
    "                  </a>\n",
    "                 </li>\n",
    "                </ul>\n",
    "               </div>\n",
    "              </div>\n",
    "              <div class=\"vector-menu mw-portlet mw-portlet-tb\" id=\"p-tb\">\n",
    "               <div class=\"vector-menu-heading\">\n",
    "                General\n",
    "               </div>\n",
    "               <div class=\"vector-menu-content\">\n",
    "                <ul class=\"vector-menu-content-list\">\n",
    "                 <li class=\"mw-list-item\" id=\"t-whatlinkshere\">\n",
    "                  <a accesskey=\"j\" href=\"/wiki/Special:WhatLinksHere/Dual-phase_evolution\" title=\"List of all English Wikipedia pages containing links to this page [j]\">\n",
    "                   <span>\n",
    "                    What links here\n",
    "                   </span>\n",
    "                  </a>\n",
    "                 </li>\n",
    "                 <li class=\"mw-list-item\" id=\"t-recentchangeslinked\">\n",
    "                  <a accesskey=\"k\" href=\"/wiki/Special:RecentChangesLinked/Dual-phase_evolution\" rel=\"nofollow\" title=\"Recent changes in pages linked from this page [k]\">\n",
    "                   <span>\n",
    "                    Related changes\n",
    "                   </span>\n",
    "                  </a>\n",
    "                 </li>\n",
    "                 <li class=\"mw-list-item\" id=\"t-upload\">\n",
    "                  <a accesskey=\"u\" href=\"/wiki/Wikipedia:File_Upload_Wizard\" title=\"Upload files [u]\">\n",
    "                   <span>\n",
    "                    Upload file\n",
    "                   </span>\n",
    "                  </a>\n",
    "                 </li>\n",
    "                 <li class=\"mw-list-item\" id=\"t-specialpages\">\n",
    "                  <a accesskey=\"q\" href=\"/wiki/Special:SpecialPages\" title=\"A list of all special pages [q]\">\n",
    "                   <span>\n",
    "                    Special pages\n",
    "                   </span>\n",
    "                  </a>\n",
    "                 </li>\n",
    "                 <li class=\"mw-list-item\" id=\"t-permalink\">\n",
    "                  <a href=\"/w/index.php?title=Dual-phase_evolution&amp;oldid=1126289474\" title=\"Permanent link to this revision of this page\">\n",
    "                   <span>\n",
    "                    Permanent link\n",
    "                   </span>\n",
    "                  </a>\n",
    "                 </li>\n",
    "                 <li class=\"mw-list-item\" id=\"t-info\">\n",
    "                  <a href=\"/w/index.php?title=Dual-phase_evolution&amp;action=info\" title=\"More information about this page\">\n",
    "                   <span>\n",
    "                    Page information\n",
    "                   </span>\n",
    "                  </a>\n",
    "                 </li>\n",
    "                 <li class=\"mw-list-item\" id=\"t-cite\">\n",
    "                  <a href=\"/w/index.php?title=Special:CiteThisPage&amp;page=Dual-phase_evolution&amp;id=1126289474&amp;wpFormIdentifier=titleform\" title=\"Information on how to cite this page\">\n",
    "                   <span>\n",
    "                    Cite this page\n",
    "                   </span>\n",
    "                  </a>\n",
    "                 </li>\n",
    "                 <li class=\"mw-list-item\" id=\"t-wikibase\">\n",
    "                  <a accesskey=\"g\" href=\"https://www.wikidata.org/wiki/Special:EntityPage/Q25109659\" title=\"Structured data on this page hosted by Wikidata [g]\">\n",
    "                   <span>\n",
    "                    Wikidata item\n",
    "                   </span>\n",
    "                  </a>\n",
    "                 </li>\n",
    "                </ul>\n",
    "               </div>\n",
    "              </div>\n",
    "              <div class=\"vector-menu mw-portlet mw-portlet-coll-print_export\" id=\"p-coll-print_export\">\n",
    "               <div class=\"vector-menu-heading\">\n",
    "                Print/export\n",
    "               </div>\n",
    "               <div class=\"vector-menu-content\">\n",
    "                <ul class=\"vector-menu-content-list\">\n",
    "                 <li class=\"mw-list-item\" id=\"coll-download-as-rl\">\n",
    "                  <a href=\"/w/index.php?title=Special:DownloadAsPdf&amp;page=Dual-phase_evolution&amp;action=show-download-screen\" title=\"Download this page as a PDF file\">\n",
    "                   <span>\n",
    "                    Download as PDF\n",
    "                   </span>\n",
    "                  </a>\n",
    "                 </li>\n",
    "                 <li class=\"mw-list-item\" id=\"t-print\">\n",
    "                  <a accesskey=\"p\" href=\"/w/index.php?title=Dual-phase_evolution&amp;printable=yes\" title=\"Printable version of this page [p]\">\n",
    "                   <span>\n",
    "                    Printable version\n",
    "                   </span>\n",
    "                  </a>\n",
    "                 </li>\n",
    "                </ul>\n",
    "               </div>\n",
    "              </div>\n",
    "             </div>\n",
    "            </div>\n",
    "           </div>\n",
    "          </div>\n",
    "         </nav>\n",
    "        </div>\n",
    "       </div>\n",
    "      </div>\n",
    "      <div class=\"vector-column-end\">\n",
    "       <nav aria-label=\"More options\" class=\"vector-page-tools-landmark vector-sticky-pinned-container\">\n",
    "        <div class=\"vector-pinned-container\" id=\"vector-page-tools-pinned-container\">\n",
    "        </div>\n",
    "       </nav>\n",
    "      </div>\n",
    "      <div aria-labelledby=\"firstHeading\" class=\"vector-body\" data-mw-ve-target-container=\"\" id=\"bodyContent\">\n",
    "       <div class=\"vector-body-before-content\">\n",
    "        <div class=\"mw-indicators\">\n",
    "        </div>\n",
    "        <div class=\"noprint\" id=\"siteSub\">\n",
    "         From Wikipedia, the free encyclopedia\n",
    "        </div>\n",
    "       </div>\n",
    "       <div id=\"contentSub\">\n",
    "        <div id=\"mw-content-subtitle\">\n",
    "        </div>\n",
    "       </div>\n",
    "       <div class=\"mw-body-content mw-content-ltr\" dir=\"ltr\" id=\"mw-content-text\" lang=\"en\">\n",
    "        <div class=\"mw-parser-output\">\n",
    "         <div class=\"shortdescription nomobile noexcerpt noprint searchaux\" style=\"display:none\">\n",
    "          Process that drives self-organization within complex adaptive systems\n",
    "         </div>\n",
    "         <table class=\"box-Multiple_issues plainlinks metadata ambox ambox-content ambox-multiple_issues compact-ambox\" role=\"presentation\">\n",
    "          <tbody>\n",
    "           <tr>\n",
    "            <td class=\"mbox-image\">\n",
    "             <div class=\"mbox-image-div\">\n",
    "              <img alt=\"\" data-file-height=\"40\" data-file-width=\"40\" decoding=\"async\" height=\"40\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/b/b4/Ambox_important.svg/40px-Ambox_important.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/b/b4/Ambox_important.svg/60px-Ambox_important.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/b/b4/Ambox_important.svg/80px-Ambox_important.svg.png 2x\" width=\"40\"/>\n",
    "             </div>\n",
    "            </td>\n",
    "            <td class=\"mbox-text\">\n",
    "             <div class=\"mbox-text-span\">\n",
    "              <div class=\"multiple-issues-text mw-collapsible\">\n",
    "               <b>\n",
    "                This article has multiple issues.\n",
    "               </b>\n",
    "               Please help\n",
    "               <b>\n",
    "                <a class=\"external text\" href=\"https://en.wikipedia.org/w/index.php?title=Dual-phase_evolution&amp;action=edit\">\n",
    "                 improve it\n",
    "                </a>\n",
    "               </b>\n",
    "               or discuss these issues on the\n",
    "               <b>\n",
    "                <a href=\"/wiki/Talk:Dual-phase_evolution\" title=\"Talk:Dual-phase evolution\">\n",
    "                 talk page\n",
    "                </a>\n",
    "               </b>\n",
    "               .\n",
    "               <small>\n",
    "                <i>\n",
    "                 (\n",
    "                 <a href=\"/wiki/Help:Maintenance_template_removal\" title=\"Help:Maintenance template removal\">\n",
    "                  Learn how and when to remove these template messages\n",
    "                 </a>\n",
    "                 )\n",
    "                </i>\n",
    "               </small>\n",
    "               <div class=\"mw-collapsible-content\">\n",
    "                <link href=\"mw-data:TemplateStyles:r1097763485\" rel=\"mw-deduplicated-inline-style\"/>\n",
    "                <table class=\"box-More_citations_needed plainlinks metadata ambox ambox-content ambox-Refimprove\" role=\"presentation\">\n",
    "                 <tbody>\n",
    "                  <tr>\n",
    "                   <td class=\"mbox-image\">\n",
    "                    <div class=\"mbox-image-div\">\n",
    "                     <a class=\"image\" href=\"/wiki/File:Question_book-new.svg\">\n",
    "                      <img alt=\"\" data-file-height=\"399\" data-file-width=\"512\" decoding=\"async\" height=\"39\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/50px-Question_book-new.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/75px-Question_book-new.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/100px-Question_book-new.svg.png 2x\" width=\"50\"/>\n",
    "                     </a>\n",
    "                    </div>\n",
    "                   </td>\n",
    "                   <td class=\"mbox-text\">\n",
    "                    <div class=\"mbox-text-span\">\n",
    "                     This article\n",
    "                     <b>\n",
    "                      needs additional citations for\n",
    "                      <a href=\"/wiki/Wikipedia:Verifiability\" title=\"Wikipedia:Verifiability\">\n",
    "                       verification\n",
    "                      </a>\n",
    "                     </b>\n",
    "                     .\n",
    "                     <span class=\"hide-when-compact\">\n",
    "                      Please help\n",
    "                      <a class=\"external text\" href=\"https://en.wikipedia.org/w/index.php?title=Dual-phase_evolution&amp;action=edit\">\n",
    "                       improve this article\n",
    "                      </a>\n",
    "                      by\n",
    "                      <a href=\"/wiki/Help:Referencing_for_beginners\" title=\"Help:Referencing for beginners\">\n",
    "                       adding citations to reliable sources\n",
    "                      </a>\n",
    "                      . Unsourced material may be challenged and removed.\n",
    "                      <br/>\n",
    "                      <small>\n",
    "                       <span class=\"plainlinks\">\n",
    "                        <i>\n",
    "                         Find sources:\n",
    "                        </i>\n",
    "                        <a class=\"external text\" href=\"https://www.google.com/search?as_eq=wikipedia&amp;q=%22Dual-phase+evolution%22\" rel=\"nofollow\">\n",
    "                         \"Dual-phase evolution\"\n",
    "                        </a>\n",
    "                        –\n",
    "                        <a class=\"external text\" href=\"https://www.google.com/search?tbm=nws&amp;q=%22Dual-phase+evolution%22+-wikipedia&amp;tbs=ar:1\" rel=\"nofollow\">\n",
    "                         news\n",
    "                        </a>\n",
    "                        <b>\n",
    "                         ·\n",
    "                        </b>\n",
    "                        <a class=\"external text\" href=\"https://www.google.com/search?&amp;q=%22Dual-phase+evolution%22&amp;tbs=bkt:s&amp;tbm=bks\" rel=\"nofollow\">\n",
    "                         newspapers\n",
    "                        </a>\n",
    "                        <b>\n",
    "                         ·\n",
    "                        </b>\n",
    "                        <a class=\"external text\" href=\"https://www.google.com/search?tbs=bks:1&amp;q=%22Dual-phase+evolution%22+-wikipedia\" rel=\"nofollow\">\n",
    "                         books\n",
    "                        </a>\n",
    "                        <b>\n",
    "                         ·\n",
    "                        </b>\n",
    "                        <a class=\"external text\" href=\"https://scholar.google.com/scholar?q=%22Dual-phase+evolution%22\" rel=\"nofollow\">\n",
    "                         scholar\n",
    "                        </a>\n",
    "                        <b>\n",
    "                         ·\n",
    "                        </b>\n",
    "                        <a class=\"external text\" href=\"https://www.jstor.org/action/doBasicSearch?Query=%22Dual-phase+evolution%22&amp;acc=on&amp;wc=on\" rel=\"nofollow\">\n",
    "                         JSTOR\n",
    "                        </a>\n",
    "                       </span>\n",
    "                      </small>\n",
    "                     </span>\n",
    "                     <span class=\"date-container\">\n",
    "                      <i>\n",
    "                       (\n",
    "                       <span class=\"date\">\n",
    "                        May 2015\n",
    "                       </span>\n",
    "                       )\n",
    "                      </i>\n",
    "                     </span>\n",
    "                     <span class=\"hide-when-compact\">\n",
    "                      <i>\n",
    "                       (\n",
    "                       <small>\n",
    "                        <a href=\"/wiki/Help:Maintenance_template_removal\" title=\"Help:Maintenance template removal\">\n",
    "                         Learn how and when to remove this template message\n",
    "                        </a>\n",
    "                       </small>\n",
    "                       )\n",
    "                      </i>\n",
    "                     </span>\n",
    "                    </div>\n",
    "                   </td>\n",
    "                  </tr>\n",
    "                 </tbody>\n",
    "                </table>\n",
    "                <link href=\"mw-data:TemplateStyles:r1097763485\" rel=\"mw-deduplicated-inline-style\"/>\n",
    "                <table class=\"box-Technical plainlinks metadata ambox ambox-style ambox-technical\" role=\"presentation\">\n",
    "                 <tbody>\n",
    "                  <tr>\n",
    "                   <td class=\"mbox-image\">\n",
    "                    <div class=\"mbox-image-div\">\n",
    "                     <img alt=\"\" data-file-height=\"48\" data-file-width=\"48\" decoding=\"async\" height=\"40\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/f/f2/Edit-clear.svg/40px-Edit-clear.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/f/f2/Edit-clear.svg/60px-Edit-clear.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/f/f2/Edit-clear.svg/80px-Edit-clear.svg.png 2x\" width=\"40\"/>\n",
    "                    </div>\n",
    "                   </td>\n",
    "                   <td class=\"mbox-text\">\n",
    "                    <div class=\"mbox-text-span\">\n",
    "                     This article\n",
    "                     <b>\n",
    "                      may be too technical for most readers to understand\n",
    "                     </b>\n",
    "                     .\n",
    "                     <span class=\"hide-when-compact\">\n",
    "                      Please\n",
    "                      <a class=\"external text\" href=\"https://en.wikipedia.org/w/index.php?title=Dual-phase_evolution&amp;action=edit\">\n",
    "                       help improve it\n",
    "                      </a>\n",
    "                      to\n",
    "                      <a href=\"/wiki/Wikipedia:Make_technical_articles_understandable\" title=\"Wikipedia:Make technical articles understandable\">\n",
    "                       make it understandable to non-experts\n",
    "                      </a>\n",
    "                      , without removing the technical details.\n",
    "                     </span>\n",
    "                     <span class=\"date-container\">\n",
    "                      <i>\n",
    "                       (\n",
    "                       <span class=\"date\">\n",
    "                        May 2015\n",
    "                       </span>\n",
    "                       )\n",
    "                      </i>\n",
    "                     </span>\n",
    "                     <span class=\"hide-when-compact\">\n",
    "                      <i>\n",
    "                       (\n",
    "                       <small>\n",
    "                        <a href=\"/wiki/Help:Maintenance_template_removal\" title=\"Help:Maintenance template removal\">\n",
    "                         Learn how and when to remove this template message\n",
    "                        </a>\n",
    "                       </small>\n",
    "                       )\n",
    "                      </i>\n",
    "                     </span>\n",
    "                    </div>\n",
    "                   </td>\n",
    "                  </tr>\n",
    "                 </tbody>\n",
    "                </table>\n",
    "               </div>\n",
    "              </div>\n",
    "              <span class=\"hide-when-compact\">\n",
    "               <i>\n",
    "                (\n",
    "                <small>\n",
    "                 <a href=\"/wiki/Help:Maintenance_template_removal\" title=\"Help:Maintenance template removal\">\n",
    "                  Learn how and when to remove this template message\n",
    "                 </a>\n",
    "                </small>\n",
    "                )\n",
    "               </i>\n",
    "              </span>\n",
    "             </div>\n",
    "            </td>\n",
    "           </tr>\n",
    "          </tbody>\n",
    "         </table>\n",
    "         <p>\n",
    "          <b>\n",
    "           Dual phase evolution\n",
    "          </b>\n",
    "          (\n",
    "          <b>\n",
    "           DPE\n",
    "          </b>\n",
    "          ) is a process that drives\n",
    "          <a href=\"/wiki/Self-organization\" title=\"Self-organization\">\n",
    "           self-organization\n",
    "          </a>\n",
    "          within\n",
    "          <a href=\"/wiki/Complex_adaptive_system\" title=\"Complex adaptive system\">\n",
    "           complex adaptive systems\n",
    "          </a>\n",
    "          .\n",
    "          <sup class=\"reference\" id=\"cite_ref-DPE2_1-0\">\n",
    "           <a href=\"#cite_note-DPE2-1\">\n",
    "            [1]\n",
    "           </a>\n",
    "          </sup>\n",
    "          It arises in response to phase changes within the network of connections formed by a system's components. DPE occurs in a wide range of physical, biological and social systems. Its applications to technology include methods for manufacturing novel materials and algorithms to solve complex problems in computation.\n",
    "         </p>\n",
    "         <meta property=\"mw:PageProp/toc\">\n",
    "          <h2>\n",
    "           <span class=\"mw-headline\" id=\"Introduction\">\n",
    "            Introduction\n",
    "           </span>\n",
    "           <span class=\"mw-editsection\">\n",
    "            <span class=\"mw-editsection-bracket\">\n",
    "             [\n",
    "            </span>\n",
    "            <a href=\"/w/index.php?title=Dual-phase_evolution&amp;action=edit&amp;section=1\" title=\"Edit section: Introduction\">\n",
    "             edit\n",
    "            </a>\n",
    "            <span class=\"mw-editsection-bracket\">\n",
    "             ]\n",
    "            </span>\n",
    "           </span>\n",
    "          </h2>\n",
    "          <p>\n",
    "           Dual phase evolution (DPE) is a process that promotes the emergence of large-scale order in\n",
    "           <a class=\"mw-redirect\" href=\"/wiki/Complex_systems\" title=\"Complex systems\">\n",
    "            complex systems\n",
    "           </a>\"\"\"\n",
    "\n",
    "working_code = \"\"\"from bs4 import BeautifulSoup, Comment\n",
    "import requests\n",
    "\n",
    "# Load the webpage content\n",
    "url = 'http://your-website-url.com'\n",
    "r = requests.get(url)\n",
    "html_content = r.text\n",
    "\n",
    "# Create a BeautifulSoup object\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Remove unwanted tags: script, style, [document], head, title\n",
    "for element in soup([\"script\", \"style\", \"head\", \"title\", \"[document]\"]):\n",
    "    element.decompose()\n",
    "\n",
    "# Also remove HTML comments\n",
    "for element in soup.find_all(text=lambda text: isinstance(text, Comment)):\n",
    "    element.extract()\n",
    "\n",
    "# Define the tags to find\n",
    "tags = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']\n",
    "found_tags = soup.find_all(tags)\n",
    "\n",
    "# Extract tags and their associated content into a dictionary\n",
    "doc_dict = {}\n",
    "for tag in found_tags:\n",
    "    content = []\n",
    "    next_sibling = tag.find_next_sibling()\n",
    "\n",
    "    while next_sibling and next_sibling.name not in tags:\n",
    "        if next_sibling.name == 'p':\n",
    "            content.append(next_sibling.get_text(strip=True))\n",
    "        next_sibling = next_sibling.find_next_sibling()\n",
    "\n",
    "    doc_dict[tag.get_text(strip=True)] = \" \".join(content)\n",
    "\n",
    "# Print the resulting dictionary\n",
    "for k, v in doc_dict.items():\n",
    "    print(f'{k}: {v}')\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"The following code was created by GPT-4 and successfully for each header tag finds and returns all the subsequent p tags until the next header tag and keeps doing that for each header tag and adds them to a dict.\n",
    "\n",
    "However, it does not find the p tag for the h1 tag. Please correct it so that\n",
    "it finds the p tag after the h1 tag\n",
    "\n",
    "Working code: ####\n",
    "{working_code}\n",
    "####\n",
    "\n",
    "html soup: ####\n",
    "{broken_html}\n",
    "\"\"\"\n",
    "llm_16k.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint('To fix the code, you need to modify the while loop condition to include the h1 tag. Here\\'s the modified code:\\n\\n```python\\ndoc_dict = {}\\nfor tag in found_tags:\\n    content = []\\n    next_sibling = tag.find_next_sibling()\\n\\n    while next_sibling and (next_sibling.name not in tags or tag.name == \\'h1\\'):\\n        if next_sibling.name == \\'p\\':\\n            content.append(next_sibling.get_text(strip=True))\\n        next_sibling = next_sibling.find_next_sibling()\\n\\n    doc_dict[tag.get_text(strip=True)] = \" \".join(content)\\n```\\n\\nBy adding `tag.name == \\'h1\\'` to the while loop condition, it will include the p tags after the h1 tag as well.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_code = \"\"\"from bs4 import BeautifulSoup, Comment\n",
    "import requests\n",
    "\n",
    "# Load the webpage content\n",
    "url = 'http://your-website-url.com'\n",
    "r = requests.get(url)\n",
    "html_content = r.text\n",
    "\n",
    "# Create a BeautifulSoup object\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Remove unwanted tags: script, style, [document], head, title\n",
    "for element in soup([\"script\", \"style\", \"head\", \"title\", \"[document]\"]):\n",
    "    element.decompose()\n",
    "\n",
    "# Also remove HTML comments\n",
    "for element in soup.find_all(text=lambda text: isinstance(text, Comment)):\n",
    "    element.extract()\n",
    "\n",
    "# Define the tags to find\n",
    "tags = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']\n",
    "found_tags = soup.find_all(tags)\n",
    "\n",
    "# Extract tags and their associated content into a dictionary\n",
    "doc_dict = {}\n",
    "for tag in found_tags:\n",
    "    content = []\n",
    "    next_sibling = tag.find_next_sibling()\n",
    "\n",
    "    while next_sibling and next_sibling.name not in tags:\n",
    "        if next_sibling.name not in [\"script\", \"style\", \"head\", \"title\", \"[document]\"]:\n",
    "            content.append(next_sibling.get_text(strip=True))\n",
    "        next_sibling = next_sibling.find_next_sibling()\n",
    "\n",
    "    doc_dict[tag.get_text(strip=True)] = \" \".join(content)\n",
    "\n",
    "# Print the resulting dictionary\n",
    "for k, v in doc_dict.items():\n",
    "    print(f'{k}: {v}')\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"The following code was created by GPT-4 and successfully achives this goal: I only care about what a person reading the page will read. So just get me the headers and actual content text content that a human would see. The current code I tried gets things like 'document.documentElement.className=\"client-js ' which is obviously js code that the user would not read. add everything to a dict in the order you find them.\n",
    "\n",
    "However, it does not find the p tag for the h1 tag. Please correct it so that\n",
    "it finds the p tag after the h1 tag\n",
    "\n",
    "Working code: ####\n",
    "{working_code}\n",
    "####\n",
    "\n",
    "html soup: ####\n",
    "{broken_html}\n",
    "####\n",
    "\"\"\"\n",
    "output = llm_16k.predict(prompt)\n",
    "pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_code = \"\"\"from bs4 import BeautifulSoup, Comment\n",
    "import requests\n",
    "\n",
    "# Load the webpage content\n",
    "url = 'http://your-website-url.com'\n",
    "r = requests.get(url)\n",
    "html_content = r.text\n",
    "\n",
    "# Create a BeautifulSoup object\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Remove unwanted tags: script, style, [document], head, title\n",
    "for element in soup([\"script\", \"style\", \"head\", \"title\", \"[document]\"]):\n",
    "    element.decompose()\n",
    "\n",
    "# Also remove HTML comments\n",
    "for element in soup.find_all(text=lambda text: isinstance(text, Comment)):\n",
    "    element.extract()\n",
    "\n",
    "# Define the tags to find\n",
    "tags = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']\n",
    "found_tags = soup.find_all(tags)\n",
    "\n",
    "# Extract tags and their associated content into a dictionary\n",
    "doc_dict = {}\n",
    "for tag in found_tags:\n",
    "    content = []\n",
    "    next_sibling = tag.find_next_sibling()\n",
    "\n",
    "    while next_sibling and next_sibling.name not in tags:\n",
    "        if next_sibling.name not in [\"script\", \"style\", \"head\", \"title\", \"[document]\"]:\n",
    "            content.append(next_sibling.get_text(strip=True))\n",
    "        next_sibling = next_sibling.find_next_sibling()\n",
    "\n",
    "    doc_dict[tag.get_text(strip=True)] = \" \".join(content)\n",
    "\n",
    "# Print the resulting dictionary\n",
    "for k, v in doc_dict.items():\n",
    "    print(f'{k}: {v}')\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"The following code finds all the p tags after header tags. But it does not find the p tags for the h1 tag. Why does it not find it?\n",
    "\n",
    "Code: ####\n",
    "{working_code}\n",
    "####\n",
    "\n",
    "html soup: ####\n",
    "{broken_html}\n",
    "####\n",
    "\"\"\"\n",
    "output = llm_16k.predict(prompt)\n",
    "pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Introduction\n",
    "The DPE mechanism\n",
    "Underlying network\n",
    "Phase shifts\n",
    "Selection and variation\n",
    "System memory\n",
    "Examples\n",
    "Social networks\n",
    "Socio-economics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortest_article = all_articles[2]\n",
    "shortest_article.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_16k.predict(f\"Summarize the following text: {shortest_article.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_default.predict(f'Translate the following text into German: {all_articles[3].page_content}')\n",
    "output = llm_16k.predict(f'Translate the following text into German: {all_articles[3].page_content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens_from_string(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_articles[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_articles[3].page_content)\n",
    "\n",
    "enc = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "len(all_articles[3].page_content.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens_from_string(all_articles[3].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
