{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4451f83c1314d3ab3ab4f52d2ac6797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/275 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7e148b0e1a4c3fb8e1cebf920ca722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/348 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6adcff81cc4a3c9f3f43684947f0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8139c9f0b28d448ea20a455cb28f221d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f2abd5ae6147299a6e5d1c7063a6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3973942989cf45d2bb8516a51a6c602e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6eec9b3f1764c1fa5d47104b5cedf2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'LayoutLMv3TokenizerFast'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f291d26daf784d0f8f649f419efc6053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from copy import copy, deepcopy\n",
    "from pathlib import Path\n",
    "from pprint import pprint, PrettyPrinter\n",
    "from time import time, sleep\n",
    "from typing import List, Dict\n",
    "from uuid import uuid4\n",
    "from collections import defaultdict\n",
    "\n",
    "import evaluate\n",
    "import openai\n",
    "import requests\n",
    "import tiktoken\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from doctran import Doctran, ExtractProperty\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from evaluate import load\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.embeddings import (\n",
    "    OpenAIEmbeddings,\n",
    "    HuggingFaceEmbeddings,\n",
    ")\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.text_splitter import (\n",
    "    MarkdownTextSplitter,\n",
    "    MarkdownHeaderTextSplitter,\n",
    "    LineType,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForTokenClassification\n",
    "from loguru import logger\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "from main import (\n",
    "    divide_sections_if_too_large,\n",
    "    extract_plan_and_content_wikipedia,\n",
    "    compare_documents_content,\n",
    "    compare_documents_sections,\n",
    "    extract_plan_and_content_patent,\n",
    "    extract_plan_and_content_arxiv,\n",
    "    load_arxiv_paper,\n",
    ")\n",
    "\n",
    "import pytesseract\n",
    "\n",
    "# If you don't have tesseract executable in your PATH, include the following:\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract\"\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "llm_default = ChatOpenAI(model_name=\"gpt-3.5-turbo\", streaming=True)\n",
    "llm_16k = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\", streaming=True)\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str = \"gpt-3.5-turbo\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.get_encoding(encoding_name)\n",
    "    except ValueError:\n",
    "        encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def convert_to_markdown(article_dict):\n",
    "    md_text = \"\"\n",
    "\n",
    "    for heading, content in article_dict.items():\n",
    "        # heading is of form: 'h3 Example'\n",
    "        # Define the markdown equivalent for the heading level\n",
    "        heading_level = \"#\" * int(heading[1])\n",
    "        heading = heading[3:]\n",
    "        # Append the heading and the content to the markdown text\n",
    "        md_text += f\"{heading_level} {heading}\\n\\n{content}\\n\\n\"\n",
    "\n",
    "    return md_text\n",
    "\n",
    "\n",
    "def truncated_pprint(obj, N=5):\n",
    "    \"\"\"Pretty print an object, truncating lists and strings to N items/characters\n",
    "    for easier viewing of plan_json objects\"\"\"\n",
    "    def truncate(item, N):\n",
    "        if isinstance(item, list) and N is not None:\n",
    "            return item[:N] + (['...'] if len(item) > N else [])\n",
    "        if isinstance(item, str) and N is not None:\n",
    "            N = 125\n",
    "            return item[:N] + ('...' if len(item) > N else '')\n",
    "        return item\n",
    "\n",
    "    def trunc_recursive(item, N):\n",
    "        if isinstance(item, list):\n",
    "            return [trunc_recursive(i, N) for i in truncate(item, N)]\n",
    "        elif isinstance(item, dict):\n",
    "            return {k: trunc_recursive(v, N) for k, v in item.items()}\n",
    "        else:\n",
    "            return truncate(item, N)\n",
    "\n",
    "    truncated_obj = trunc_recursive(obj, N)\n",
    "    pprint(truncated_obj, sort_dicts=False)\n",
    "\n",
    "# Test\n",
    "data = {\n",
    "    'long_list': list(range(100)),\n",
    "    'long_string': 'a' * 100,\n",
    "    'nested': {\n",
    "        'nested_list': list(range(50)),\n",
    "        'nested_string': 'b' * 50\n",
    "    }\n",
    "}\n",
    "\n",
    "# truncated_pprint(data, 5)\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"nielsr/layoutlmv3-finetuned-funsd\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"nielsr/layoutlmv3-finetuned-funsd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-08-16 14:28:12.872\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_compare_documents\u001b[0m:\u001b[36m629\u001b[0m - \u001b[1m\n",
      "\tStarting to compare two documents on section:\n",
      "\t\tID: ffb4e5bd-d27b-44e2-a56f-da9090853629 Title: Dual-phase evolution\n",
      "\t\tID: ffb4e5bd-d27b-44e2-a56f-da9090853629 Title: Dual-phase evolution\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353f01ba483d4c3aa6c24d2942ab1708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.63k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db4c40f061f492abb6ed437252d44f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-08-16 14:28:15.831\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36m_compare_documents\u001b[0m:\u001b[36m642\u001b[0m - \u001b[1m\n",
      "\tID: ffb4e5bd-d27b-44e2-a56f-da9090853629 Title: Dual-phase evolution has 14 sections.\n",
      "\tID: ffb4e5bd-d27b-44e2-a56f-da9090853629 Title: Dual-phase evolution has 14 sections.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819e70c3feba44138119697f796e0064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48efe30841f047dbb997507458450a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1246db08b104d3a955a30512a4392a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2f6fc151874b02b91b03f1803e8cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing text...\n",
      "Loading tokenizer\n",
      "Loading model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57935768cc3c4848a8eef0cb2b82ae91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tests import run_tests\n",
    "\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = compare_documents_sections(\n",
    "    'output/wikipedia/Dual-phase evolution.json',\n",
    "    'output/wikipedia/Dual-phase evolution.json',\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_approximately_equal(x, y, epsilon=1e-10):\n",
    "    return abs(x - y) < epsilon\n",
    "\n",
    "for value in result['plan_total_similarity'].values():\n",
    "    assert is_approximately_equal(value, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = compare_documents_sections(\n",
    "    \"output/wikipedia/Dual-phase evolution.json\",\n",
    "    \"output/wikipedia/Climate Change.json\",\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = compare_documents_sections(\n",
    "    \"output/wikipedia/Dual-phase evolution.json\",\n",
    "    \"output/wikipedia/Climate Change.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(Path('data/patents').glob('*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/arxiv/A_Survey_of_Software-Defined_Smart_Grid_Networks_Security_Threats_and__Defense_Techniques.pdf'\n",
    "plan_json = await extract_plan_and_content_arxiv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_pprint(plan_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/arxiv/A_Survey_of_Software-Defined_Smart_Grid_Networks_Security_Threats_and__Defense_Techniques.pdf'\n",
    "article_dict = load_arxiv_paper(path)\n",
    "article_dict = await divide_sections_if_too_large(article_dict, doc_type=\"arxiv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_papers = [\n",
    "    \"https://arxiv.org/pdf/2307.04438.pdf\",\n",
    "    \"https://arxiv.org/pdf/2306.14697.pdf\",\n",
    "    \"https://arxiv.org/pdf/2302.09051.pdf\",\n",
    "    \"https://arxiv.org/pdf/2305.10091.pdf\",\n",
    "    \"https://arxiv.org/pdf/2305.17474.pdf\",\n",
    "    \"https://arxiv.org/pdf/2306.16960.pdf\",\n",
    "    \"https://arxiv.org/pdf/2305.20069.pdf\",\n",
    "    \"https://arxiv.org/pdf/2306.08451.pdf\",\n",
    "    \"https://arxiv.org/pdf/2306.17003.pdf\",\n",
    "    \"https://arxiv.org/pdf/2307.07573.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    \"\"\"Convert string to a valid filename.\"\"\"\n",
    "    s = str(filename).strip().replace(' ', '_')\n",
    "    # Remove any character that is not a word character\n",
    "    # (alphanumeric + underscore), not a hyphen, or not a period.\n",
    "    return re.sub(r'(?u)[^-\\w.]', '', s)\n",
    "\n",
    "def get_arxiv_metadata(arxiv_id):\n",
    "    \"\"\"Fetch metadata for the given arXiv ID.\"\"\"\n",
    "    url = f'https://export.arxiv.org/api/query?id_list={arxiv_id}'\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    data = response.text\n",
    "\n",
    "    # Use regex to extract the title. There are better ways (like parsing XML),\n",
    "    # but this is simple and should work for our purpose.\n",
    "    match = re.search(r'<title>([^<]+)</title>', data)\n",
    "    title = match.group(1) if match else None\n",
    "    return {'title': title}\n",
    "\n",
    "def download_arxiv_pdf(arxiv_url):\n",
    "    \"\"\"Given a arxiv_url, download the PDF to the data/arxiv directory.\"\"\"\n",
    "    arxiv_id = arxiv_url.split('/')[-1].replace('.pdf', '')\n",
    "    metadata = get_arxiv_metadata(arxiv_id)\n",
    "\n",
    "    if metadata.get('title'):\n",
    "        filename = sanitize_filename(metadata['title']) + '.pdf'\n",
    "    else:\n",
    "        filename = f\"{arxiv_id}.pdf\"\n",
    "\n",
    "    arxiv_dir = Path('data/arxiv')\n",
    "    os.makedirs(arxiv_dir, exist_ok=True)\n",
    "\n",
    "    response = requests.get(arxiv_url)\n",
    "    output_file = arxiv_dir / filename\n",
    "    with open(output_file, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Downloaded to {output_file}\")\n",
    "\n",
    "# Example usage:\n",
    "arxiv_url = 'https://arxiv.org/pdf/2306.14697.pdf'\n",
    "# download_arxiv_pdf(arxiv_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper in arxiv_papers:\n",
    "    download_arxiv_pdf(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Path('data/arxiv/A_Survey_of_Software-Defined_Smart_Grid_Networks_Security_Threats_and__Defense_Techniques.pdf')\n",
    "text = extract_text(doc)\n",
    "\n",
    "doc_2 = Path('data/arxiv/A_Survey_on_Blood_Pressure_Measurement_Technologies_Addressing__Potential_Sources_of_Bias.pdf')\n",
    "text_2 = extract_text(doc_2)\n",
    "\n",
    "doc_3 = Path('data/arxiv/A_survey_on_algebraic_dilatations.pdf')\n",
    "text_3 = extract_text(doc_3)\n",
    "\n",
    "doc_4 = Path('data/arxiv/A_survey_on_the_complexity_of_learning_quantum_states.pdf')\n",
    "text_4 = extract_text(doc_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = doc.stem\n",
    "title = title.replace('_', ' ')\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pattern searches for \"abstract\" followed by any content.\n",
    "# Then, it looks for one of the potential following sections:\n",
    "# \"I. Introduction\", \"1. Introduction\", or \"Contents\".\n",
    "# pattern = r'abstract(.*?)(i\\. introduction|1\\. introduction|contents)'\n",
    "\n",
    "# The pattern searches for \"abstract\" followed by any content.\n",
    "# Then, it looks for one of the potential following sections:\n",
    "# \"I. Introduction\", \"1. Introduction\", or \"Contents\".\n",
    "# We use a positive lookahead (?=...) to assert that the introduction or contents\n",
    "# pattern exists, but we don't include it in the main match.\n",
    "pattern = r'abstract(.*?)(?=(i\\. introduction|1\\. introduction|contents))'\n",
    "\n",
    "\n",
    "# The re.DOTALL flag allows the . in the pattern to match newline characters,\n",
    "match = re.search(pattern, text.lower(), re.DOTALL)\n",
    "\n",
    "if match:\n",
    "    abstract_start = match.start()\n",
    "    abstract_end = match.end()\n",
    "    abstract = match.group(1).strip()  # Extracted abstract content\n",
    "    print(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'references\\n'\n",
    "# regions = []\n",
    "matches = [match for match in re.finditer(pattern, text.lower())]\n",
    "\n",
    "references = ''\n",
    "if matches:\n",
    "    final_match = matches[-1]\n",
    "    reference_start = final_match.start()\n",
    "    reference_end = final_match.end()\n",
    "    references = text[reference_start:]\n",
    "print(references[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = text[abstract_end:reference_start]\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_dict = {\n",
    "    'Title': title,\n",
    "    'Abstract': abstract,\n",
    "    'Content': content,\n",
    "    'References': references,\n",
    "}\n",
    "truncated_pprint(article_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dict = await divide_sections_if_too_large(article_dict, doc_type='arxiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(list(split_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[reference_start: reference_start+250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = text[abstract_end:reference_start]\n",
    "content[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [text, text_2, text_3, text_4]\n",
    "for t in texts:\n",
    "    print(repr(t[:500]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern details:\n",
    "# 1. `^` asserts start of a line.\n",
    "# 2. `(.*?)` captures everything lazily.\n",
    "# 3. The lookahead `(?=...)` asserts that what directly follows is:\n",
    "#   a. an email-like pattern, OR\n",
    "#   b. words like \"University\", \"Department\", \"Research Center\", OR\n",
    "#   c. a date pattern (e.g., \"July 12, 2023\").\n",
    "pattern = r'^.*?(?=\\S+@\\S+|\\bUniversity\\b|\\bDepartment\\b|\\bResearch Center\\b|\\b[\\w\\s]{3,20}\\b \\d{1,2}, \\d{4})'\n",
    "\n",
    "for text in texts:\n",
    "    match = re.search(pattern, text, re.DOTALL | re.MULTILINE)\n",
    "    if match:\n",
    "        title = match.group(0).strip()\n",
    "        print(title)\n",
    "        print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'references\\n' in text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'references\\n', text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'abstract'\n",
    "regions = []\n",
    "for match in re.finditer(pattern, text.lower()):\n",
    "    # regions.append(text[match.start():])\n",
    "    start_index = max(0, match.start() - 250)\n",
    "    end_index = min(len(text), match.end() + 250)\n",
    "    regions.append(text[start_index:end_index])\n",
    "\n",
    "# Print the regions\n",
    "for i, region in enumerate(regions, 1):\n",
    "    print(f\"Match {i}:\\n{region}\\n{'-'*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'abstract', text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_dict = {'article': text}\n",
    "split_dict = await divide_sections_if_too_large(article_dict, doc_type='arxiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(list(split_dict.keys()), width=125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens_from_string(text.lower().split('references\\n')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[num_tokens_from_string(x) for x in split_dict.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split_dict['A Survey of Software-Defined Smart Grid Networks: Security Threats and Defense Techniques'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in sorted(split_dict.keys(), key=len, reverse=True):\n",
    "    print(f\"Length: {len(x)}, num tokens: {num_tokens_from_string(x)}\")\n",
    "    print(x)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens_from_string(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from pdfminer.high_level import extract_text_to_fp\n",
    "from pdfminer.layout import LAParams\n",
    "output_string = StringIO()\n",
    "with open(doc, 'rb') as fin:\n",
    "    extract_text_to_fp(fin, output_string, laparams=LAParams(),\n",
    "                       output_type='html', codec=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(output_string.getvalue(), \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
