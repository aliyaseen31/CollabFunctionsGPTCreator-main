{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "URxFGfSNvKpS",
        "outputId": "6baee03f-b6b1-4b99-86b7-9fe5df14695d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.1.6-py3-none-any.whl (22 kB)\n",
            "Collecting neo4j\n",
            "  Downloading neo4j-5.20.0.tar.gz (202 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.0/203.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jsonify\n",
            "  Downloading jsonify-0.5.tar.gz (1.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.0.2)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from neo4j) (2023.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask) (2.1.5)\n",
            "Building wheels for collected packages: neo4j, jsonify\n",
            "  Building wheel for neo4j (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neo4j: filename=neo4j-5.20.0-py3-none-any.whl size=280771 sha256=8251e60e4463218714ea79e3946cdcf001a8efcd6444924fa7fdf0b4076911d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/12/66/764554d079caad4b9a11a02cfc0d200dd876d12935b9cf7e64\n",
            "  Building wheel for jsonify (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonify: filename=jsonify-0.5-py3-none-any.whl size=1539 sha256=387fc00318725aa95a8d10f7297bee6f3671ed1fb78714ed6e89bd53a7a26238\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/6d/b2/9cb590fb5654ff13457f49d4aa5b16469419681613c46a2ec1\n",
            "Successfully built neo4j jsonify\n",
            "Installing collected packages: jsonify, pyngrok, neo4j\n",
            "Successfully installed jsonify-0.5 neo4j-5.20.0 pyngrok-7.1.6\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "# Get an Authentication token from https://ngrok.com/\n",
        "!pip install flask pyngrok neo4j jsonify\n",
        "!ngrok authtoken '2f34.....'\n",
        "# ngrok http --domain=scarcely-wired-asp.ngrok-free.app 80\n",
        "!mkdir -p /root/.ngrok2/\n",
        "!echo \"authtoken: 2f34yq......\" > /root/.ngrok2/ngrok.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Iz1-iJ25zelX",
        "outputId": "ff4accc1-33fd-48c4-d45b-e54dd73bccba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2024-04-29T17:03:28+0000 lvl=warn msg=\"ngrok config file found at both XDG and legacy locations, using XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ngrok tunnel \"public_url\" accessible on: https://scarcely-wired-asp.ngrok-free.app on port: 5000\n",
            "Running Flask, you can stop it by stopping cell\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Apr/2024 17:03:33] \"GET /problems HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Apr/2024 17:03:35] \"DELETE /problems/734 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UPDATED: {'key': '05532a0c-51e0-4ef7-b269-9ab5655836f7', 'name': 'NEW Large language model', 'goal': 'Generate a research survey on the given name and context.', 'context': 'A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process.[1] LLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word.', 'chiko': 'gadaboum', 'title_embedding_1': [0], 'title_embedding_2': [0], 'abstract_embedding_1': [0], 'abstract_embedding_2': [0], 'plan_embedding_1': [0], 'plan_embedding_2': [0], 'embedding1_model': 'text-embedding-ada-002', 'embedding2_model': 'e5-base-v2', 'success': True}\n",
            "UPDATED: {'description': 'create an extensive research on the subject', 'progress': 80}\n",
            "UPDATED: {'description': 'create a state of the art bibliography', 'progress': 80}\n",
            "UPDATED: {'description': 'create a paper on the state of the art', 'progress': 80}\n",
            "UPDATED: {'key': 'task1', 'description': 'task 1', 'status': 'not started', 'remaining_work': '100%', 'workload_hours': '8'}\n",
            "UPDATED: {'section_id': 1, 'section': 'h2 None', 'content': \"Since 2022, source-available models have been gaining popularity, especially at first with BLOOM and LLaMA, though both have restrictions on the field of use. Mistral AI's models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. As of January\\xa02024[update], Mixtral 8x7b is the most powerful open LLM according to the LMSYS Chatbot Arena Leaderboard, being more powerful than GPT-3.5 but not as powerful as GPT-4.[16]\", 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 2, 'section': 'h2 Dataset preprocessing', 'content': ' ', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 3, 'section': 'h3 Probabilistic tokenization', 'content': 'Because machine learning algorithms process numbers rather than text, the text must be converted to numbers. In the first step, a vocabulary is decided upon, then integer indexes are arbitrarily but uniquely assigned to each vocabulary entry, and finally, an embedding is associated to the integer index. Algorithms include byte-pair encoding and WordPiece.\\n Probabilistic tokenization also compresses the datasets. Because LLMs generally require input to be an array that is not jagged, the shorter texts must be \"padded\" until they match the length of the longest one. How many tokens are, on average, needed per word depends on the language of the dataset.[17][18]\\n', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 4, 'section': 'h4 BPE', 'content': 'Using a modification of byte-pair encoding, in the first step, all unique characters (including blanks and punctuation marks) are treated as an initial set of n-grams (i.e. initial set of uni-grams). Successively the most frequent pair of adjacent characters is merged into a bi-gram and all instances of the pair are replaced by it. All occurrences of adjacent pairs of (previously merged) n-grams that most frequently occur together are then again merged into even lengthier n-gram repeatedly until a vocabulary of prescribed size is obtained (in case of GPT-3, the size is 50257).', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'description': 'Incremental'}\n",
            "UPDATED: {'section_id': 5, 'key': 'agadebou', 'section': 'h3 Dataset cleaning', 'content': 'In the context of training LLMs, datasets are typically cleaned by removing toxic passages from the dataset, discarding low-quality data, and de-duplication.[22] Cleaned datasets can increase training efficiency and lead to improved downstream performance.[23][24]\\n With the increasing proportion of LLM-generated content on the web, data cleaning in the future may include filtering out such content.', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 6, 'section': 'h2 Training and architecture', 'content': ' ', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'description': 'Incremental'}\n",
            "UPDATED: {'section_id': 7, 'key': 'chedebambou', 'previous_key': 'agadebou', 'section': 'h3 Reinforcement learning from human feedback (RLHF)', 'content': 'Reinforcement learning from human feedback (RLHF) through algorithms, such as proximal policy optimization, is used to further fine-tune a model based on a dataset of human preferences.[26]\\n', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 8, '_node_label': 'task', 'section': 'h3 Instruction tuning', 'content': 'Using \"self-instruct\" approaches, LLMs have been able to bootstrap correct responses, replacing any naive responses, starting from human-generated corrections of a few cases. For example, in the instruction \"Write an essay about the main themes represented in Hamlet,\" an initial naive completion might be \"If you submit the essay after March 17, your grade will be reduced by 10% for each day of delay,\" based on the frequency of this textual sequence in the corpus.[27]\\n', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 9, 'section': 'h3 Mixture of experts', 'content': 'The largest LLM may be too expensive to train and use directly. For such models, mixture of experts (MoE) can be applied, a line of research pursued by Google researchers since 2017 to train models reaching up to 1 trillion parameters.[28][29][30]\\n', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 10, 'section': 'h3 None', 'content': \"The matter of LLM's exhibiting intelligence or understanding has two main aspects – the first is how to model thought and language in a computer system, and the second is how to enable the computer system to generate human like language.[89] These aspects of language as a model of cognition have been developed in the field of cognitive linguistics. American linguist George Lakoff presented Neural Theory of Language (NTL)[98] as a computational basis for using language as a model of learning tasks and understanding.\", 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 11, 'section': 'h2 Training cost', 'content': 'Advances in software and hardware have reduced the cost substantially since 2020, such that in 2023 training of a 12-billion-parameter LLM computational cost is 72,300 A100-GPU-hours, while in 2020 the cost of training a 1.5-billion-parameter LLM (which was two orders of magnitude smaller than the state of the art in 2020) was between $80 thousand and $1.6 million.[39][40][41] Since 2020, large sums were invested in increasingly large models.', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 12, '_operation': 'delete', 'section': 'h2 Tool use', 'content': \"There are certain tasks that, in principle, cannot be solved by any LLM, at least not without the use of external tools or additional software. An example of such a task is responding to the user's input '354 * 139 = ', provided that the LLM has not already encountered a continuation of this calculation in its training corpus. In such cases, the LLM needs to resort to running program code that calculates the result, which can then be included in its response.\", 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 13, '_operation': 'delete', 'section': 'h2 Agency', 'content': 'An LLM is a language model, which is not an agent as it has no goal, but it can be used as a component of an intelligent agent.[50] Researchers have described several methods for such integrations.[citation needed]\\n The ReAct (\"Reason\\xa0+\\xa0Act\") method constructs an agent out of an LLM, using the LLM as a planner. The LLM is prompted to \"think out loud\". Specifically, the language model is prompted with a textual description of the environment, a goal, a list of possible actions, and a record of the actions and observations so far.', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 23, 'section': 'h3 Task-specific datasets and benchmarks', 'content': 'A large number of testing datasets and benchmarks have also been developed to evaluate the capabilities of language models on more specific downstream tasks. Tests may be designed to evaluate a variety of capabilities, including general knowledge, commonsense reasoning, and mathematical problem-solving.\\n One broad category of evaluation dataset is question answering datasets, consisting of pairs of questions and correct answers.', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'description': 'minimum of 5000 words'}\n",
            "UPDATED: {'description': 'written in English'}\n",
            "UPDATED: {'description': 'has at least 5 sections'}\n",
            "UPDATED: {'key': 'bullshit', 'description': 'is really bullshit'}\n",
            "UPDATED: {'key': 'oh my god!', 'description': 'is oh my god!'}\n",
            "UPDATED: {'key': 'paper-1', 'title': 'Created resource 1', 'description': 'Created resource description 1', 'url': 'https://arxiv.com/resource1'}\n",
            "UPDATED: {'key': 'paper-2', 'title': 'Created resource 2', 'description': 'Created resource description 2', 'url': 'https://arxiv.com/resource2'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [29/Apr/2024 17:03:43] \"\u001b[35m\u001b[1mPOST /problems HTTP/1.1\u001b[0m\" 201 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: \n",
            "        MATCH (p:Problem)\n",
            "        WHERE id(p) = $id\n",
            "        OPTIONAL MATCH (p)-[rel]->(related)\n",
            "        RETURN id(p) AS problem_id, p AS problem,\n",
            "                collect(DISTINCT {relation_type: type(rel), related_id: id(related), related_node: related}) AS relatedEntities\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [29/Apr/2024 17:03:44] \"GET /problems/751 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Apr/2024 17:03:45] \"GET /problems HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Apr/2024 17:03:59] \"\u001b[33mGET /problems HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UPDATED: {'key': '05532a0c-51e0-4ef7-b269-9ab5655836f7', 'name': 'NEW Large language model', 'goal': 'Generate a research survey on the given name and context.', 'context': 'A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process.[1] LLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word.', 'chiko': 'gadaboum', 'title_embedding_1': [0], 'title_embedding_2': [0], 'abstract_embedding_1': [0], 'abstract_embedding_2': [0], 'plan_embedding_1': [0], 'plan_embedding_2': [0], 'embedding1_model': 'text-embedding-ada-002', 'embedding2_model': 'e5-base-v2', 'success': True}\n",
            "UPDATED: {'description': 'create an extensive research on the subject', 'progress': 80}\n",
            "UPDATED: {'description': 'create a state of the art bibliography', 'progress': 80}\n",
            "UPDATED: {'description': 'create a paper on the state of the art', 'progress': 80}\n",
            "UPDATED: {'key': 'task1', 'description': 'task 1', 'status': 'not started', 'remaining_work': '100%', 'workload_hours': '8'}\n",
            "UPDATED: {'section_id': 1, 'section': 'h2 None', 'content': \"Since 2022, source-available models have been gaining popularity, especially at first with BLOOM and LLaMA, though both have restrictions on the field of use. Mistral AI's models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. As of January\\xa02024[update], Mixtral 8x7b is the most powerful open LLM according to the LMSYS Chatbot Arena Leaderboard, being more powerful than GPT-3.5 but not as powerful as GPT-4.[16]\", 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 2, 'section': 'h2 Dataset preprocessing', 'content': ' ', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 3, 'section': 'h3 Probabilistic tokenization', 'content': 'Because machine learning algorithms process numbers rather than text, the text must be converted to numbers. In the first step, a vocabulary is decided upon, then integer indexes are arbitrarily but uniquely assigned to each vocabulary entry, and finally, an embedding is associated to the integer index. Algorithms include byte-pair encoding and WordPiece.\\n Probabilistic tokenization also compresses the datasets. Because LLMs generally require input to be an array that is not jagged, the shorter texts must be \"padded\" until they match the length of the longest one. How many tokens are, on average, needed per word depends on the language of the dataset.[17][18]\\n', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 4, 'section': 'h4 BPE', 'content': 'Using a modification of byte-pair encoding, in the first step, all unique characters (including blanks and punctuation marks) are treated as an initial set of n-grams (i.e. initial set of uni-grams). Successively the most frequent pair of adjacent characters is merged into a bi-gram and all instances of the pair are replaced by it. All occurrences of adjacent pairs of (previously merged) n-grams that most frequently occur together are then again merged into even lengthier n-gram repeatedly until a vocabulary of prescribed size is obtained (in case of GPT-3, the size is 50257).', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'description': 'Incremental'}\n",
            "UPDATED: {'section_id': 5, 'key': 'agadebou', 'section': 'h3 Dataset cleaning', 'content': 'In the context of training LLMs, datasets are typically cleaned by removing toxic passages from the dataset, discarding low-quality data, and de-duplication.[22] Cleaned datasets can increase training efficiency and lead to improved downstream performance.[23][24]\\n With the increasing proportion of LLM-generated content on the web, data cleaning in the future may include filtering out such content.', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 6, 'section': 'h2 Training and architecture', 'content': ' ', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'description': 'Incremental'}\n",
            "UPDATED: {'section_id': 7, 'key': 'chedebambou', 'previous_key': 'agadebou', 'section': 'h3 Reinforcement learning from human feedback (RLHF)', 'content': 'Reinforcement learning from human feedback (RLHF) through algorithms, such as proximal policy optimization, is used to further fine-tune a model based on a dataset of human preferences.[26]\\n', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 8, '_node_label': 'task', 'section': 'h3 Instruction tuning', 'content': 'Using \"self-instruct\" approaches, LLMs have been able to bootstrap correct responses, replacing any naive responses, starting from human-generated corrections of a few cases. For example, in the instruction \"Write an essay about the main themes represented in Hamlet,\" an initial naive completion might be \"If you submit the essay after March 17, your grade will be reduced by 10% for each day of delay,\" based on the frequency of this textual sequence in the corpus.[27]\\n', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 9, 'section': 'h3 Mixture of experts', 'content': 'The largest LLM may be too expensive to train and use directly. For such models, mixture of experts (MoE) can be applied, a line of research pursued by Google researchers since 2017 to train models reaching up to 1 trillion parameters.[28][29][30]\\n', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 10, 'section': 'h3 None', 'content': \"The matter of LLM's exhibiting intelligence or understanding has two main aspects – the first is how to model thought and language in a computer system, and the second is how to enable the computer system to generate human like language.[89] These aspects of language as a model of cognition have been developed in the field of cognitive linguistics. American linguist George Lakoff presented Neural Theory of Language (NTL)[98] as a computational basis for using language as a model of learning tasks and understanding.\", 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 11, 'section': 'h2 Training cost', 'content': 'Advances in software and hardware have reduced the cost substantially since 2020, such that in 2023 training of a 12-billion-parameter LLM computational cost is 72,300 A100-GPU-hours, while in 2020 the cost of training a 1.5-billion-parameter LLM (which was two orders of magnitude smaller than the state of the art in 2020) was between $80 thousand and $1.6 million.[39][40][41] Since 2020, large sums were invested in increasingly large models.', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 12, '_operation': 'delete', 'section': 'h2 Tool use', 'content': \"There are certain tasks that, in principle, cannot be solved by any LLM, at least not without the use of external tools or additional software. An example of such a task is responding to the user's input '354 * 139 = ', provided that the LLM has not already encountered a continuation of this calculation in its training corpus. In such cases, the LLM needs to resort to running program code that calculates the result, which can then be included in its response.\", 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 13, '_operation': 'delete', 'section': 'h2 Agency', 'content': 'An LLM is a language model, which is not an agent as it has no goal, but it can be used as a component of an intelligent agent.[50] Researchers have described several methods for such integrations.[citation needed]\\n The ReAct (\"Reason\\xa0+\\xa0Act\") method constructs an agent out of an LLM, using the LLM as a planner. The LLM is prompted to \"think out loud\". Specifically, the language model is prompted with a textual description of the environment, a goal, a list of possible actions, and a record of the actions and observations so far.', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 23, 'section': 'h3 Task-specific datasets and benchmarks', 'content': 'A large number of testing datasets and benchmarks have also been developed to evaluate the capabilities of language models on more specific downstream tasks. Tests may be designed to evaluate a variety of capabilities, including general knowledge, commonsense reasoning, and mathematical problem-solving.\\n One broad category of evaluation dataset is question answering datasets, consisting of pairs of questions and correct answers.', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'description': 'minimum of 5000 words'}\n",
            "UPDATED: {'description': 'written in English'}\n",
            "UPDATED: {'description': 'has at least 5 sections'}\n",
            "UPDATED: {'key': 'bullshit', 'description': 'is really bullshit'}\n",
            "UPDATED: {'key': 'oh my god!', 'description': 'is oh my god!'}\n",
            "UPDATED: {'key': 'paper-1', 'title': 'Created resource 1', 'description': 'Created resource description 1', 'url': 'https://arxiv.com/resource1'}\n",
            "UPDATED: {'key': 'paper-2', 'title': 'Created resource 2', 'description': 'Created resource description 2', 'url': 'https://arxiv.com/resource2'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [29/Apr/2024 17:04:07] \"\u001b[35m\u001b[1mPOST /problems HTTP/1.1\u001b[0m\" 201 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: \n",
            "        MATCH (p:Problem)\n",
            "        WHERE id(p) = $id\n",
            "        OPTIONAL MATCH (p)-[rel]->(related)\n",
            "        RETURN id(p) AS problem_id, p AS problem,\n",
            "                collect(DISTINCT {relation_type: type(rel), related_id: id(related), related_node: related}) AS relatedEntities\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [29/Apr/2024 17:04:08] \"GET /problems/663 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Apr/2024 17:04:09] \"GET /problems HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Apr/2024 17:15:33] \"\u001b[33mGET /problems HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UPDATED: {'key': 'problem-05532a0c', 'name': 'NEW Large language model', 'goal': 'Generate a research survey on the given name and context.', 'context': 'A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process.[1] LLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word.', 'chiko': 'gadaboum', 'title_embedding_1': [0], 'title_embedding_2': [0], 'abstract_embedding_1': [0], 'abstract_embedding_2': [0], 'plan_embedding_1': [0], 'plan_embedding_2': [0], 'embedding1_model': 'text-embedding-ada-002', 'embedding2_model': 'e5-base-v2', 'success': True}\n",
            "UPDATED: {'key': 'sub_goals-1', 'description': 'create an extensive research on the subject', 'progress': 80}\n",
            "UPDATED: {'key': 'sub_goals-2', 'description': 'create a state of the art bibliography', 'progress': 80}\n",
            "UPDATED: {'key': 'sub_goals-3', 'description': 'create a paper on the state of the art', 'progress': 80}\n",
            "UPDATED: {'key': 'task1', 'description': 'task 1', 'status': 'not started', 'remaining_work': '100%', 'workload_hours': '8'}\n",
            "UPDATED: {'section_id': 1, 'key': 'solution-1', 'section': 'h2 None', 'content': \"Since 2022, source-available models have been gaining popularity, especially at first with BLOOM and LLaMA, though both have restrictions on the field of use. Mistral AI's models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. As of January\\xa02024[update], Mixtral 8x7b is the most powerful open LLM according to the LMSYS Chatbot Arena Leaderboard, being more powerful than GPT-3.5 but not as powerful as GPT-4.[16]\", 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 2, 'key': 'solution-2', 'section': 'h2 Dataset preprocessing', 'content': ' ', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 3, 'key': 'solution-3', 'section': 'h3 Probabilistic tokenization', 'content': 'Because machine learning algorithms process numbers rather than text, the text must be converted to numbers. In the first step, a vocabulary is decided upon, then integer indexes are arbitrarily but uniquely assigned to each vocabulary entry, and finally, an embedding is associated to the integer index. Algorithms include byte-pair encoding and WordPiece.\\n Probabilistic tokenization also compresses the datasets. Because LLMs generally require input to be an array that is not jagged, the shorter texts must be \"padded\" until they match the length of the longest one. How many tokens are, on average, needed per word depends on the language of the dataset.[17][18]\\n', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 4, 'key': 'solution-4', 'section': 'h4 BPE', 'content': 'Using a modification of byte-pair encoding, in the first step, all unique characters (including blanks and punctuation marks) are treated as an initial set of n-grams (i.e. initial set of uni-grams). Successively the most frequent pair of adjacent characters is merged into a bi-gram and all instances of the pair are replaced by it. All occurrences of adjacent pairs of (previously merged) n-grams that most frequently occur together are then again merged into even lengthier n-gram repeatedly until a vocabulary of prescribed size is obtained (in case of GPT-3, the size is 50257).', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'description': 'Incremental'}\n",
            "UPDATED: {'section_id': 5, 'key': 'agadebou', 'section': 'h3 Dataset cleaning', 'content': 'In the context of training LLMs, datasets are typically cleaned by removing toxic passages from the dataset, discarding low-quality data, and de-duplication.[22] Cleaned datasets can increase training efficiency and lead to improved downstream performance.[23][24]\\n With the increasing proportion of LLM-generated content on the web, data cleaning in the future may include filtering out such content.', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 6, 'key': 'solution-6', 'section': 'h2 Training and architecture', 'content': ' ', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'description': 'Incremental'}\n",
            "UPDATED: {'section_id': 7, 'key': 'chedebambou', 'previous_key': 'agadebou', 'section': 'h3 Reinforcement learning from human feedback (RLHF)', 'content': 'Reinforcement learning from human feedback (RLHF) through algorithms, such as proximal policy optimization, is used to further fine-tune a model based on a dataset of human preferences.[26]\\n', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 8, 'key': 'solution-8', '_node_label': 'task', 'section': 'h3 Instruction tuning', 'content': 'Using \"self-instruct\" approaches, LLMs have been able to bootstrap correct responses, replacing any naive responses, starting from human-generated corrections of a few cases. For example, in the instruction \"Write an essay about the main themes represented in Hamlet,\" an initial naive completion might be \"If you submit the essay after March 17, your grade will be reduced by 10% for each day of delay,\" based on the frequency of this textual sequence in the corpus.[27]\\n', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 9, 'key': 'solution-9', 'section': 'h3 Mixture of experts', 'content': 'The largest LLM may be too expensive to train and use directly. For such models, mixture of experts (MoE) can be applied, a line of research pursued by Google researchers since 2017 to train models reaching up to 1 trillion parameters.[28][29][30]\\n', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 10, 'key': 'solution-10', 'section': 'h3 None', 'content': \"The matter of LLM's exhibiting intelligence or understanding has two main aspects – the first is how to model thought and language in a computer system, and the second is how to enable the computer system to generate human like language.[89] These aspects of language as a model of cognition have been developed in the field of cognitive linguistics. American linguist George Lakoff presented Neural Theory of Language (NTL)[98] as a computational basis for using language as a model of learning tasks and understanding.\", 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 11, 'key': 'solution-11', 'section': 'h2 Training cost', 'content': 'Advances in software and hardware have reduced the cost substantially since 2020, such that in 2023 training of a 12-billion-parameter LLM computational cost is 72,300 A100-GPU-hours, while in 2020 the cost of training a 1.5-billion-parameter LLM (which was two orders of magnitude smaller than the state of the art in 2020) was between $80 thousand and $1.6 million.[39][40][41] Since 2020, large sums were invested in increasingly large models.', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 12, 'key': 'solution-12', '_operation': 'delete', 'section': 'h2 Tool use', 'content': \"There are certain tasks that, in principle, cannot be solved by any LLM, at least not without the use of external tools or additional software. An example of such a task is responding to the user's input '354 * 139 = ', provided that the LLM has not already encountered a continuation of this calculation in its training corpus. In such cases, the LLM needs to resort to running program code that calculates the result, which can then be included in its response.\", 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 13, 'key': 'solution-13', '_operation': 'delete', 'section': 'h2 Agency', 'content': 'An LLM is a language model, which is not an agent as it has no goal, but it can be used as a component of an intelligent agent.[50] Researchers have described several methods for such integrations.[citation needed]\\n The ReAct (\"Reason\\xa0+\\xa0Act\") method constructs an agent out of an LLM, using the LLM as a planner. The LLM is prompted to \"think out loud\". Specifically, the language model is prompted with a textual description of the environment, a goal, a list of possible actions, and a record of the actions and observations so far.', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'section_id': 23, 'key': 'solution-23', 'section': 'h3 Task-specific datasets and benchmarks', 'content': 'A large number of testing datasets and benchmarks have also been developed to evaluate the capabilities of language models on more specific downstream tasks. Tests may be designed to evaluate a variety of capabilities, including general knowledge, commonsense reasoning, and mathematical problem-solving.\\n One broad category of evaluation dataset is question answering datasets, consisting of pairs of questions and correct answers.', 'section_embedding_1': [0], 'section_embedding_2': [0], 'content_embedding_1': [0], 'content_embedding_2': [0]}\n",
            "UPDATED: {'key': 'constraint-1', 'description': 'minimum of 5000 words'}\n",
            "UPDATED: {'key': 'constraint-2', 'description': 'written in English'}\n",
            "UPDATED: {'key': 'constraint-3', 'description': 'has at least 5 sections'}\n",
            "UPDATED: {'key': 'bullshit', 'description': 'is really bullshit'}\n",
            "UPDATED: {'key': 'oh my god!', 'description': 'is oh my god!'}\n",
            "UPDATED: {'key': 'paper-1', 'title': 'Created resource 1', 'description': 'Created resource description 1', 'url': 'https://arxiv.com/resource1'}\n",
            "UPDATED: {'key': 'paper-2', 'title': 'Created resource 2', 'description': 'Created resource description 2', 'url': 'https://arxiv.com/resource2'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [29/Apr/2024 17:15:42] \"\u001b[35m\u001b[1mPOST /problems HTTP/1.1\u001b[0m\" 201 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: \n",
            "        MATCH (p:Problem)\n",
            "        WHERE id(p) = $id\n",
            "        OPTIONAL MATCH (p)-[rel]->(related)\n",
            "        RETURN id(p) AS problem_id, p AS problem,\n",
            "                collect(DISTINCT {relation_type: type(rel), related_id: id(related), related_node: related}) AS relatedEntities\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [29/Apr/2024 17:15:42] \"GET /problems/683 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Apr/2024 17:15:43] \"GET /problems HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "from array import array\n",
        "#from google.colab import output\n",
        "#output.serve_kernel_port_as_window(5000)\n",
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "from neo4j import GraphDatabase\n",
        "import uuid  # For generating unique keys\n",
        "import traceback\n",
        "import inspect\n",
        "\n",
        "problem_relations = [\n",
        "    (\"subgoals\", \"subgoal\", \"HAS_SUB_GOAL\"),\n",
        "    (\"sub_goals\", \"subgoal\", \"HAS_SUB_GOAL\"),\n",
        "    (\"tasks\", \"task\", \"HAS_TASK\"),\n",
        "    (\"solutions\", \"solution\", \"HAS_SOLUTION\"),\n",
        "    (\"constraints\", \"constraint\", \"HAS_CONSTRAINT\"),\n",
        "    (\"prompts\", \"prompt\", \"HAS_PROMPT\"),\n",
        "    (\"tags\", \"tag\", \"HAS_TAG\"),\n",
        "    (\"resources\", \"resource\", \"HAS_RESOURCE\"),]\n",
        "\n",
        "relations_keys = {item[0]: None for item in problem_relations}\n",
        "\n",
        "myport = 5000\n",
        "domain = \"scarcely-wired-asp.ngrok-free.app\"\n",
        "run_in_thread = False\n",
        "counter = 0\n",
        "\n",
        "# Neo4j connection details\n",
        "uri = \"neo4j+s://4fb62461.databases.neo4j.io:7687\"\n",
        "username = \"neo4j\"\n",
        "password = \"YM3M......\"\n",
        "\n",
        "# Create a Neo4j driver instance\n",
        "driver = GraphDatabase.driver(uri, auth=(username, password))\n",
        "\n",
        "# Créer une instance de l'application Flask\n",
        "app = Flask(__name__)\n",
        "\n",
        "def serialize_neo4j_record(record):\n",
        "    \"\"\"Serializes a Neo4j record to a JSON-friendly dictionary.\"\"\"\n",
        "    result = {}\n",
        "    for key in record.keys():\n",
        "        item = record[key]\n",
        "        if isinstance(item, list):\n",
        "            result[key] = [serialize_neo4j_node(node) for node in item]\n",
        "        else:\n",
        "            result[key] = serialize_neo4j_node(item)\n",
        "    return result\n",
        "\n",
        "def serialize_neo4j_node(node):\n",
        "    \"\"\"Serializes a Neo4j Node or Relationship to a dictionary of properties.\"\"\"\n",
        "    if node is None:\n",
        "        return None\n",
        "    elif hasattr(node, 'properties'):\n",
        "        # Ensure all properties are serializable\n",
        "        serialized = {k: serialize_neo4j_node(v) for k, v in node.properties.items()}\n",
        "        serialized['id'] = node.id  # Add the internal ID of the node or relationship\n",
        "        return serialized\n",
        "    elif isinstance(node, (int, float, str)):\n",
        "        return node  # JSON serializable\n",
        "    else:\n",
        "        # Recursively serialize dictionary contents\n",
        "        return {k: serialize_neo4j_node(v) for k, v in node.items()}\n",
        "\n",
        "def serialize_neo4j_node(node):\n",
        "    \"\"\"Serializes a Neo4j Node, Relationship, or any data to a dictionary.\"\"\"\n",
        "    if node is None:\n",
        "        return None\n",
        "    elif isinstance(node, list):\n",
        "        # If it's a list, serialize each item in the list\n",
        "        return [serialize_neo4j_node(item) for item in node]\n",
        "    elif hasattr(node, 'properties'):\n",
        "        # If it has properties, serialize them and add an ID\n",
        "        serialized = {k: serialize_neo4j_node(v) for k, v in node.properties.items()}\n",
        "        serialized['id'] = node.id  # Add the internal ID of the node or relationship\n",
        "        return serialized\n",
        "    elif isinstance(node, (int, float, str)):\n",
        "        # If it's a basic type, return it\n",
        "        return node\n",
        "    else:\n",
        "        # Otherwise, assume it's a dictionary and serialize its contents\n",
        "        return {k: serialize_neo4j_node(v) for k, v in node.items()}\n",
        "\n",
        "def handle_exception(e, error_number=500):\n",
        "    # Get the current function name\n",
        "    current_frame = inspect.currentframe()\n",
        "    calling_frame = inspect.getouterframes(current_frame, 2)[1]\n",
        "    function_name = calling_frame.function\n",
        "\n",
        "    # Capture the traceback information\n",
        "    traceback_info = traceback.format_exc()  # Gets the full traceback as a string\n",
        "\n",
        "    # Build the error message\n",
        "    error_message = {\n",
        "        \"error\": str(e),\n",
        "        \"function\": function_name,\n",
        "        \"traceback\": traceback_info,\n",
        "    }\n",
        "\n",
        "    # Print the error information for debugging\n",
        "    print(f\"Error in function '{function_name}': {str(e)}\")\n",
        "    print(\"Traceback:\")\n",
        "    print(traceback_info)\n",
        "\n",
        "    # Return a JSON response with the error information\n",
        "    return jsonify(error_message), error_number\n",
        "\n",
        "# Route de base\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    global counter\n",
        "    counter += 1\n",
        "    return \"Hello from Colab! Counter:\" + str(counter)\n",
        "\n",
        "@app.route(\"/problems/<problem_id>\", methods=[\"GET\"])\n",
        "def get_problem(problem_id):\n",
        "    if problem_id.isdigit():\n",
        "        filter, parameters = \"id(p) = $id\", {'id': int(problem_id)}\n",
        "    else:\n",
        "        filter, parameters = \"p.name =~ $name\", {'name': f\"(?i).*{problem_id}.*\"}\n",
        "\n",
        "    # Safe parameterization for numeric ID\n",
        "    query = \"\"\"\n",
        "        MATCH (p:Problem)\n",
        "        WHERE \"\"\" + filter + \"\"\"\n",
        "        OPTIONAL MATCH (p)-[rel]->(related)\n",
        "        RETURN id(p) AS problem_id, p AS problem,\n",
        "                collect(DISTINCT {relation_type: type(rel), related_id: id(related), related_node: related}) AS relatedEntities\n",
        "    \"\"\"\n",
        "    print(f\"Query: {query}\")\n",
        "\n",
        "    with driver.session() as session:\n",
        "        result = session.run(query, parameters=parameters)\n",
        "        data = [serialize_neo4j_record(record) for record in result]\n",
        "        if data:\n",
        "            return jsonify(data)\n",
        "        else:\n",
        "            return jsonify({\"error\": \"Problem not found\"}), 404\n",
        "\n",
        "@app.route(\"/problems\", methods=[\"GET\"])\n",
        "def list_problems():\n",
        "    # Cypher query to retrieve all problems\n",
        "    query = \"\"\"\n",
        "        MATCH (p:Problem)\n",
        "        RETURN DISTINCT id(p) as uid, p.name as name, p.context as context, p.goal as goal\n",
        "        ORDER BY p.name\n",
        "    \"\"\"\n",
        "    with driver.session() as session:\n",
        "        result = session.run(query)\n",
        "        data = [{\"name\": record[\"name\"], \"uid\": record[\"uid\"], \"goal\": record[\"goal\"], \"context\": record[\"context\"]} for record in result]\n",
        "        if data:\n",
        "            return jsonify(data)\n",
        "        else:\n",
        "            return jsonify({\"error\": \"No problems found\"}), 404\n",
        "\n",
        "def create_or_update_node(tx, parent_key, node_label, relation_label, attributes, relations_keys, parent_node_label='Problem', create=True):\n",
        "    # Only allow primitive types or arrays for Neo4j properties and do not update key or id\n",
        "    update_fields = filter_element_fields(attributes)\n",
        "    node_key = attributes.get(\"key\", str(uuid.uuid4()))  # Generate unique key\n",
        "    node_label = attributes.get('_node_label', attributes.get('node_label', node_label))\n",
        "\n",
        "    if attributes.get(\"_operation\", None) == \"delete\":\n",
        "        # Delete the node if it exists\n",
        "        tx.run(\"\"\"\n",
        "            MATCH (node:{node_label} {key: $node_key})\n",
        "            DETACH DELETE node\n",
        "        \"\"\", {\"node_key\": node_key, \"node_label\": node_label})\n",
        "        return node_key\n",
        "\n",
        "    # Check if the node exists based on the key\n",
        "    existing_node = tx.run(f\"\"\"MATCH (node:{node_label} {{ key: $node_key }}) RETURN node\"\"\", {\"node_key\": node_key}).single()\n",
        "\n",
        "    if existing_node:\n",
        "        # Update specific fields only\n",
        "        tx.run(f\"\"\"MATCH (node:{node_label} {{ key: $node_key }}) SET {', '.join([f'node.{k} = ${k}' for k in update_fields.keys()])}\"\"\",\n",
        "            {**update_fields, \"node_key\": node_key},)\n",
        "    elif create:\n",
        "        # Create a new node and link it to the parent\n",
        "        parent_key = attributes.get('_parent_key', attributes.get('parent_key', parent_key))\n",
        "        parent_node_label = attributes.get('_parent_node_label', attributes.get('parent_node_label', parent_node_label))\n",
        "        tx.run(f\"\"\"\n",
        "            MATCH (parent:{parent_node_label} {{ key: $parent_key }})\n",
        "            CREATE (node:{node_label} {{ key: $node_key, {', '.join([f'{k}: ${k}' for k in update_fields.keys()])} }})\n",
        "            MERGE (parent)-[:{relation_label}]->(node)\n",
        "            \"\"\",\n",
        "            {\"parent_key\": parent_key, \"node_key\": node_key, **update_fields},\n",
        "        )\n",
        "    return node_key  # Return the key used for the node\n",
        "\n",
        "def filter_element_fields(element, id_excluded=[\"id\"]):\n",
        "    update_fields = {}\n",
        "    for k, v in element.items():\n",
        "        if k not in id_excluded and k not in relations_keys and (isinstance(v, (int, float, bool, str)) or (isinstance(v, list) and not any(isinstance(item, dict) for item in v))):  # Only primitive types or arrays\n",
        "            update_fields[k] = v\n",
        "    print(f\"UPDATED: {update_fields}\")\n",
        "    return update_fields\n",
        "\n",
        "def process_nested_elements(tx, parent_key, elements, node_label, relation_label, problem_relations, parent_node_label='Problem', create=True):\n",
        "    if not isinstance(elements, list): elements = [elements]\n",
        "    previous_key = None\n",
        "    for element in elements:\n",
        "        previous_key = element.get('_previous_key', element.get('previous_key', previous_key))\n",
        "        cur_node_label = element.get('_node_label', element.get('node_label', node_label))\n",
        "        current_key = create_or_update_node(tx, parent_key, node_label, relation_label, element, relations_keys, parent_node_label, create=create)\n",
        "        # Process nested elements recursively\n",
        "        for key, sub_label, sub_relation in problem_relations:\n",
        "            if key in element:\n",
        "                #print(f\" process_nested_elements(tx, {current_key}, {element[key]}, {sub_label}, {sub_relation}, {problem_relations})\")\n",
        "                process_nested_elements(tx, current_key, element[key], sub_label, sub_relation, problem_relations, sub_label, create=create)\n",
        "        # Handle NEXT relationships\n",
        "        previous_key = element.get('_previous_key', element.get('previous_key', previous_key))\n",
        "        if previous_key:\n",
        "            tx.run(\n",
        "                f\"\"\"\n",
        "                MATCH (prev:{node_label}), (curr:{node_label})\n",
        "                WHERE prev.key = $previous_key AND curr.key = $current_key\n",
        "                MERGE (prev)-[:NEXT]->(curr)\"\"\",\n",
        "                {\"previous_key\": previous_key, \"current_key\": current_key},\n",
        "            )\n",
        "        previous_key = current_key\n",
        "\n",
        "@app.route(\"/problems\", methods=[\"POST\"])\n",
        "def create_problem():\n",
        "    data = request.get_json()\n",
        "    update_fields = filter_element_fields(data)\n",
        "    problem_name = data[\"name\"]\n",
        "\n",
        "    with driver.session() as session:\n",
        "        if session.run(\"MATCH (p:Problem {name: $name}) RETURN p\", {\"name\": problem_name}).single():\n",
        "            return jsonify({\"error\": \"Problem already exists\"}), 409\n",
        "\n",
        "        try:\n",
        "            with session.begin_transaction() as tx:\n",
        "              problem_key = data.get(\"key\", str(uuid.uuid4()))\n",
        "              create_problem_query = \"\"\"\n",
        "              MERGE (p:Problem {\"\"\" + ', '.join([f'{k}: ${k}' for k in update_fields.keys()]) + \"\"\"})\n",
        "              RETURN id(p) AS problem_id, p.key AS problem_key\n",
        "              \"\"\"\n",
        "              #print(create_problem_query) #print(update_fields)\n",
        "              problem_id_key = tx.run(create_problem_query, {**update_fields}).single()\n",
        "              problem_id, problem_key = problem_id_key['problem_id'], problem_id_key['problem_key']\n",
        "\n",
        "              for key, sub_label, sub_relation in problem_relations:\n",
        "                if data.get(key, None):\n",
        "                  process_nested_elements(tx, problem_key, data.get(key, []), sub_label, sub_relation, problem_relations, create=True)\n",
        "\n",
        "              tx.commit()\n",
        "\n",
        "            return jsonify({\n",
        "              \"message\": \"Problem created successfully\",\n",
        "              \"problem_key\": problem_key,\n",
        "              \"problem_id\": problem_id,\n",
        "            }), 201\n",
        "\n",
        "        except Exception as e:\n",
        "            return handle_exception(e)\n",
        "\n",
        "@app.route(\"/problems/<problem_id>\", methods=[\"PUT\"])\n",
        "def update_problem(problem_id):\n",
        "    try:\n",
        "        problem_id_int = int(problem_id)  # Ensure problem ID is an integer\n",
        "    except ValueError:\n",
        "        return jsonify({\"error\": \"Invalid problem ID\"}), 400\n",
        "\n",
        "    data = request.get_json()  # Get JSON data from the request\n",
        "    update_fields = filter_element_fields(data)\n",
        "\n",
        "    with driver.session() as session:\n",
        "        try:\n",
        "            with session.begin_transaction() as tx:\n",
        "                # Get the key for the problem\n",
        "                problem_key_query = tx.run(\"\"\"MATCH (p:Problem) WHERE id(p) = $problem_id RETURN p.key\"\"\", {\"problem_id\": problem_id_int},)\n",
        "                problem_key = problem_key_query.single().get(\"p.key\", str(uuid.uuid4()))\n",
        "\n",
        "                # Update the main problem node\n",
        "                tx.run(f\"\"\"MATCH (p:Problem) WHERE id(p) = $problem_id\n",
        "                    SET {', '.join([f'p.{k}= ${k}' for k in update_fields.keys()])}\"\"\",\n",
        "                    {\n",
        "                        \"problem_id\": problem_id_int,\n",
        "                        **update_fields})\n",
        "\n",
        "                # Process elements while ensuring proper deletion and re-creation\n",
        "                for key, sub_label, sub_relation in problem_relations:\n",
        "                    if data.get(key, None):\n",
        "                        process_nested_elements(tx, problem_key, data.get(key, []), sub_label, sub_relation, problem_relations, create=False)\n",
        "\n",
        "                # Commit the transaction after all operations\n",
        "                tx.commit()\n",
        "\n",
        "            return jsonify({\"message\": \"Problem updated successfully\"}), 200\n",
        "\n",
        "        except Exception as e:\n",
        "            return handle_exception(e)\n",
        "\n",
        "@app.route(\"/problems/<problem_id>\", methods=[\"DELETE\"])\n",
        "def delete_problem(problem_id):\n",
        "    try:\n",
        "        problem_id_int = int(problem_id)\n",
        "    except ValueError:\n",
        "        return jsonify({\"error\": \"Invalid problem ID\"}), 400\n",
        "\n",
        "    try:\n",
        "        with driver.session() as session:\n",
        "            # Check if the problem exists\n",
        "            result = session.run(\"\"\"\n",
        "                MATCH (p:Problem)\n",
        "                WHERE id(p) = $id\n",
        "                RETURN count(p) AS count\n",
        "            \"\"\", {'id': problem_id_int}).single()\n",
        "\n",
        "            if result[\"count\"] == 0:\n",
        "                return jsonify({\"error\": \"Problem not found\"}), 404\n",
        "\n",
        "            with session.begin_transaction() as tx:\n",
        "                # Delete the problem itself\n",
        "                tx.run(\"\"\"\n",
        "                    MATCH (p:Problem)\n",
        "                    WHERE id(p) = $id\n",
        "                    DETACH DELETE p\n",
        "                \"\"\", {'id': problem_id_int})\n",
        "\n",
        "                # Delete any orphaned entities that were related to the problem\n",
        "                tx.run(\"\"\"\n",
        "                    MATCH (e)\n",
        "                    WHERE (e)<-[:HAS_SUB_GOAL|HAS_TASK|HAS_RESOURCE|HAS_PROMPT|HAS_SOLUTION]-(:Problem)\n",
        "                    DETACH DELETE e\n",
        "                \"\"\")\n",
        "\n",
        "                # Delete any node not linked to any Problem\n",
        "                tx.run(\"\"\"\n",
        "                    MATCH (n)\n",
        "                    WHERE NOT (n)-[:HAS_SUB_GOAL|HAS_TASK|HAS_RESOURCE|HAS_PROMPT|HAS_SOLUTION]-(:Problem)\n",
        "                    DETACH DELETE n\n",
        "                \"\"\")\n",
        "\n",
        "                tx.commit()\n",
        "                return jsonify({\"message\": \"Problem and all its related entities deleted successfully\"}), 200\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"error: {e}\")\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "# Initialiser ngrok lors du démarrage de l'application\n",
        "def start_app_and_ngrok(app, domain=None, port=5000, run_in_thread=False):\n",
        "    public_url = ngrok.connect(port, domain=domain, inspect=False).public_url if domain else ngrok.connect(port).public_url  # Assurez-vous que le port correspond à celui utilisé par Flask\n",
        "    print(f'ngrok tunnel \"public_url\" accessible on: {public_url} on port: {port}')\n",
        "    app.config[\"BASE_URL\"] = public_url\n",
        "    if run_in_thread:\n",
        "      import threading\n",
        "      # Start the Flask server in a new thread\n",
        "      threading.Thread(target=app.run, kwargs={\"use_reloader\": False}).start()\n",
        "      print(\"Flask running on a separate THREAD, you have to stop session to stop it\")\n",
        "    else:\n",
        "      print(\"Running Flask, you can stop it by stopping cell\")\n",
        "      app.run(port=myport)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start_app_and_ngrok(app, domain=domain, port=myport, run_in_thread=run_in_thread)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
